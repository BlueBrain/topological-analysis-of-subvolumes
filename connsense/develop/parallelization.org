#+PROPERTY: header-args:jupyter :session  ~/jupyter-run/active-ssh.json :pandoc t
#+PROPERTY: header-args:jupyter-python :session ~/jupyter-run/active-ssh.json :pandoc t

#+STARTUP: overview
#+STARTUP: logdrawer
#+STARTUP: hideblocks

#+title: Parallelization

We have a parallelization scheme in place that we used to parallelize the computations of analyses.
With multi-population nodes and edges, and to start the support of multiple circuit variants, definitions of subtargets, extraction of neuron properties, and extraction of connectivity should also be parallelized. Here we develop a scheme for parallelization of computations per subtarget    .

* Setup :noexport:
In our discussion we will develop scientific concepts to measure the circuit, and implement Python functions to compute them. Here we setup a notebook template to test and explore, and the structure of a ~Python~ package for our methods.

Let us setup an interactive ~Python~ session where we can run the code developed here.

#+begin_src jupyter
from pathlib import Path
print("WelCome to EMACS Jupyter in %s"%(Path.cwd()))
#+end_src

#+RESULTS:
: WelCome to EMACS Jupyter in /gpfs/bbp.cscs.ch/home/sood/work/workspaces

** Introduction

Let us initialize the notebook with some general-purpose packages that we may need for things like plotting and loading a circuit. We will discuss ~connsense packages~ in the next section.


#+name: notebook-init
#+begin_src jupyter-python
# %% [markdown]
"""# Parallelization scheme for `connsense-TAP`

We develop a parallelization scheme for `connsense-TAP` computations.

"""
# %% [code]

from importlib import reload
from collections.abc import Mapping
from collections import OrderedDict
from pprint import pprint, pformat
from pathlib import Path

import numpy as np
import pandas as pd

import matplotlib

reload(matplotlib)
from matplotlib import pylab as plt
import seaborn as sbn
GOLDEN = (1. + np.sqrt(5.))/2.

from IPython.display import display

from bluepy import Synapse, Cell, Circuit

print("We will plot golden aspect ratios: ", GOLDEN)
#+end_src

** Connsense modules

We have run ~connsense-CRAP~ for the SSCx dissemination variant /Bio-M/, extracting data that we will use to compute the factology. Here is a list of workspaces we will need to generate factsheets.

#+name: notebook-modules
#+begin_src jupyter-python
# %% [markdown]
"""
We will need some ~connsense~ modules for our experiments
"""
# %% [code]
from connsense.pipeline import pipeline
from connsense.pipeline.parallelization import parallelization as prl
from connsense.pipeline.store import store as tap_store
#+end_src

** Connsense workspace
We will load circuit data and compute analyses on it. To read and write we need paths. We can choose from a real circuit.
#+name: notebook-workspaces
#+begin_src jupyter-python
# %% [markdown]
"""
We can set paths to load data, and to save the results of our experiments. Paths listed below are to artefacts associated with a SSCx-Dissemination circuit.
"""
# %% [code]
ROOTSPACE = Path("/")
PROJSPACE = ROOTSPACE / "gpfs/bbp.cscs.ch/project/proj83"
CONNSPACE = PROJSPACE / "home/sood" / "topological-analysis-subvolumes/test/v2"
#CONNSPACE = (PROJSPACE / "home/sood" / "portal/develop/factology-v2/analyses/connsense"/
#             "redefine-subtargets/create-index/morphology-mtypes")
#+end_src

** Tap store

While test-developing it will be good to have direct access to the ~connsense-TAP-store~ we will use.
#+name: notebook-connsense-tap
#+begin_src jupyter-python
# %% [markdown]
"""
For our experiments, we will need a circuit, an object to run / investigate the pipeline, and another to load / investigate the computated data.
"""
topaz = pipeline.TopologicalAnalysis(CONNSPACE/"pipeline.yaml", CONNSPACE/"runtime.yaml")
tap = tap_store.HDFStore(topaz._config)
print("Available analyses: ")
#+end_src

As of <2022-10-18 Tue> we are developing a fresh interface for ~connsense-TAP~ that will be used for accessing ~connsense-TAP~ data. The current implementation will continue to be used internally by ~connsense-TAP-parallelization~, and eventually absorbed into ~TopologicalAnalysisPipeline~.
#+name: notebook-connsense-topotap
#+begin_src jupyter-python
# %% [markdown]
"""Load a connsense-TAP to analyze topology of a circuit
"""
# %% [code]

from connsense.develop import (topotap as topotap_store, parallelization as devprl)
reload(topotap_store)
topotap = topotap_store.HDFStore(CONNSPACE/"pipeline.yaml")
print("Available analyses: ")
pprint(topotap.analyses)
#+end_src

** A notebook template to explore and develop

#+begin_src jupyter-python :tangle develop_parallelization.py :noweb yes :comments no :padline yes
<<notebook-init>>

<<notebook-modules>>

<<notebook-workspaces>>

<<notebook-connsense-tap>>

<<notebook-connsense-topotap>>
#+end_src

#+RESULTS:
:  2023-02-21 17:41:14,304: Configure slurm for define-subtargets
:  2023-02-21 17:41:14,304: Configure slurm for create-index
:  2023-02-21 17:41:14,305: No runtime configured for computation type create-index
:  2023-02-21 17:41:14,305: Configure slurm for extract-node-populations
:  2023-02-21 17:41:14,306: Configure slurm for extract-edge-populations
:  2023-02-21 17:41:14,306: Configure slurm for analyze-connectivity
: We will plot golden aspect ratios:  1.618033988749895
: Available analyses:
: Available analyses:
: {'connectivity': {'simplex-counts': <connsense.develop.topotap.TapDataset object at 0x7fff1929d0d0>}}

* Describe the computation
Computations are entered as sections in the config. We will reference a computation by it's ~computation-type~ and the ~quantity~ it computes. We will enter computations of the same ~computation-type~ together as a section in the config, enterting the computation parameters for each individual quantity.  Normally a process of ~(computation-type, quantity)~ should produce a frame or series that will be saved as a HDF dataset. In some cases there may be multiple datasets, each saved in their own dataset under the group ~<computation-type>/<quantity>~.  The names of these datasets will be read from the config.

/Multiple groups/ of the same ~<computation-type>/<quantity>~ will apply to computations of ~extract-node-types~, which configures ~modeltypes~ to extract. Each ~modeltype~ will have components configured as a dict mapping the component-name to a dict of applicable parameters, and a reference to the method to use for extraction.

We repressent a ~computation~ as a string (read from the CLI). We can /sub/-configure the computation as a YAML/JSON and pass it's location as a CLI argument. Or we can just extend the input string to ~<computation-type>/<of_quantity>/<component-if-any>~.

A string representation of a computation will need a method to read it, that we can extend as we further develop the concept of a computation.

#+name: parallelization-describe-computation
#+begin_src python
def describe(computation):
    """...Describe a `connsense-TAP computation`
    as the `connsense-TAP computation-type` to run, and the `quantity` that it computes.

    The parsed values will be used to look up parameters in the `connsense-TAP config.`
    """
    if isinstance(computation, str):
        description = computation.split('/')
        computation_type = description[0]
        quantity = '/'.join(description[1:])
    elif isinstance(computation, (tuple, list)):
        computation_type, quantity = computation
    else:
        raise TypeError(f"copmutation of illegal type {computation}")

    return (computation_type, quantity)

#+end_src

Computation of a quantity is parameterized in the config. To compute all ~quantities~ of a ~computation-type~, they are grouped in the config using a ~computation-key~ assigned to the ~computation-type~.

** Parameterize the step

Once have described a computation, we can parameterize it using the config. A ~quantity~ can be configured as a singleton, or with as a ~multicomp-quantity~ with components. Each component of a ~multicomp-quantity~ is represented as ~<quantity>/<component>~.

#+name: parallelization-parameterize-step
#+begin_src python
def parameterize(computation_type, of_quantity, in_config):
    """..."""
    """..."""
    paramkey = PARAMKEY[computation_type]

    if not computation_type in in_config["parameters"]:
        raise RuntimeError(f"Unknown {computation_type}")

    configured = in_config["parameters"][computation_type][paramkey]

    if of_quantity not in configured:
        try:
            multicomp, component = of_quantity.split('/')
        except ValueError:
            raise ConfigurationError(f"Unknown {paramkey} {of_quantity} for {computation_type}")
        configured_quantity =  configured[multicomp][component]

    else:
        configured_quantity = configured[of_quantity]

    return deepcopy(configured_quantity)

    if computation_type != "define-subtargets":
        if of_quantity not in in_config["parameters"][computation_type][paramkey]:
            raise RuntimeError(f"Unknown {paramkey[:-1]} {of_quantity} for {computation_type}")
        return deepcopy(in_config["parameters"][computation_type][paramkey][of_quantity])

    return deepcopy(in_config["parameters"]["define-subtargets"])

#+end_src

#+RESULTS: parallelization-parameterize-step
: None

** Test develop

Let us test develop computation of simplex-counts,

#+begin_src jupyter-python :tangle develop_parallelization.py
computation = "analyze-connectivity/simplex-counts"
print("Let test develop computation of")
pprint(prl.describe(computation))

params = prl.parameterize(*prl.describe(computation), topaz._config)
print("Using parameters")
pprint(params)
#+end_src

#+RESULTS:
#+begin_example
Let test develop computation of
('analyze-connectivity', 'simplex-counts')
Using parameters
{'computation': {'method': 'simplex_counts',
                 'source': '/gpfs/bbp.cscs.ch/project/proj83/analyses/topological-analysis-subvolumes/proj83/connectome_analysis/library/topology.py'},
 'description': 'Number of simplices by dimension.',
 'index': {'circuit': ['Bio_M'],
           'connectome': ['local'],
           'subtarget': {'dataset': ['define-subtargets', 'flatmap-columns']}},
 'input': {'adjacency': {'dataset': ['extract-edge-populations', 'local']}},
 'output': 'pandas.Series'}
#+end_example

* What gets parallelized?
To run a computation of multiple inputs in parallel on multiple ~compute-nodes~, we will need to assign to each input a ~compute-node~ to on, and the parallel ~batch~ it will be queued in,
#+name: develop-parallelization-batched-inputs-0
#+begin_src python
def batch_multinode_0(computation, of_inputs, in_config, at_dirpath, using_parallelization,
                      single_submission=500):
    """...Just read the method definition above,
    and code below
    """
    n_compute_nodes, n_parallel_jobs = using_parallelization

    LOG.info("Assign batches to %s inputs", len(of_inputs))
    batches = batch_parallel_groups(of_inputs, upto_number=n_parallel_jobs)

    n_batches = batches.max() + 1
    LOG.info("Assign compute nodes to %s batches of %s of_inputs", len(batches), n_batches)
    compute_nodes = distribute_compute_nodes(batches, upto_number=n_compute_nodes)

    LOG.info("Group %s compute node into launchscript submissions", compute_nodes.nunique())
    submissions = group_launchscripts(compute_nodes, max_entries=single_submission)

    assignment = pd.concat([batches, compute_nodes, submissions], axis=1)
    if at_dirpath:
        assignment_h5, dataset = COMPUTE_NODE_ASSIGNMENT
        assignment.to_hdf(at_dirpath/assignment_h5, key=dataset)
    return assignment

    #+end_src
Here is a newer version that should adjust the slicing's reduced load.
#+name: develop-parallelization-batched-inputs
#+begin_src python :noweb yes

def batch_multinode(computation, of_inputs, in_config, at_dirpath, using_parallelization,
                    single_submission=500,  with_weights=True, unit_weight=None,
                    njobs_to_estimate_load=None, max_parallel_jobs=None):
    """...Just read the method definition above,
        and code below
    """
    from tqdm import tqdm; tqdm.pandas()

    n_compute_nodes, n_parallel_jobs, order_complexity = using_parallelization
    n_parallel_biggest = int(n_parallel_jobs / n_compute_nodes)

    LOG.info("Assign compute-nodes to %s inputs", len(of_inputs))
    toc_index = of_inputs.index

    weights = (
        #of_inputs.progress_apply(lambda l: estimate_load(to_compute=None)(l())).dropna()
        of_inputs.progress_apply(estimate_load(order_complexity)).dropna()
        if not njobs_to_estimate_load
        else multiprocess_load_estimate(order_complexity, of_inputs, njobs_to_estimate_load))
    weights = (
        weights[~np.isclose(weights, 0.)].groupby(toc_index.names).max()
        .sort_values(ascending=True)).rename("weight")

    unit_weight = max(unit_weight or 0.,  weights.max())

    compute_nodes = ((n_compute_nodes * (np.cumsum(weights) / weights.sum() - 1.e-6))
                     .astype(int).rename("compute_node"))
    LOG.info("Assign batches to compute node subtargets")

    def weigh_one(compute_node):
        return weights.loc[compute_node.index]

    def batch(compute_node):
        """..."""
        cn_weights = weigh_one(compute_node)
        n_parallel = int(min(max_parallel_jobs or multiprocessing.cpu_count(), #/2
                             min(int(n_parallel_biggest * unit_weight / cn_weights.max()),
                                 len(cn_weights))))
        #batches = np.random.choice(np.arange(n_parallel), size=len(cn_weights), replace=True)
        n_comp = len(cn_weights)
        batch_values = list(range(n_parallel))
        batches = ((int(n_comp / n_parallel) + 1) * batch_values)[0:n_comp]
        return pd.Series(batches, name="batch", index=cn_weights.index)
        #return  pd.Series(np.linspace(0, n_parallel - 1.e-6, len(cn_weights), dtype=int),
        #                  name="batch",  index=cn_weights.index)

    batches = ((pd.concat([batch(compute_nodes)], keys=[compute_nodes.unique()[0]], names=["compute_node"])
                if compute_nodes.nunique() == 1
                else pd.DataFrame(compute_nodes).groupby("compute_node").apply(batch))
               .reorder_levels(compute_nodes.index.names + ["compute_node"]))

    if not with_weights:
        return batches

    if not isinstance(weights.index, pd.MultiIndex):
        weights.index = (pd.MultiIndex
                         .from_arrays([weights.index.values], names=[weights.index.name]))

    cn_weights = (
        pd.concat([weigh_one(compute_nodes)], keys=[compute_nodes.unique()[0]],
                  names=["compute_node"])
        if compute_nodes.nunique() == 1
        else pd.DataFrame(compute_nodes).groupby("compute_node").apply(weigh_one)
    ).reorder_levels(compute_nodes.index.names + ["compute_node"])

    assignment =  pd.concat([batches, cn_weights], axis=1) if with_weights else batches

    if at_dirpath:
        assignment_h5, dataset = COMPUTE_NODE_ASSIGNMENT
        assignment.to_hdf(at_dirpath/assignment_h5, key=dataset)
    return assignment

#+end_src

We have defined a multiple ~compute-node~ parallelization batching scheme that operates at three levels:
** 1. batch parallel groups of inputs together

Inputs with the same value will be queued in the same batch of inputs to with several such batches of inputs running in parallel on the same compute node. When assigning batches, we will try to estimate the computation load of an input and distribute batches to sum to the same amount of computation in each batch.

#+name: develop-parallelization-batched-inputs-parallel-groups
#+begin_src python
def batch_parallel_groups(of_inputs, upto_number, to_compute=None, return_load=False):
    """..."""
    from tqdm import tqdm; tqdm.pandas()

    if isinstance(of_inputs, pd.Series):
        weights = (of_inputs
                   .progress_apply(estimate_load(to_compute)).rename("load")
                   .sort_values(ascending=True))
    elif isinstance(of_inputs, pd.DataFrame):
        weights = (of_inputs
                   .progress_apply(estimate_load(to_compute), axis=1).rename("load")
                   .sort_values(ascending=True))
    else:
        raise TypeError(f"Unhandled type of input: {of_inputs}")

    nan_weights = weights[weights.isna()]
    if len(nan_weights) > 0:
        LOG.warning("No input data for %s / %s of_inputs:\n%s",
                    len(nan_weights), len(weights), pformat(nan_weights))
        weights = weights.dropna()

    computational_load = (np.cumsum(weights) / weights.sum()).rename("load")
    n = np.minimum(upto_number, len(weights))
    batches = ((n * (computational_load - computational_load.min()))
               .apply(int).rename("batch"))

    LOG.info("Load balanced batches for %s of_inputs: \n %s",
             len(of_inputs), batches.value_counts())
    return (batches if not return_load
            else pd.concat([batches, weights/weights.sum()], axis=1))
#+end_src
We infer the computational load to compute the result of an input from the first argument among the inputs. In ~connsense-TAP~, the inputs are ~lazyvals / lambdas~, /i.e./ ~unit-computations~ that can be called upon to return the input value defined in them. We assume that the cost of a computation will depend on the /size/ of it's inputs.  A ~connsense-TAP~ ~computation~ may take more than one input argument passed around as a ~dict~. We assume that the size of the input can be infered from the first argument in this ~dict~.  The estimate method we implement should work for other types of inputs as well.
*** Estimate loads
Not all inputs to a computation need the same resources. For efficient parallelization we will need to estimate load of each input.
#+name: develop-parallelization-estimate-batched-inputs-load-method
#+begin_src python
def estimate_load(order_complexity, to_compute=None):
    def of_input_data(d):
        """What would it take to compute input data d?
        """
        LOG.info("Estimate load to compute a %s", type(d))
        if d is None:
            return None

        try:
            S = d.shape
        except AttributeError:
            pass
        else:
            if order_complexity == -1:
                return np.prod(S)

            assert order_complexity >= 0

            return (np.nan if np.isnan(S[0]) else
                    0 if np.isclose(S[0], 0.0) else S[0] ** order_complexity)

        if callable(d):
            try:
                dload = d(to_get="shape")
            except TypeError:
                dload = d()
            return of_input_data(dload)

        if isinstance(d, Mapping):
            if not d: return 1
            first = next(v for v in d.values())
            return of_input_data(first)

        if isinstance(d, pd.Series):
            return d.apply(of_input_data).sum()

        try:
            N = len(d)
        except TypeError as terror:
            try:
                S = d.shape
            except AttributeError as aerror:
                LOG.error("Neither length, nor shape for input \n%s\n%s\n%s",
                          d, terror, aerror)
                return 1
            else:
                N = S[0]

        if N == 0:
            return 0

        if order_complexity == -1:
            try:
                S = d.shape
            except AttributeError:
                return N
            return np.prod(S)

        return N ** order_complexity

    return of_input_data

#+end_src

#+RESULTS: develop-parallelization-estimate-batched-inputs-load-method

We can use the method on a series or a frame, serially, or parallely

#+name: develop-parallelization-estimate-batched-inputs-load
#+begin_src python :noweb yes
<<develop-parallelization-estimate-batched-inputs-load-method>>

def multiprocess_load_estimate(order_complexity, inputs, njobs):
    """..."""
    from tqdm import tqdm; tqdm.pandas()
    assert njobs > 1, f"njobs={njobs} does not need multi-process."

    def weigh(batch_inputs, *, in_bowl, index):
        """..."""
        weight = batch_inputs.progress_apply(estimate_load(None))
        in_bowl[index] = weight
        return weight

    manager = Manager()
    bowl = manager.dict()
    processes = []

    batched_inputs = pd.DataFrame({"input": inputs,
                                   "batch": np.linspace(0, njobs - 1.e-6, len(inputs), dtype=int)})
    for b, batch in batched_inputs.groupby("batch"):
        LOG.info("Estimate load of %s inputs in batch %s / %s", len(batch), b, njobs)
        p = Process(target=weigh, args=(batch.input,), kwargs={"index": b, "in_bowl": bowl})
        p.start()
        processes.append(p)
    LOG.info("LAUNCHED %s processes", len(processes))

    for p in processes:
        LOG.info("Join process %s", p)
        p.join()
    LOG.info("Parallel load estimation results %s", len(bowl))

    return pd.concat([weights for weights in bowl.values()])

#+end_src

** 2. distribute among compute nodes, the ~parallel-batches~ assigned in step 1

We will distribute the parallel batches among a specified number of compute-nodes.
#+name: develop-parallelization-batched-inputs-distribute-compute-nodes
#+begin_src python
def distribute_compute_nodes(parallel_batches, upto_number):
    """..."""
    LOG.info("Assign compute nodes to batches \n%s", parallel_batches)
    _, dset = COMPUTE_NODE_ASSIGNMENT

    n_parallel_batches = parallel_batches.max() + 1
    compute_nodes = np.linspace(0, upto_number - 1.e-6, n_parallel_batches, dtype=int)
    assignment = pd.Series(compute_nodes[parallel_batches.values], name=dset, index=parallel_batches.index)
    return assignment


def read_compute_nodes_assignment(at_dirpath):
    """..."""
    assignment_h5, dataset = COMPUTE_NODE_ASSIGNMENT

    if not (at_dirpath/assignment_h5).exists():
        raise RuntimeError(f"No compute node assignment saved at {at_dirpath}")

    return pd.read_hdf(at_dirpath / assignment_h5, key=dataset)


#+end_src
** 3. group compute nodes into submissions

There may be a limit on the number of submissions to the compute-cluster's queue, requiring us to define another level of parallelization,

#+name: develop-parallelization-batched-inputs-group-launchscripts
#+begin_src python
def group_launchscripts(compute_nodes, max_entries):
    """..."""
    submit = lambda compute_node: int(compute_node / max_entries)
    return compute_nodes.apply(submit).rename("submission")

#+end_src


** Test-develop

* Inputs to parallelize

In ~connsense-TAP~ we develop the concept of a ~unit-computation~. A single ~unit-computation~ will run as a single process from ~connsense-TAP~'s viewpoint. The compuation's method might have it's own mind, and even run it's own implementation of a muliticore process. This can be setup using ~number_tasks_per_compute_node=1~ in the ~runtime-config~. This allows for an external parallelization scheme that ~connsense-TAP~'s parallelization model does not allow. To implement such a feature, we will have to manipulate the ~connsense-TAP-index~ that we will demonstrate in one of the computations implemented for ~connsense-TAP~. Here, let us first define a ~unit-computation~.

For a given ~computation~, it's ~unit-computation~ is configured as the config section entry ~inde  x~. If an ~index~ is not configured, the configured ~input~ will be used.
#+name: develop-parallelization-inputs-index
#+begin_src python
def read_index(of_computation, in_config):
    """..."""
    LOG.info("READ index of computation %s", of_computation)
    parameters = parameterize(*describe(of_computation), in_config)

    try:
        return parameters["index"]
    except KeyError as missing_index:
        LOG.info("No index configured for computation %s: \n%s",
                 of_computation, missing_index)
        try:
            LOG.info("read index from the configured input.")
            return parameters["input"]
        except KeyError as missing_input:
            LOG.info("Neither an index, nor inputs were configured for computation %s",
                     of_computation)
            raise NotConfiguredError("%s `input` was not configured %s") from missing_input
    raise RuntimeError("Python executtion must not reach here.")


def index_inputs(of_computation, in_tap):
    """..."""
    index_vars = read_index(of_computation, in_tap._config)

    if len(index_vars) > 1:
        return pd.MultiIndex.from_product([to_tap.subset_index(var, values)
                                           for var, values in index_vars.items()])

    var, values = next(iter(index_vars.items()))
    return pd.Index(to_tap.subset_index(var, values))


def slice_units(of_computation, in_tap):
    """..."""
    unit_computations = input_units(of_computation, in_tap)
    return [unit_computations[s:s+1] for s in range(0, len(unit_computations))]

#+end_src

If all input ~unit-computations~ could be determined from the configured ~index~, we would not have to open up the data of inputs just to batch them to setup parallel runs. This is possible if each computation's input data is available in the HDFstore. However, certain computations' input data may be a transform of a pervious ~connsense-TAP~ step and too large to save to the store. There is no reason to save these transformations in H5, as we can just transform the input before sending it to the computation run.

While the inputs' index is read from config section ~index~, inputs are listed in section ~input~. An input may be the circuit to analyze, or a connectome of that circuit. Such inputs are not computed by ~connsense-TAP~ --- they are entered as strings in the config. However an input can be specified as a dataset that is a result of another ~connsense-TAP-computation~. We can configure analyses that transform such  ~connsense-dataset-inputs~. Transformation of inputs allows for analyses with statistical controls.

We will need to map each entry in a ~computation~'s ~inputs-index~ to an ~input-value~. The circuit and connectome can be provided by just reading the config. Let us make a device that delivers ~connsense-dataset-inputs~.
#+name: develop-parallelization-inputs-filter-datasets
#+begin_src python
def filter_datasets(described):
    """..."""
    return {var: val for var, val in described.items()
            if (var not in ("circuit", "connectome") and isinstance(val, Mapping)
                and any(dataterm in val
                        for dataterm in ("dataset", "datacall", "datajoin", "dataprod")))}
#+end_src

We can load a ~dataset~ from ~connsense-TAP~, and subject it to any configured transformations. There may be too many datasets (one per ~computation-subtarget~), and thus not feasible to load them all to memory. So we load only one at at time.
#+name: develop-parallelization-inputs-dataset-lazy-load
#+begin_src python
def lazily(to_evaluate):
    """..."""
    LOG.info("Evaluate %s lazily", to_evaluate.__name__)
    def evaluate_subtarget(s):
        return lambda: to_evaluate(s)
    return evaluate_subtarget

#+end_src
o instead of a dataset with all it's members loaded, we will return methods ready to do so when needed.

** An algebra to combine ~connsense~ datasets

We cannot register input data for every single computation. For an analysis that takes multiple inputs, we will have to combine the individual datasets into a single frame of inputs, with each row representing a ~connsense-unit-computation~. Here we develop an algebra of methods to combine ~connsense-datasets~, that we will use to load input-datasets for ~connsense-computations~.

We can deduce the ~connsense-datatype~ for named description read from a ~connsense-config~,
#+name: develop-parallelization-inputs-variable-load-dataset-0
#+begin_src python :noweb-ref load-dataset
def load_dataset(tap, variable, values):
    """...Load a configured `computation-variable` from `connsense-TAP`
       values: as configured
    """
    properties = values.get("properties", None)

    def unpack_value(v):
        """..."""
        try:
            return v()
        except TypeError:
            pass

        try:
            get = v.get_value
        except AttributeError:
            return v

        data = get()
        return data if properties is None else data[properties]

    try:
        dset = values["dataset"]
    except KeyError:
        try:
            to_call = values["datacall"]
        except KeyError:
            try:
                to_join = values["datajoin"]
            except KeyError:
                try:
                    to_prod = values["dataprod"]
                except KeyError:
                    raise ValueError("values need to define either"
                                     " dataset, datacall, datajoin, or dataprod")
                else:
                    dataset = cross_datasets(tap, variable, to_prod)
            else:
                dataset = mix_datasets(tap, variable, recipe=to_join)
        else:
            dataset = brew_dataset(tap, variable, to_call)
    else:
        LOG.info("Pour %s dataset: \n%s", variable, dset)
        #columns = values.get("columns", None)
        dataset = tap.pour_dataset(*dset).apply(lazily(to_evaluate=unpack_value))

    if isinstance(dataset, pd.DataFrame):
        return (pd.Series([r for _, r in dataset.iterrows()], index=dataset.index)
                .apply(DataCall))

    try:
        values_reindex = values["reindex"]
    except KeyError:
        pass
    else:
        dataset = reindex(tap, dataset, values_reindex)

    try:
        subset = values["subset"]
    except KeyError:
        return dataset
    return dataset.loc[subset]


def bind_belazy(call):
    """..."""
    def get_value(belazy):
        """..."""
        def f(): raise TypeError("not a callable, i am")
        f.get_value = lambda : call(belazy.get_value())
        return f
    return get_value


def bind_lazy(call, **kwargs):
    """..."""
    def datacall(in_store):
        return DataCall(in_store, transform=(call, kwargs))
    return datacall
#+end_src

where we see four ~connsense-datatypes~, ~dataset~, ~datacall~, ~datajoin~, and ~dataprod~. These ~connsense-datatypes~ or ~datacombinators~ will be used to mix individual ~connsense-datasets~ argued to these ~datacombinators~. However, for some computations we may need data for the entire circuit, but not the circuit. For example we have to make summaries over all source cells in the circuit to compute a ~flatmap-column~'s cell's white-matter innervation. In such cases we can have a special ~connsense-datatype~ that implies data for the whole circuit, with an appropriate index.

*** circuit
We can get circuit data from the circuit object directly. This can be useful when we want to measure a quantity for each ~circuit-subtarget~ that is defined in reference to the whole circuit. For example, to measure the white-matter innervation of cortical cells in a ~flatmap-column~ we will need input cells from entire circuit. We can
#+begin_src yaml
summarize_sources:
  dataset: ["circuit-node-populations", "default"]
  columns: ["layer", "mtype"]
#+end_src

This will delegate the responsibility of interpreting the reference ~(circuit-node-populations, default)~ as a dataset to ~connsense-tap~. For the scientist, the reference should make sense in the context of the extract ~connsense-dataset~ of ~(extract-node-populations, default)~. The two datasets contain the same format of the data, except that the extraction ~dataset~ is per ~subtarget~ while the circuit's ~dataset~ for the whole circuit. We will implement the obvious thing in ~tap~. The result should be indexed by ~circuit-id~, either a series or frame depending if a single or more ~properties~ configured.

There are other possibilities. One would be to define a ~connsense-datatype~ to tag datasets that are defined for the whole circuit. This data will be outside the control of ~connsense~, /i.e./ ~connsense~ will not be responsbile for storing and indexing this data. So in a config,
#+begin_src yaml
summarize_sources:
  circuit:
    nodes:
      population: "default"
      group: null
      properties: ["layer", "mtype"]
#+end_src

will tell ~connsense~ to load the SONATA like config. The data implied by the config above is a reference to circuit's nodes. However we may have more than one circuits in ~connsense~ -- thus the implied dataset should be indexed by ~circuit-id~.

#+name: circuit-datasets
#+begin_src jupyter-python :noweb-ref load-dataset
def check_circuit_dataset(tap, spec):
    """Check if a configured dataset is to be retrieved from a circuit."""
    try:
        circuit_dataset = spec["circuit"]
    except KeyError:
        return None
    return load_circuit_dataset(tap, spec)


def load_circuit_dataset(tap, spec):
    """Load a dataset from the circuit's in connsense-config."""
    nodes = (load_circuit_nodes(tap, **spec) if (p:=spec.get("nodes", None))
             else None)
    edges = (load_circuit_edges(tap, population=p) if (p:=spec.get("edges", None))
             else None)
    circuit = {"nodes": nodes, "edges": edges}
    return {arte: fact for arte,fact in circuit.items() if fact is not None}


def load_circuit_nodes(tap, *, population="", group="default",
                       circuit_type=bluepy.Circuit):
    """Load a population of nodes from each circuit on connsense-config."""
    if issubclass(circuit_type, bluepy.Circuit):
        return (tap.create_index("circuit")
                .apply(lambda c: input_circuit(c, tap._config)))

    varpaths = tap._config["path"]["circuit"]
    variants = tap.create_index("circuit").apply(lambda c: varpaths[c])
    return variants.apply(lambda variant:
                          circuit_type(Path(variant), population, group))


def load_circuit_edges(*args, **kwargs):
    """...Warn that this method may never be implemented."""
    raise NotImplementedError("Edges of a circuit may be too many to use a dataframe.")
#+end_src


We should also notice another possibilitu that ~connsense~ already allows. We can configure a ~datacall~ that will be run;. A ~connsense-datacall~ is basically ~connsense-computation~ which is the building block of connsense.
#+begin_src yaml
summarize_sources:
  datacall:
    input:
      subtarget:
        dataset: ["define-subtargets", "Flatmap_column"]
      circuit:
        - "Bio_M"
    recipe:
      source: "path-to-python-file"
      method: "that_provides_a_dataframe_of_annotations_to_summarize_sources.py"
#+end_src
which will allow us to provide special methods to customize inputs to our analyses beyond what the ~connsense-index~ allows us to do. However, the details are hidden away in the method called. If properly documented ~connsense-doc~ can even load the ~docstrings~ of the reported methods and build a long description of the pipeline.

*** dataset
A ~connsense-dataset~ must be in the ~TapStore~,

#+name: connsense-dataset
#+begin_src jupyter-python
def check_dataset(tap, reference):
    """Check if reference are a dataset in tap."""
    try:
        dset = reference["dataset"]
    except KeyError:
        return None
    return tap.pour_dataset(*dset)
#+end_src

*** datacall
A ~connsense-datacall~ will create a ~dataset~ on a method-call,
#+name: connsense-datacall
#+begin_src jupyter-python
def check_datacall(tap, reference):
    """Check if reference are a datacall in tap."""
    try:
        dcall = reference["datacall"]
    except KeyError:
        return None
    return brew_dataset(tap, variable, values)


#+end_src

#+name: develop-parallelization-inputs-variable-load-dataset
#+begin_src jupyter-python :noweb yes
<<develop-parallelization-inputs-variable-load-dataset-0>>

def brew_dataset(tap, variable, call):
    """..."""
    in_store = pour(tap, call["input"]).apply(lazy_keyword).apply(DataCall)
    _, recipe = plugins.import_module(call["recipe"])
    return in_store.apply(bind_lazy(call=recipe, **call["kwargs"]))

def mix_datasets(tap, variable, recipe):
    """..."""
    how = recipe.get("how", "cross")
    dsets = [(var, load_dataset(tap, var, values)) for var, values in recipe.items()
             if var!= "how"]

    assert len(dsets) > 0
    assert isinstance(dsets[0][1], pd.Series)
    if len(dsets) == 1:
        to_include_varname = {varid: dsets[0][0] + '_' + varid
                              for varid in dsets[0][1].index.names
                              if varid not in ("circuit_id", "connectome_id")}
        dataset = dsets[0][1].copy()
        dataset.index.rename(to_include_varname, inplace=True)
        return dataset

    assert len(dsets) == 2

    assert isinstance(dsets[1][1], pd.Series)
    assert dsets[0][1].name == dsets[1][1].name
    quantity = dsets[0][1].name

    assert dsets[1][1].index.names == dsets[0][1].index.names

    def merge(d0, d1):
        assert "circuit_id" not in d0.index.names
        assert "connectome_id" not in d0.index.names
        idxvars = d0.index.names

        dd = (pd.merge(d0.reset_index(), d1.reset_index(),
                       suffixes=("_"+n for n,_ in dsets), how=how)
              .rename(columns={c+'_'+n:n+'_'+c for c in idxvars for n,_ in dsets}))
        return (dd.set_index([n+'_'+c for c in idxvars for n,_ in dsets])
                .rename(columns={quantity+'_'+n: n for n,_ in dsets}))
                #.rename(columns={quantity+'_'+n: n+'_'+quantity for n,_ in dsets}))

    try:
        circuits_0 = dsets[0][1].index.get_level_values("circuit_id").unique()
    except KeyError:
        circuits = None
    else:
        circuits_1 = dsets[0][1].index.get_level_values("circuit_id").unique()
        circuits = circuits_0.intersection(circuits_1)

    if circuits is None:
        return merge(dsets[0][1], dsets[1][1])

    def merge_circuit(c):
        d0 = dsets[0][1].xs(c, level="circuit_id")
        d1 = dsets[1][1].xs(c, level="circuit_id")
        try:
            connectomes_0 = d0.index.get_level_values("connectome_id").unique()
        except KeyError:
            return merge(d0, d1)
        connectomes_1 = d1.index.get_level_values("connectome_id").unique()
        connectomes = connectomes_0.intersection(connectomes_1)

        def merge_connectome(x):
            d0_no_conn = d0.xs(x, level="connectome_id")
            d1_no_conn = d1.xs(x, level="connectome_id")
            return merge(d0_no_conn, d1_no_conn)

        merged = pd.concat([merge_connectome(x) for x in connectomes], axis=0,
                           keys=connectomes, names=["connectome_id"])

    merged = pd.concat([merge_circuit(c) for c in circuits], axis=0,
                       keys=circuits, names=["circuit_id"])

    indices = (merged.index.names[1:] + ["circuit_id"]
               if "connectome_id" not in merged.index.names else
               merged.index.names[2:] + ["circuit_id", "connectome_id"])

    return merged.reorder_levels(indices).apply(lambda r: r.to_dict(), axis=1).rename(variable)


def cross_datasets(tap, variable, recipe):
    """..."""
    assert isinstance(recipe, list)
    assert len(recipe) == 2

    dsets = [load_dataset(tap, variable, values) for values in recipe]

    assert len(dsets) == 2

    assert isinstance(dsets[0], pd.Series)
    assert isinstance(dsets[1], pd.Series)
    assert dsets[0].name == dsets[1].name
    quantity = dsets[0].name

    assert dsets[1].index.names == dsets[0].index.names

    def merge(d0, d1):
        assert "circuit_id" not in d0.index.names
        assert "connectome_id" not in d0.index.names
        idxvars = d0.index.names

        dd = pd.merge(d0.reset_index(), d1.reset_index(), how="cross")
        mdd = dd.set_index([f"{c}_x" for c in idxvars] + [f"{c}_y" for c in idxvars])

        if len(idxvars) == 1:
            mdd.index = pd.Index(mdd.index.values, name=idxvars[0])
        else:
            idxframe = pd.DataFrame({var: (mdd.index.to_frame()[[f"{var}_x", f"{var}_y"]]
                                           .apply(tuple, axis=1))
                                     for var in idxvars})
            mdd.index = pd.MultiIndex.from_frame(idxframe)

    try:
        circuits_0 = dsets[0].index.get_level_values("circuit_id").unique()
    except KeyError:
        circuits = None
    else:
        circuits_1 = dsets[0].index.get_level_values("circuit_id").unique()
        circuits = circuits_0.intersection(circuits_1)

    if circuits is None:
        return merge(dsets[0], dsets[1])

    def merge_circuit(c):
        d0 = dsets[0].xs(c, level="circuit_id")
        d1 = dsets[1].xs(c, level="circuit_id")
        try:
            connectomes_0 = d0.index.get_level_values("connectome_id").unique()
        except KeyError:
            return merge(d0, d1)
        connectomes_1 = d1.index.get_level_values("connectome_id").unique()
        connectomes = connectomes_0.intersection(connectomes_1)

        def merge_connectome(x):
            d0_no_conn = d0.xs(x, level="connectome_id")
            d1_no_conn = d1.xs(x, level="connectome_id")
            return merge(d0_no_conn, d1_no_conn)

        merged = pd.concat([merge_connectome(x) for x in connectomes], axis=0,
                           keys=connectomes, names=["connectome_id"])

    merged = pd.concat([merge_circuit(c) for c in circuits], axis=0,
                       keys=circuits, names=["circuit_id"])

    indices = (merged.index.names[1:] + ["circuit_id"]
               if "connectome_id" not in merged.index.names else
               merged.index.names[2:] + ["circuit_id", "connectome_id"])

    return merged.reorder_levels(indices).apply(lambda r: (r["x"], r["y"]), axis=1).rename(variable)

#+end_src

While we can ~pour~ ~datasets~ that are already in ~connsense-TAP-HDFStore~, sometimes we may want to define a ~dataset~ that we do not want to keep in ~connsense-TAP-HDFStore~, but use as input in another --- /i.e./ we want to ~brew~ a fresh ~dataset~,

Let us use Haskel notation to understand the types we want to manipulate for chained function calls.
#+begin_src haskell

load_dataset Type => :: TapStore -> Variable -> Value[Type] -> [-> DataSet[Type]]

argue_keywords :: Map[String -> (-> DataSet)] -> [-> (String -> DataSet)]
#+end_src
#+name: develop-parallelization-inputs-lazy-dataset-keywords
#+begin_src python
def lazy_keyword(input_datasets):
    """...Repack a Mapping[String->CallData[D]] to CallData[Mapping[String->Data]]
    """
    def unpack(value):
        """..."""
        if callable(value):
            return value()

        try:
            get_value = value.get_value()
        except AttributeError:
            pass
        else:
            return get_value()

        return value

    return lambda: {var: unpack(value) for var, value in input_datasets.items()}

#+end_src

We can load datasets for all the input variables in a computation.
#+name: develop-parallelization-inputs-pour-datasets-cross
#+begin_src python

def pour_cross(tap, datasets):
    """...develop a version of pour that can cross datasets.
    The levels of circuit_id and connectome_id in the right dataset will be dropped,
    assuming that the dataset has already been filtered to contain circuit_id and connectome_id
    that are present in the left dataset.
    """
    LOG.info("Pour cross product of tap \n%s\n values for variables:\n%s",
             tap._root, pformat(datasets))
    dsets = sorted([(variable, load_dataset(tap, variable, values))
                    for variable, values in datasets.items()],
                   key=lambda x: len(x[1].index.names), reverse=True)

    assert len(dsets) == 2, f"Cross can only be between two datasets. Provided: {len(dsets)}"

    assert dsets[0][1].index.names == dsets[1][1].index.names, "Index variables must be the same."
    circargs = [level for level in dsets[0][1].index.names if level in ("circuit_id", "connectome_id")]

    def prefix_dset(name, to_index):
        return {varid: f"{name}_{varid}" for varid in to_index.names if varid not in circargs}

    left_name = dsets[0][0]
    left = dsets[0][1].rename(left_name)
    left_index = prefix_dset(left_name, left.index)
    LOG.info("rename left index to:\n%s", pformat(left_index))
    left.index.rename(left_index, inplace=True)

    right_name = dsets[1][0]
    right = dsets[1][1].rename(right_name)
    right_index = prefix_dset(right_name, right.index)
    LOG.info("rename right index to:\n%s", pformat(right_index))
    right.index.rename(right_index, inplace=True)

    return (pd.merge(left, right, left_index=True, right_index=True)
            .reorder_levels(list(left_index.values()) + list(right_index.values()) + circargs)
            .apply(lambda row: row.to_dict(), axis=1))

#+end_src


#+header: :noweb yes
#+name: develop-parallelization-inputs-pour-datasets
#+begin_src python
<<develop-parallelization-inputs-pour-datasets-cross>>

def pour(tap, datasets):
    """..."""
    LOG.info("Pour tap \n%s\n to get values for variables:\n%s",
             tap._root, pformat(datasets))

    dsets = sorted([(variable, load_dataset(tap, variable, values))
                    for variable, values in datasets.items()],
                   key=lambda x: len(x[1].index.names), reverse=True)

    def rename_series(dset, data):
        try:
            rename = data.rename
        except AttributeError:
            LOG.warning("Dataset reference %s does not point to a pandas.Series", dset)
            return data
        return rename(dset)

    primary = rename_series(*dsets[0])

    if len(dsets) == 1:
        return (primary.apply(lambda value: {dsets[0][0]: value})
                if isinstance(primary, pd.Series) else
                primary.apply(lambda row: row.to_dict(), axis=1))

    common_index = dsets[0][1].index.names

    def rename_index(dset, data):
        if not (join_index := datasets[dset].get("join_index", None)):
            return rename_series(dset, data)

        data = data.droplevel([varid for varid, revarid in join_index.items()
                               if revarid.lower() == "drop"])
        data.index.rename({varid: revarid for varid, revarid in join_index.items()
                           if revarid and revarid.lower() != "drop"}, inplace=True)
        return rename_series(dset, data)

    def merge_with(leading, dset, data):
        reindexed = rename_index(dset, data)
        return pd.merge(leading, reindexed, left_index=True, right_index=True)

    leading = primary
    for dset, data in dsets[1:]:
        leading = merge_with(leading, dset, data)
    return leading.apply(lambda row: row.to_dict(), axis=1).reorder_levels(common_index)

    # def reindex (dset):
    #     in_dset = dset[1].index.names
    #     not_in_dset = [n for n in primary.index.names if n not in in_dset]
    #     return (rename_index(*dset)
    #             .reindex(primary.reorder_levels(in_dset + not_in_dset).index)
    #             .reorder_levels(primary.index.names))

    # return (pd.concat([reindex(dset) for dset in dsets], axis=1,
    #                   keys=[name for name, _ in dsets])
    #         .apply(lambda row: row.to_dict(), axis=1))

#+end_src

#+RESULTS: develop-parallelization-inputs-pour-datasets
: None

If a ~transformation~ has been specified, we need to apply it. What should we expect of a ~dataset-transformation~?
If a ~transformation~ has been specified, we need to apply it. What should we expect of a ~dataset-transformation~?
Let us consider specific cases that we want.

** Reindex the input
A ~connsense-TAP-dataset~ is saved as a ~pandas.Series~ indexed by the ~computation-subtargets~ that the dataset applies to. For example for the SSCx-Portal we have saved a sample of connections for each pathway in each central-column. In ~connsense-TAP~ this data is saved per ~subtarget-id, circuit-id, connectome-id~ as a ~pandas.DataFrame~ containing values of the source and target ~node-id~. We want to compute PSP traces for each    connection, but grouped by their pathway. Within ~connsense-TAP~ we can reindex the data, concatenating with additional levels for the source and target pathways. The scheme to reindex can be specified in the config,
#+begin_src yaml
input:
  connections:
    dataset: ["analyze-physiology", "psp/connections"]
    reindex:
      source_mtype: "mtype"
      target_mtype: "mtype"
#+end_src

We can use the configured information to reindex a ~connsense-TAP-dataset~. A ~connsense-TAP-dataset~ values are always held as a ~pandas.Series~ of lazy data --- data-methods that can be loaded with ~()~ operator to load the data of a single ~computation-subtarget~. For reindexing, we do not need to load the data value, but just manipulate the index.
#+name: develop-parallelization-inputs-reindex-datasets
#+begin_src python

def reindex(tap, inputs, variables):
    """..."""
    connsense_ids = {v: tap.index_variable(value) for v, value in variables.items()}
    def index_ids(subtarget):
        return pd.MultiIndex.from_frame(pd.DataFrame({f"{var}_id": connsense_ids[var].loc[values.values].values
                                                      for var, values in subtarget.index.to_frame().items()}))
    reinputs = inputs.apply(lambda s: pd.DataFrame(s()).set_index(index_ids(subtarget=s())))
    frame = pd.concat(reinputs.values, keys=reinputs.index)
    groups_of_inner_frames = list(frame.groupby(frame.index.names))
    return (pd.Series([d.reset_index(drop=True) for _, d in groups_of_inner_frames],
                      index=pd.MultiIndex.from_tuples([i for i,_ in groups_of_inner_frames],
                                                      names=frame.index.names))
            .apply(DataCall))


#+end_src

** Controls for inputs
An analysis is incomplete without controls. Controls can be chosen to see if any ~pheneomenon/quantity~ signficantly stands out. We see high ~simplex-counts~ in the ~flatmap-columns~, but are these values signficantly different from those in an Erdos-Renyi graph of the same size? Properly chosen controls can provide a hierarchy of models of increasing complexity that progressively approximate a ~phenomenon/quantity~'s value in the circuit.

Controling an analysis' input will proceed by transforming each input according to the configured ~controls~. There may be more than ~dataset~ inputs to an analysis method, which will have a common index. Each of the ~control~ method should also respond to the full input of the analysis method.
#+begin_src yaml
simplex-counts:
  description: >-
     Number of simplices by dimension.

  index:
    subtarget:
      dataset: ["define-subtargets", "flatmap-columns"]
    circuit:
      "Bio_M"
    connectome:
      "local"

  input:
    adjacency:
      dataset: ["extract-edge-populations", "local"]
    node_properties:
      dataset: ["extract-node-populations", "default"]


  controls:
    description: >-
      Each other entry than this description must be a statistical control to apply to the input's
      `(adjacency, node_properties)` to the computation of `simplex-counts`. The listed method will
      be called by the full input of the analysis method.

    erdos-renyi:
      description: >-
        Erdos-Renyi shuffle of edges.
      seeds: [0, 1, 2, 3, 4]
      algorithm:
        source: "/gpfs/bbp.cscs.ch/project/proj83/analyses/topological-analysis-subvolumes/proj83/connectome_analysis/library/randomization.py"
        method: "ER_shuffle"

    dd2:
      description: >-
        Randomize edges by a 2nd order distance dependent model.
      tap_datasets:
        model_params_dd2: ["analyze-connectivity", "model_params_dd2"]
      seeds: [0, 1, 2, 3, 4]
      algorithm:
        source: "/gpfs/bbp.cscs.ch/project/proj83/analyses/topological-analysis-subvolumes/proj83/connectome_analysis/library/randomization.py"
        method: "run_DD2_model"

#+end_src
We can take an explicit approach, and code separate behaviour for each type of transformation we want to use in our analyses.
#+name: develop-parallelization-inputs-control-datasets
#+begin_src python
def load_control(transformations, lazily=True):
    """..."""
    def load_config(control, description):
        """..."""
        LOG.info("Load configured control %s: \n%s", control, pformat(description))

        _, algorithm = plugins.import_module(description["algorithm"])

        seeds = description.get("seeds", [0])

        try:
            to_tap = description["tap_datasets"]
        except KeyError:
            LOG.info("No tap datasets for control: \n%s", description)
            to_tap = None

        kwargs = description.get("kwargs", {})

        def seed_shuffler(s):
            """..."""
            def shuffle(inputs):
                """..."""
                if lazily:
                    return lambda: algorithm(**inputs(), seed=s, **kwargs)
                return algorithm(**inputs, seed=s, **kwargs)
            return (f"{control}-{s}", shuffle, to_tap)
        return [seed_shuffler(s) for s in seeds]

    controls = {k: v for k, v in transformations.items() if k != "description"}
    return [shfld for ctrl, cfg in controls.items() for shfld in load_config(ctrl, cfg)]

#+end_src

*** Test develop

We have setup a workspace to compute simplex-counts, with a config based on our discussion. Let us see what inputs we can load for ~simplex-count~ analysis.
#+begin_src jupyter-python :tangle develop_parallelization.py
computation = "analyze-connectivity/simplex-counts"
params = devprl.parameterize(*devprl.describe(computation), topaz._config)

print("inputs for simplex-counts:\n")
pprint({k: v for k, v in params["input"].items() if k not in ("transformations", "slicing")})

print("\nwith transformations\n")
pprint(params["input"]["transformations"])
#+end_src

We can pass these ~config~ to ~connsense-TAP~ to generate the inputs,
#+begin_src jupyter-python :tangle develop_parallelization.py
inputs = devprl.generate_inputs(computation, topaz._config)

display(inputs)
#+end_src


The inputs have 11 times the number of ~flatmap-column~s, 1 for the original, 5 per original for ~erodos-renyi~ and ~dd2-model~ each,
#+begin_src jupyter-python :tangle develop_parallelization.py
subtargets_per_control = inputs.groupby("control").size()
display(subtargets_per_control)
#+end_src

#+RESULTS:
#+begin_example
control
dd2-0            239
dd2-1            239
dd2-2            239
dd2-3            239
dd2-4            239
erdos-renyi-0    239
erdos-renyi-1    239
erdos-renyi-2    239
erdos-renyi-3    239
erdos-renyi-4    239
original         239
dtype: int64
#+end_example

Each entry amont ~inputs~ contains a ~datacall~ that will return the configured inputs, in the case of ~simplex-counts~ a ~mapping~ that provides values for ~adjacency~ and ~node_properties~ that the configured computation method expects,
#+begin_src jupyter-python :tangle develop_parallelization.py
input_quantities_0 = inputs.iloc[0]()
for variable, value in input_quantities_0.items():
    print("input %s: \n%s"%(variable, type(value)))
#+end_src

#+RESULTS:
: input adjacency:
: <class 'scipy.sparse.csr.csr_matrix'>
: input node_properties:
: <class 'pandas.core.frame.DataFrame'>

** Sliced input

The second example is that of analyzing slices of an input --- for example the /intra-layer-subgraphs/. We will be guided by how we have implemented controls.

#+name:  config-simplex-counts
#+begin_src yaml
simplex-counts:
  description: >-
     Number of simplices by dimension.

  index:
    subtarget:
      dataset: ["define-subtargets", "flatmap-columns"]
    circuit:
      "Bio_M"
    connectome:
      "local"

  input:
    adjacency:
      dataset: ["extract-edge-populations", "local"]
    node_properties:
      dataset: ["extract-node-properties", "default"]

    transformations:
      description: >-
        Transformations are configured by their type. Each type of transformation may contain several inidividual definitions. Transformations will be applied in sequence to each original input. A given transformation such as a randomization may produce more than one output for a single input. Subsequent transformations will be applied to each of it's output.  The result will be an input-dataset containing an additional level for each

    slicing:
      description: >-
        Configure `do-full: true` to run the analyses on the full subtarget as a separate dataset than the slices. If `false`, analyses will not be run for full. If you do not want to analyze slices, then
        remove this section.
      do-full: true
      layer:
        description: >-
          Intralayer subgraphs.
        slices:
          layer: [1, 2, 3, 4, 5, 6]
        algorithm:
          source: "/gpfs/bbp.cscs.ch/project/proj83/analyses/topological-analysis-subvolumes/proj83/connectome_analysis/library/topology.py"
          method: "subgraph_intralayer"
      interlayer:
        description: >-
          Interlayer subgraphs.
        slices:
          source_layer: [1, 2, 3, 4, 5, 6]
          target_layer: [1, 2, 3, 4, 5, 6]
        algorithm:
          source: "/gpfs/bbp.cscs.ch/project/proj83/analyses/topological-analysis-subvolumes/proj83/connectome_analysis/library/topology.py"
          method: "subgraph_interlayer"

#+end_src

Above we have configured two slicings for ~simplex-counts~ analysis. There is an ~algorithm~ that specifies the computation to slice, and a specification of what to slice. Specification of ~slices~ is expected to be ~mapping~ from ~kwargs~ that make sense to the ~algorithm~'s computation to their values. The specified values will be used ~connsense-TAP~ to create the ~algorithm~'s ~kwargs~. In the case of ~intralayer~, there will be six slices, one each for the mentioned layer. If we want only a handful of slices, writing them explicitly in the config is the most efficient way. Otherwise we can refer to a ~connsense-dataset~,
#+name: config-simplex-counts-with-mtype-slicing
#+begin_src yaml :noweb yes
<<config-simplex-counts>>
      mtype:
        description: >-
          Intra-mtype slicing.
        slices:
          mtype:
            dataset: ["extract-node-types", "biophysical/mtype"]
        algorithm:
            source: "/gpfs/bbp.cscs.ch/project/proj83/analyses/topological-analysis-subvolumes/proj83/connectome_analysis/library/topology.py"
            method: "subgraph_intralayer"
#+end_src

Slicing by ~mtype~ does not make much biological sense, and is a /subset/ of slicing by ~pathway~,
#+name: config-simplex-counts-with-pathway-slicing
#+begin_src yaml :noweb yes
<<config-simplex-counts-with-mtype-slicing>>
      pathway:
        description: >-
          Slice subtargets by mtype --> mtype pathway.
        slices:
          sources:
            dataset: ["extract-node-types", "biophysical/mtype"]
          targets:
            dataset: ["extract-node-types", "biophysical/mtype"]
        algorithm:
            source: "/gpfs/bbp.cscs.ch/project/proj83/analyses/topological-analysis-subvolumes/proj83/connectome_analysis/library/topology.py"
            method: "subgraph_pathway"
#+end_src

which will generate computations of ~simplex-counts~ for each ~mtype --> mtype~ pathway for each if the inputs, and their controls.

To implement an ~loader~ for such a prescription of slicing, we will start by parsing the ~slices~ of a ~slicing~.
#+name: develop-parallelization-inputs-parse-slicing
#+begin_src python

def parse_slices(slicing):
    """..."""
    from itertools import product
    def prepare_singleton(slicespec):
        """..."""
        assert len(slicespec) == 2, "must be a tuple from a dict items"
        key, values = slicespec
        if isinstance(values, list):
            return ((key, value) for value in values)
        if isinstance(values, Mapping):
            if len(values) == 1:
                innerkey, innervalues = next(iter(values.items()))
                if not isinstance(innervalues, list):
                    innervalues = [innervalues]
                return ((key, {innerkey: val}) for val in innervalues)
            innerdicts = product(*(s for s in (prepare_singleton(slicespec=s)  for s in values.items())))
            return ((key, dict(dvalue)) for dvalue in innerdicts)

        return ((key, value) for value in [values])

    slicing = slicing["slices"].items()
    if len(slicing) == 1:
        return (dict([s]) for s in prepare_singleton(next(iter(slicing))))

    slices = product(*(singleton for singleton in (prepare_singleton(slicespec=s) for s in slicing)))
    return (dict(s) for s in slices)

#+end_src

which we can use to collect

#+name: develop-parallelization-inputs-subset-datasets
#+begin_src python :noweb yes
<<develop-parallelization-inputs-parse-slicing>>

def flatten_slicing(_slice):
    """..."""
    def denest(key, value):
        if not isinstance(value, dict):
            return value
        return {f"{key}_{innerkey}": denest(innerkey, innervalue)
                for innerkey, innervalue in value.items()}
    if len(_slice) == 1:
        key, value = next(iter(_slice.items()))
        return {key: denest(key, value)}
    flat = {}
    for var, values in _slice.items():
        denested = denest(var, values)
        flat.update(denested)
    return flat


def load_slicing(transformations, using_tap=None, lazily=True, **circuit_args):
    """..."""
    from copy import deepcopy

    LOG.info("Load slicing %s transformations for circuit args: \n%s",
             len(transformations), circuit_args)

    def load_dataset(slicing):
        """..."""
        slicing = deepcopy(slicing)
        slices = slicing["slices"]
        def load_dataset(values):
            """..."""
            if isinstance(values, dict):
                if "dataset" in values:
                    return using_tap.pour_dataset(*values["dataset"]).tolist()
                return {var: load_dataset(vals) for var, vals in values.items()}
            return values
        slicing["slices"] = {variable: load_dataset(values) for variable, values in slices.items()}
        return slicing

    def load(slicing):
        """..."""
        _, algorithm = plugins.import_module(slicing["algorithm"])
        kwargs = slicing.get("kwargs", {})
        slices = list(parse_slices(load_dataset(slicing)))
        def specify(aslice):
            """..."""
            def slice_input(datasets):
                if lazily:
                    assert callable(datasets)
                    return lambda: algorithm(**circuit_args, **datasets(), **aslice,
                                             ,**kwargs)
                assert not callable(datasets)
                return algorithm(**datasets, **aslice, **circuit_args, **kwargs)
            return slice_input
        return pd.Series([specify(aslice) for aslice in slices],
                         index=pd.MultiIndex.from_frame(pd.DataFrame([flatten_slicing(s) for s in slices])))
    return {slicing: load(slicing=s) for slicing, s in transformations.items()}

#+end_src

#+name: develop-parallelization-inputs-subset-datasets-0
#+begin_src python :noweb yes

def load_slicing_0(transformations, using_tap=None, lazily=True):
    """..."""
    from itertools import product

    slicing = {k:v for k, v in transformations.items() if k != "description"}
    def load_config(subset, description):
        """..."""
        LOG.info("Load configured subset %s: \n%s", subset, pformat(description))

        _, algorithm = plugins.import_module(description["algorithm"])

        try:
            slices = description["slices"]
        except KeyError:
            LOG.warning("No slices set for subsetting the inputs.")
            slices = {}

        def label(a_slice):
            """..."""
            if len(a_slice) == 1:
                return "-".join([f"{key}_{value}" for key, value in a_slice.items()])

            source = a_slice["sources"]
            if len(source) == 1:
                source = list(source.values())[0]
            else:
                source = "_".join(source.values())

            target = a_slice["targets"]
            if len(target) == 1:
                target = list(target.values())[0]
            else:
                target = "_".join(target.values())

            return source + "-" + target

        def prepare(slices):
            """..."""
            LOG.info("prepare slices \n%s", slices)
            if not slices:
                return {}

            #assert len(slices) == 1,\
               # f"INPRPGRESS crose product variants specified by {len(slices)} variables"

            if "source_types" in slices:
                assert len(slices) == 2 and "target_types" in slices

                sources = slices["source_types"]
                if "dataset" in sources:
                    assert using_tap
                    dataset = using_tap.pour_dataset(*sources["dataset"])
                    assert isinstance(dataset, pd.Series)
                    source_variable = dataset.name
                    source_values = dataset.values
                else:
                    assert len(sources) == 1
                    source_variable, source_values = list(sources.items())[0]

                targets = slices["target_types"]
                if "dataset" in targets:
                    assert using_tap
                    dataset = using_tap.pour_dataset(*targets["dataset"])
                    assert isinstance(dataset, pd.Series)
                    target_variable = dataset.name
                    target_values = dataset.values
                else:
                    assert len(targets) == 1
                    targets = slices["target_types"]
                    assert len(targets) == 1
                    target_variable, target_values = list(targets.items())[0]

                return ({"sources": {source_variable: s}, "targets": {target_variable: t}}
                        for s in source_values for t in target_values)

            return ({"sources": dict(x), "targets": dict(x)}
                    for x in product(*[[(key, value) for value in slices[key]] for key in slices]))

        kwargs = description.get("kwargs", {})

        def specify(aslice):
            """..."""
            def slice_input(datasets):
                """..."""
                if lazily:
                    assert callable(datasets)
                    return lambda: algorithm(**datasets(), **aslice, **kwargs)
                assert not callable(datasets)
                return algorithm(**datasets, **aslice, **kwargs)
            return (f"{label(aslice)}", slice_input)
        return [specify(aslice) for aslice in prepare(slices)]
    return [slice_ip for aslice, described in slicing.items() for slice_ip in load_config(aslice, described)]


#+end_src

*** TODO
#+begin_src jupyter-python
simplex_counts(subtarget=None, slicing=FULL, controls=ORIGINAL)

simplex_counts.describe("controls")

simplex_counts.available_controls(): #list of keys of available controls
simplex_coounts.available_slicing():  #list of keys of available slicing
#+end_src

** Transformations of inputs -- to remove -- does not apply

Transformations may result in a list of transformed datasets. For example, controlled datasets will result in as many shuffled datasets as configured. The result of transformations will be a /flattened/ concatenation of such individual applications. We can iterate through expected transformations in a config to obtain a final collection of datasets, applying them one at a time in the order specified in the config.

A computation's input datasets will be transformed to return the same datatypes as the input datasets. During parallelization setup we will estimate the computational load of an input, include in the transformed ones. This can be computationally costly if there are many large inputs. Instead of transforming them to estimate the load of the transformed input, we can use the load of the original input.

** A unit of computation.

#+name: develop-parallelization-inputs-unit-computations
#+begin_src python

def input_units(computation, to_tap):
    """..."""
    described = parameterize(*describe(computation), to_tap._config)
    datasets = {variable: apply_transformations(in_values, to_tap, variable,
                                                load_dataset(to_tap, variable, in_values), of_analysis=computation)
                for variable, in_values in filter_datasets(described["input"]).items()}
    return datasets


#+end_src

** Call data
We will use callables that return dataset values,
#+name: develop-parallelization-multinode-datacall
#+begin_src python

class DataCall:
    """Call data..."""
    def __init__(self, dataitem, transform=None, preserves_shape=True, cache=False):
        self._dataitem = dataitem
        self._transform = transform
        self._preserves_shape = preserves_shape
        self._cache = None if not cache else {}

    @lazy
    def dataset(self):
        """..."""
        LOG.warning("This will hold the result of this DataCall. DO NOT USE with a series.apply")
        return self()

    @lazy
    def shape(self):
        """..."""
        LOG.debug("Get shape for DataCall %s, \n\t with dataitem %s", self, self._dataitem)
        if self._transform and self._preserves_shape:
            return self._dataitem.shape

        value = self(to_get="shape")

        if isinstance(value, Mapping):
            value = next(v for v in value.values())

        try:
            return value.shape
        except AttributeError:
            pass

        try:
            length = len(value)
        except TypeError:
            length = 1

        return (length, )

    def transform(self, original=None):
        """..."""
        assert self._transform is not None

        if original is None:
            original = self._cache["original"]

        try:
            transform, kwargs = self._transform
        except TypeError as not_tuple_error:
            try:
                return self._transform(original)
            except TypeError as not_callable_error:
                LOG.error("self._transform should be a callable or tuple of (callable, kwargs)"
                          "not %s\nErrors:\n%s\n%s",
                          type(self._transform), not_tuple_error, not_callable_error)
                raise TypeError("self._transform type %s inadmissible."
                                " Plesase use a (callable, kwargs) or callable"%
                                (type(self._transform),))
            return transform(original, **kwargs)

    def __call__(self, to_get=None):
        """Call Me."""
        LOG.debug("Data called %s. | Has Transform? %s | Will it Cache? %s|", self,
                  self._transform is not None, self._cache is not None)

        if self._cache is not None and "original" in self._cache:
            original = self._cache["original"]
        else:
            try:
                get_value = self._dataitem.get_value
            except AttributeError:
                try:
                    original = self._dataitem(to_get)
                except TypeError:
                    try:
                        original = self._dataitem()
                    except TypeError:
                        original = self._dataitem
            else:
                original = get_value()

            if self._cache is not None:
                self._cache["original"] = original

        if not self._transform:
            return original

        if to_get and to_get.lower() == "shape":
            return original if self._preserves_shape else self.transform(original)

        return self.transform(original)
#+end_src
*** Shape of data under a call.
We cannot make any ~DataCall.shape~ efficient. We implemented ~.shape~ above assuming the case of ~randomizations~ as a ~DataCall._transform~. However a ~DataCall(slicing(randomization))~ causes the randomizaion to be called. In general this is the most generally efficient way.
** Generate input datasets
We will load the inputs as a list ~unit-computationss~ derived from the ~pipeline-config~.
#+name: develop-parallelization-multinode-generate-inputs
#+begin_src python
def generate_inputs(of_computation, in_config, slicing=None, circuit_args=None,
                     datacalls_for_slices=False, **circuit_kwargs):
    """..."""
    from connsense.develop.topotap import HDFStore
    LOG.info("Generate inputs for %s.", of_computation)

    computation_type, of_quantity = describe(of_computation)
    params = parameterize(computation_type, of_quantity, in_config)
    tap = HDFStore(in_config)

    join_input = params["input"].get("join", None)

    if not join_input:
        datasets = pour(tap, filter_datasets(params["input"]))
    elif join_input.lower() == "cross":
        datasets = pour_cross(tap, filter_datasets(params["input"]))
    else:
        raise ValueError(f"Unhandled join of inputs: {join_input}")

    original = datasets.apply(lazy_keyword).apply(DataCall)

    controlled = control_inputs(of_computation, in_config, using_tap=tap)
    if controlled is not None:
        original = pd.concat([original], axis=0, keys=["original"], names=["control"])
        full = pd.concat([original, controlled])
        full = full.reorder_levels([l for l in full.index.names if l != "control"]
                                   + ["control"])
    else:
        full = original

    def index_circuit_args(inputs):
        """..."""
        if circuit_args is None:
            return inputs

        missing = {}
        for var, value in circuit_args.items():
            varid = f"{var}_id"
            idval = tap.index_variable(var, value)

            if varid not in inputs.index.names:
                missing[varid] = idval
            else:
                inputs = inputs.xs(idval, level=varid, drop_level=False)

        #missing = {f"{var}_id": tap.index_variable(var, value)
                   #for var, value in circuit_args.items()
                   #if f"{var}_id" not in inputs.index.names}
        for variable_id, value in missing.items():
            inputs = (pd.concat([inputs], axis=0, keys=[value], names=[variable_id])
                      .reorder_levels(inputs.index.names + [variable_id]))
        return inputs

    if not slicing or slicing == "full":
        LOG.info("generate inputs for slicing None or full %s", slicing)
        return index_circuit_args(full)

    assert slicing in params["slicing"]
    cfg = {slicing: params["slicing"][slicing]}

    if (not datacalls_for_slices
        and (cfg[slicing].get("compute_mode", "EXECUTE") in ("execute", "EXECUTE"))):
        LOG.info("generate inputs for slicing %s with compute mode execute", slicing)
        return index_circuit_args(full)

    LOG.info("generate inputs for slicing %s with compute mode not execute", slicing)
    to_cut = load_slicing(cfg, tap, lazily=False, **circuit_kwargs)[slicing]
    slices = generate_slices(tap, inputs=full, using_knives=to_cut)
    return index_circuit_args(slices)


def generate_slices(of_tap, inputs, using_knives):
    """Generate slices of inputs accoring to configured knives."""
    from tqdm import tqdm; tqdm.pandas()
    def datacut(c):
        return datacall(c, preserves_shape=False)
    slices = pd.concat([inputs.apply(datacut(c)) for c in using_knives], axis=0,
                       keys=using_knives.index)
    n_slice_index = len(slices.index.names) - len(inputs.index.names)
    return slices.reorder_levels(inputs.index.names
                                 + slices.index.names[0:n_slice_index])
#+end_src

#+name: develop-parallelization-multinode-generate-inputs-0
#+begin_src python
def generate_inputs_0(of_computation, in_config):
    """..."""
    from connsense.develop.topotap import HDFStore
    LOG.info("Generate inputs for %s.", of_computation)

    computation_type, of_quantity = describe(of_computation)
    params = parameterize(computation_type, of_quantity, in_config)
    tap = HDFStore(in_config)

    datasets = pour(tap=HDFStore(in_config), datasets=filter_datasets(params["input"]))
    original = datasets.apply(lazy_keyword).apply(DataCall)

    def tap_datasets(for_inputs, and_additionally=None):
        """..."""
        LOG.info("Get input data from tap: \n%s", for_inputs)
        references = deepcopy(for_inputs)
        if and_additionally:
            LOG.info("And additionally: \n%s", and_additionally)
            references.update({key: {"dataset": ref} for key, ref in and_additionally.items()} or {})
        datasets = pour(tap, datasets=references)
        return datasets.apply(lazy_keyword).apply(DataCall)

    def datacall(transformation):
        """..."""
        def transform(dataitem):
            """..."""
            return DataCall(dataitem, transformation)
        return transform

    try:
        randomizations = params["input"]["controls"]
    except KeyError:
        LOG.warning("It seems no controls have been configured for the input of %s", of_computation)
        result = original
    else:
        controls = apply_controls()

        controls = load_control(randomizations, lazily=False)
        if controls:
            for_input = filter_datasets(params["input"])
            controlled = pd.concat([tap_datasets(for_input, and_additionally=to_tap).apply(datacall(shuffle))
                                    for _, shuffle, to_tap in controls], axis=0,
                                keys=[control_label for control_label, _, _ in controls], names=["control"])
            original = pd.concat([original], axis=0, keys=["original"], names=["control"])
            result = pd.concat([original, controlled])
            result = result.reorder_levels([l for l in result.index.names if l != "control"] + ["control"])
        else:
            result = original

    try:
        slicings = params["input"]["slicing"]
    except KeyError:
        LOG.warning("No slicing configured for the input of %s", of_computation)
        return result

    slicing = load_slicing(slicings, lazily=False)
    if slicing:
        sliced = pd.concat([result.apply(datacall(sliced_input)) for _, sliced_input in slicing], axis=0,
                               keys=[slicing_label for slicing_label,_ in slicing], names=["slicing"])
        result = pd.concat([result], axis=0, keys=["full"], names=["slicing"])
        result = pd.concat([result, sliced])
        result = result.reorder_levels([l for l in result.index.names if l != "slicing"] + ["slicing"])

    return result

#+end_src
We setup our analyses suite in a pipeline so that we can use the results of one computation in another. We configure inputs of a ~connsense-computation~ as references to ~connsense-TAP~ datasets. However a ~transformation~ of an input may also use a ~connsense-dataset~. We allow the configuration of ~tap-datasets~ in the section for ~controls~ of an ~analysis. To load the datasets ~connsense-pipeline- will need to,
#+name: develop-parallelization-multinode-generate-inputs-apply-controls
#+begin_src python
def pour_datasets(from_tap, for_inputs, and_additionally=None):
    """..."""
    LOG.info("Get input data from tap: \n%s", for_inputs)
    references = deepcopy(for_inputs)
    if and_additionally:
        LOG.info("And additionally: \n%s", and_additionally)
        references.update({key: {"dataset": ref} for key, ref in and_additionally.items()} or {})
    datasets = pour(from_tap, datasets=references)
    return datasets.apply(lazy_keyword).apply(DataCall)


def datacall(transformation, preserves_shape=True):
    """Apply a transformation, lazily."""
    def transform(dataitem):
        """..."""
        return DataCall(dataitem, transformation, preserves_shape=preserves_shape, cache=False)
    return transform


def control_inputs(of_computation, in_config, using_tap):
    """..."""
    params = parameterize(*describe(of_computation), in_config)
    try:
        randomizations = params["controls"]
    except KeyError:
        LOG.warning("No controls have been configured for the input of %s: \n%s",
                    of_computation, params)
        return None

    controls = load_control(randomizations, lazily=False)
    assert controls, "Cannot be empty. Check your config."

    for_input = filter_datasets(params["input"])
    return pd.concat([pour_datasets(using_tap, for_input,
                                    and_additionally=to_tap).apply(datacall(shuffle))
                      for _, shuffle, to_tap in controls], axis=0,
                     keys=[control_label for control_label, _, _ in controls], names=["control"])

#+end_src
And a similar procedure for
#+name: develop-parallelization-multinode-generate-inputs-slice-controls
#+begin_src python
def slice_inputs(of_computation, in_config, datasets=None, using_tap=None):
    """..."""
    params = parameterize(*describe(of_computation), in_config)
    try:
        configured = params["slicing"]
    except KeyError:
        LOG.warning("It seems no slicing have been configured for the input of %s:\n%s", of_computation, params)
        return (None, None)

    knives = {k: v for k, v in configured.items() if k not in ("description", "do-full")}

    slicing = load_slicing(knives, lazily=False, using_tap=using_tap)
    assert slicing, "Cannot be empty. Check your config."

    for_input = filter_datasets(params["input"])
    if datasets is None:
        assert using_tap
        datasets = pour_datasets(using_tap, for_input)
    return pd.concat([datasets.apply(datacall(cut, preserves_shape=False)) for _, cut in slicing],
                     axis=0, keys=[knife_label for knife_label, _ in slicing], names=["slice"]),

#+end_src
* Multinode parallel computation
A multinode parallel computation can be modeled,
#+begin_src haskell
unit_computation :: subtarget -> measurement

data ComputeNode = ComputeNode { at_path :: Path
                               , to_run :: [unit_computation]
                               } deriving Show

run_multinode :: [UnitComputation] -> Integer -> Integer -> [ComputeNode]

run_multinode unitcomps n_compute_nodes n_parallel_tasks
  = [ComputeNode assignment]
#+end_src

To compute a quantity using multinode parallelization ~connsense-TAP~ will setup a directory structure with one subdirectory for each compute node containing data required for the computation. Each compute node will get it's own list of inputs to run. To launch the runs, a ~connsense-TAP~ will write a ~launchscript.sh~ that we can source in a ~unix-shell~. Once all the compute nodes have been run, the results can be collected using ~connsense-TAP~. To accomplish the task of distributing computations over the compute nodes and collecting the results we define.

The version that sequences slices and full separately.
#+name: develop-parallelization-multinode-setup
#+begin_src python
def setup_multinode(process, of_computation, in_config, using_runtime, *,
                    in_mode=None, njobs_to_estimate_load=None):
    """Setup a multinode process.
    """
    from tqdm import tqdm; tqdm.pandas()
    from connsense.develop.topotap import HDFStore

    n_compute_nodes, n_parallel_jobs, o_complexity =\
        prepare_parallelization(of_computation, in_config, using_runtime)

    def prepare_compute_nodes(inputs, at_dirpath, slicing, output_type, unit_weight=None):
        """..."""
        at_dirpath.mkdir(exist_ok=True, parents=False)
        using_configs = configure_multinode(process, of_computation, in_config, at_dirpath)

        if process == setup_compute_node:
            batched = batch_multinode(
                of_computation, inputs, in_config, at_dirpath,
                unit_weight=unit_weight,
                using_parallelization=(n_compute_nodes, n_parallel_jobs, o_complexity),
                njobs_to_estimate_load=njobs_to_estimate_load
            )
            using_configs["slurm_params"] = (
                configure_slurm(of_computation, in_config, using_runtime)
                .get("sbatch", None)
            )
            compute_nodes = {
                c: setup_compute_node(c, of_computation, inputs, using_configs, at_dirpath,
                                      in_mode=in_mode, slicing=slicing)
                for c, inputs in batched.groupby("compute_node")
            }
            return {"configs": using_configs,
                    "number_compute_nodes": n_compute_nodes,
                    "number_total_jobs": n_parallel_jobs,
                    "setup": write_multinode_setup(compute_nodes, inputs, at_dirpath)}

        if process == collect_results:
            batched = read_compute_nodes_assignment(at_dirpath)
            _, output_paths = read_pipeline.check_paths(in_config, step=computation_type)
            h5_group = output_paths["steps"][computation_type]

            setup = {c: read_setup_compute_node(c, for_quantity=at_dirpath)
                     for c, _ in batched.groupby("compute_node")}
            return collect_results(computation_type, setup, at_dirpath, h5_group, slicing,
                                    output_type=output_type)

        return ValueError(f"Unknown multinode {process}")

    _, to_stage = get_workspace(of_computation, in_config)

    using_configs = configure_multinode(process, of_computation, in_config,
                                        at_dirpath=to_stage)

    computation_type, of_quantity = describe(of_computation)
    params = parameterize(*describe(of_computation), in_config)
    circuit_args = input_circuit_args(of_computation, in_config, load_circuit=False)
    circuit_kwargs = input_circuit_args(of_computation, in_config, load_circuit=True)

    full = generate_inputs(of_computation, in_config, circuit_args=circuit_args, **circuit_kwargs)


    if process == setup_compute_node:
        full_weights = (
            #full.progress_apply(lambda l: estimate_load(to_compute=None)(l())).dropna()
            full.progress_apply(estimate_load(o_complexity)).dropna()
            if not njobs_to_estimate_load
            else multiprocess_load_estimate(o_complexity, full, njobs_to_estimate_load))

        have_zero_weight = np.isclose(full_weights.values, 0.)
        LOG.info("Inputs with zero weight %s: \n%s",
                 have_zero_weight.sum(), full_weights[have_zero_weight])

        full = full[~have_zero_weight]
        full_weights = full_weights[~have_zero_weight]
        max_weight = full_weights.max()
    else:
        max_weight = None

    full_path, full_slicing = ((to_stage, None) if "slicing" not in params else
                               (to_stage/"full", "full"))
    compute_nodes = {"full": prepare_compute_nodes(full, full_path, full_slicing,
                                                   params["output"], max_weight)}

    if "slicing" not in params:
        return compute_nodes

    exclude = ("description", "do-full")
    cfg_slicings = {k: v for k, v in params["slicing"].items() if k not in exclude}
    of_tap = HDFStore(in_config)
    slicings = load_slicing(cfg_slicings, of_tap, lazily=False, **circuit_kwargs)
    for slicing, to_slice in slicings.items():
        slicing_mode = cfg_slicings[slicing].get("compute_mode", "EXECUTE")
        if slicing_mode in ("datacall", "DATACALL"):
            sliced_inputs = generate_slices(of_tap, inputs=full, using_knives=to_slice)
            output_type = params["output"]
        elif slicing_mode in ("execute", "EXECUTE"):
            sliced_inputs = full
            output_type = matrices.type_series_store(params["output"])
        else:
            raise ValueError(f"Unhangled compute mode for slicing: {slicing_mode}")

        compute_nodes[slicing] = prepare_compute_nodes(sliced_inputs, to_stage/slicing,
                                                       slicing, output_type, None)
    return compute_nodes
#+end_src

To check the compute nodes,
#+name: develop-parallelization-multinode-setup-check
#+begin_src jupyter-python :comments org :padline no :tangle no

def report_run(of_computation, in_config, on_compute_node=None, tap=None, master_inputs=None, setup=None):
    """..."""

    if tap is None:
        tap = connsense_tap.HDFStore(in_config)

    if master_inputs is None:
        master_inputs = generate_inputs(of_computation, in_config)

    if setup is None:
        _, run_at = get_workspace(of_computation, in_config)
        batched = read_compute_nodes_assignment(at_dirpath=run_at)
        setup = {c: read_setup_compute_node(c, for_quantity=run) for c, _ in batched.groupby("compute_node")}

    if isinstance(on_compute_node, (int, np.int)):
        dirpath = Path(setup[on_compute_node]["dirpath"])
        inputs = (master_inputs.loc[pd.read_hdf(setup[on_compute_node]["input"], key="subtargets").index]
                  .apply(lambda l: l.shape[0]))
        tap_cn = devtap.HDFStore(tap._config, setup[on_compute_node]["output"])
        tap_dataset = devtap.TapDataset(tap_cn, dataset=describe(of_computation), belazy=True)

        try:
            toc = tap_dataset.dataset
        except FileNotFoundError:
            toc_size = inputs.apply(lambda _: 0)
        else:
            toc_size = toc.groupby(toc.index.names).size().reindex(inputs.index).fillna(0)

        status = ("DONE" if (dirpath/"DONE").exists()
                  else ("INPROGRESS" if (dirpath/"INPROGRESS").exists()
                        else None))

        return pd.concat([inputs, toc_size], axis=1, keys=["inputs", "outputs"]).assign(status=status)

    cns = (pd.Index(range(len(setup)), name="compute_node").to_series() if on_compute_node is None
           else pd.Index(on_compute_node, name="compute_node").to_series())

    cns_check = cns.progress_apply(lambda on_compute_node: report_run(setup, master_inputs, on_compute_node,
                                                                      tap, master_inputs, setup))
    return pd.concat(cns_check.values, keys=cns_check.index)


def report_collection(of_computation, in_config):
    """..."""
    _, run_at = get_workspace(of_computation, in_config)
    batched = read_compute_nodes_assignment(at_dirpath=run_at)
    setup = {c: read_setup_compute_node(c, for_quantity=run) for c, _ in batched.groupby("compute_node")}

    inputs = generate_inputs(of_computation, in_config)

    tap = connsense_tap.HDFStore(in_config)

    def report_compute_node(n):
        """..."""
        return report_run(of_computation, in_config, on_compute_node=n, tap=tap, master_inputs=inputs, setup=setup)



#+end_src

Test develop check,
#+begin_src jupyter-python :comments org :padline no :tangle no
def check_compute_node(setup, master_inputs, n=None):
    """..."""
    from tqdm import tqdm; tqdm.pandas()

    if isinstance(n, (int, np.int)):
        dirpath = Path(setup[n]["dirpath"])
        inputs = (master_inputs.loc[pd.read_hdf(setup[n]["input"], key="subtargets").index]
                .apply(lambda l: l.shape[0]))
        tap_cn = devtap.HDFStore(tap._config, setup[n]["output"])
        tap_traces_dset = devtap.TapDataset(tap_cn, dataset=["analyze-physiology", "psp/traces"],
                                            belazy=False)
        try:
            tap_traces = (pd.concat(tap_traces_dset.dataset.values, keys=tap_traces_dset.dataset.index)
                          .n_trials.groupby(inputs.index.names).size())
        except FileNotFoundError:
            tap_traces = inputs.apply(lambda _: 0)

        status = ("DONE" if (dirpath/"DONE").exists()
                  else ("INPROGRESS" if (dirpath/"INPROGRESS").exists()
                        else None))

        return (pd.concat([inputs, tap_traces], axis=1, keys=["inputs", "outputs"])
                .assign(status=status))

    cns = (pd.Index(range(len(setup)), name="compute_node").to_series() if n is None
           else pd.Index(n, name="compute_node").to_series())

    cns_check = cns.progress_apply(lambda n: check_compute_node(setup, master_inputs, n))
    return pd.concat(cns_check.values, keys=cns_check.index)

#+end_src

The version that sequences slicing and full together.
#+name: develop-parallelization-multinode-setup-0
#+begin_src python
def setup_multinode(process, of_computation, in_config, using_runtime, in_mode=None):
    """Setup a multinode process.
    """
    _, to_stage = get_workspace(of_computation, in_config)

    using_configs = configure_multinode(process, of_computation, in_config, at_dirpath=to_stage)

    computation_type, of_quantity = describe(of_computation)

    inputs = generate_inputs(of_computation, in_config)
    n_compute_nodes, n_parallel_jobs = prepare_parallelization(of_computation, in_config, using_runtime)

    if process == setup_compute_node:
        batched = batch_multinode(of_computation, inputs, in_config,
                                  at_dirpath=to_stage, using_parallelization=(n_compute_nodes, n_parallel_jobs))
        using_configs["slurm_params"] = (configure_slurm(of_computation, in_config, using_runtime)
                                         .get("sbatch", None))
        compute_nodes = {c: setup_compute_node(c, of_computation, inputs, using_configs,  at_dirpath=to_stage,
                                               in_mode=in_mode)
                         for c, inputs in batched.groupby("compute_node")}
        return {"configs": using_configs,
                "number_compute_nodes": n_compute_nodes, "number_total_jobs": n_parallel_jobs,
                "setup": write_multinode_setup(compute_nodes, inputs, at_dirpath=to_stage)}

    if process == collect_results:
        batched = read_compute_nodes_assignment(at_dirpath=to_stage)
        _, output_paths = read_pipeline.check_paths(in_config, step=computation_type)
        at_path = output_paths["steps"][computation_type]

        setup = {c: read_setup_compute_node(c, for_quantity=to_stage) for c, _ in batched.groupby("compute_node")}
        return collect_results(computation_type, setup, from_dirpath=to_stage, in_connsense_store=at_path)

    return ValueError(f"Unknown multinode {process}")

#+end_src

** Configure Slurm
#+name: develop-parallelization-multinode-configure-runtime-slurm
#+begin_src python
def configure_slurm(computation, in_config, using_runtime):
    """..."""
    computation_type, quantity = describe(computation)
    pipeline_config = in_config if isinstance(in_config, Mapping) else read_pipeline.read(in_config)
    from_runtime = (read_runtime_config(for_parallelization=using_runtime, of_pipeline=pipeline_config)
                    if not isinstance(using_runtime, Mapping) else using_runtime)

    params = from_runtime["pipeline"].get(computation_type, {})
    try:
        configured = params[quantity]
    except KeyError:
        quantity, component = quantity.split('/')
        configured = params[quantity][component]
    return configured

#+end_src

** Workspace

The workspace to run a computation will be nested under the project workspace,
#+name: develop-parallelization-workspace
#+begin_src python
def get_workspace(for_computation, in_config, in_mode=None):
    """..."""
    m = {'r': "test", 'w': "prod", 'a': "develop"}.get(in_mode, "test")
    computation_type, of_quantity = describe(for_computation)
    rundir = workspace.get_rundir(in_config, step=computation_type, substep=of_quantity, in_mode=m)
    basedir = workspace.find_base(rundir)
    return (basedir, rundir)
#+end_src

** Configs

To setup a multinode computation, we will write configs dervied from the main ~pipeline~ config in the computation's workspace, and read them while collecting,
#+name: develop-parallelization-multinode-configs
#+begin_src python
def configure_multinode(process, of_computation, in_config, at_dirpath):
    """..."""
    if process == setup_compute_node:
        return write_configs(of_computation, in_config, at_dirpath)
    if process == collect_results:
        return read_configs(of_computation, in_config, at_dirpath)
    raise ValueError(f"Unknown multinode {process}")


def write_configs(of_computation, in_config, at_dirpath):
    """..."""
    LOG.info("Write configs of %s at %s", of_computation, at_dirpath)
    return {"base": write_pipeline_base_configs(in_config, at_dirpath),
            "description": write_description(of_computation, in_config, at_dirpath)}


def read_configs(of_computation, in_config, at_dirpath):
    """..."""
    LOG.info("Read configs of %s at %s", of_computation, at_dirpath)
    return {"base": read_pipeline_base_configs(in_config, at_dirpath)}
#+end_src

where we write the base pipeline configs,
#+name: develop-parallelization-multinode-base-configs-write
#+begin_src python
def write_pipeline_base_configs(in_config, at_dirpath):
    """..."""
    basedir = find_base(rundir=at_dirpath)
    LOG.info("Check base configs at %s", basedir)

    def write_config(c):
        """..."""
        def write_format(f):
            filename = f"{c}.{f}"
            base_config = basedir / filename
            if base_config.exists():
                run_config = at_dirpath / filename
                _remove_link(run_config)
                run_config.symlink_to(base_config)
                return run_config
            LOG.info("Not found config %s", base_config)
            return None
        return {f: write_format(f) for f in ["json", "yaml"] if f}
    return {c: write_config(c) for c in ["pipeline", "runtime", "config", "parallel"]}

#+end_src

allowing for multiple naming schemes (~pipeline~ or ~config~ for the main config and ~runtime~ or ~parallel~ for parallelization config) and multiple formats (~yaml~ or ~json~). We will need to read these configs to launch individual compute-nodes, or to collect their results.
#+name: develop-parallelization-multinode-base-configs-read
#+begin_src python
def read_pipeline_base_configs(in_config, at_dirpath):
    """..."""
    basedir = find_base(rundir=at_dirpath)

    def read_config(c):
        """..."""
        def read_format(f):
            """..."""
            filename = f"{c}.{f}"
            path_config = at_dirpath / filename
            if path_config.exists():
                LOG.warning("Pipeline config %s found at %s", filename, at_dirpath)

                if c in ("pipeline", "config"):
                    return read_pipeline.read(path_config)

                if c in ("runtime", "parallel"):
                    return read_runtime_config(path_config, of_pipeline=in_config)

                raise ValueError(f"NOT a connsense config: {filename}")

            LOG.warning("No pipeline config %s found at %s", filename, at_dirpath)
            return None

        return {f: read_format(f) for f in ["json", "yaml"] if f}

    return {c: read_config(c) for c in ["pipeline", "runtime", "config", "parallel"]}

#+end_src

While the pipeline config is handled by ~TopologicalAnalysis~ object, the runtime config will be read here infer the configured parallelization parameters.
#+name: develop-parallelization-multinode-read-runtime
#+begin_src python
def read_runtime_config(for_parallelization, *, of_pipeline=None, return_path=False):
    """..."""
    assert not of_pipeline or isinstance(of_pipeline, Mapping), of_pipeline

    if not for_parallelization:
        return (None, None) if return_path else None

    try:
        path = Path(for_parallelization)
    except TypeError:
        assert isinstance(for_parallelization, Mapping)
        path = None
        config = for_parallelization
    else:
        if path.suffix.lower() in (".yaml", ".yml"):
            with open(path, 'r') as fid:
                config = yaml.load(fid, Loader=yaml.FullLoader)
        elif path.suffix.lower() == ".json":
            with open(path, 'r') as fid:
                config = json.load(fid)
        else:
            raise ValueError(f"Unknown config type {for_parallelization}")

    if not of_pipeline:
        return (path, config) if return_path else config

    from_runtime = config["pipeline"]
    default_sbatch = lambda : deepcopy(config["slurm"]["sbatch"])

    def configure_slurm_for(computation_type):
        """..."""
        LOG.info("Configure slurm for %s", computation_type)
        try:
            cfg_computation_type = of_pipeline["parameters"][computation_type]
        except KeyError:
            return None
        else:
            LOG.info("Pipeline for %s: \n%s", computation_type, pformat(cfg_computation_type))

        paramkey = PARAMKEY[computation_type]
        try:
            quantities_to_configure = cfg_computation_type[paramkey]
        except KeyError:
            LOG.warning("No quantities %s: \n%s", computation_type, cfg_computation_type)
            return None
        else:
            LOG.info("Configure runtime for %s %s", computation_type, quantities_to_configure)

        try:
            runtime = from_runtime[computation_type]
        except KeyError:
            LOG.warning("No runtime configured for computation type %s", computation_type)
            return None
        else:
            LOG.info("Use configuration: \n%s", pformat(runtime))

        configured = runtime[paramkey]

        if not configured:
            return None

        def decompose_quantity(q):
            """..."""
            return [var for var in quantities_to_configure[q].keys() if var not in COMPKEYS]

        def configure_quantity(q):
            """..."""
            LOG.info("configure quantity %s", q)

            q_cfg = deepcopy(configured.get(q) or {})
            if "sbatch" not in q_cfg:
                q_cfg["sbatch"] = default_sbatch()
            if "number-compute-nodes" not in q_cfg:
                q_cfg["number-compute-nodes"] = 1
            if "number-tasks-per-node" not in q_cfg:
                q_cfg["number-tasks-per-node"] = 1

            def configure_component(c):
                """..."""
                cfg = deepcopy(configured.get(q, {}).get(c, {}))
                if "sbatch" not in cfg:
                    cfg["sbatch"] = q_cfg["sbatch"]
                if "number-compute-nodes" not in cfg:
                    cfg["number-compute-nodes"] = q_cfg["number-compute-nodes"]
                if "number-tasks-per-node" not in cfg:
                    cfg["number-tasks-per-node"] = q_cfg['number-tasks-per-node']

                return cfg

            LOG.info("decomposed quantity: \n%s", decompose_quantity(q))
            for c in decompose_quantity(q):
                q_cfg[c] = configure_component(c)

            return q_cfg

        return {q: configure_quantity(q) for q in quantities_to_configure if q != "description"}

    runtime_pipeline = {c: configure_slurm_for(computation_type=c) for c in of_pipeline["parameters"]
                        if c != "description"}
    config = {"version": config["version"], "date": config["date"], "pipeline": runtime_pipeline}
    return (path, config) if return_path else config


#+end_src

We can read the runtime config, and use the information to prepare parallelization of a computation.
#+name: develop-parallelization-multinode-prepare-parallelization
#+begin_src python
def prepare_parallelization(of_computation, in_config, using_runtime):
    """..."""
    computation_type, quantity = describe(of_computation)
    from_runtime = (read_runtime_config(for_parallelization=using_runtime, of_pipeline=in_config)
                    if not isinstance(using_runtime, Mapping) else using_runtime)
    LOG.info("Prepare parallelization %s using runtime \n%s", of_computation, pformat(from_runtime))
    configured = from_runtime["pipeline"].get(computation_type, {})
    LOG.info("\t Configure \n%s", pformat(configured))
    n_compute_nodes, n_parallel_batches =  read_njobs(to_parallelize=configured,
                                                      computation_of=quantity)
    #order_complexity = configured[quantity].get("order_complexity", -1)
    order_complexity = _read_runtime("order_complexity",
                                     to_parallelize=configured, computation_of=quantity,
                                     default=-1)
    return (n_compute_nodes, n_parallel_batches, order_complexity)


def _read_runtime(key, to_parallelize, computation_of, default=None):
    """..."""
    if not to_parallelize:
        return default
    try:
        q = computation_of.name
    except AttributeError:
        q = computation_of

    try:
        p = to_parallelize[q]
    except KeyError:
        if '/' in q:
            try:
                q0, q1 = q.split('/')
            except ValueError:
                return default
            else:
                try:
                    p0 = to_parallelize[q0]
                except KeyError:
                    return default
                else:
                    try:
                        p = p0[q1]
                    except KeyError:
                        return default
                    else:
                        pass
        else:
            return default

    return p.get(key, default)

def read_njobs(to_parallelize, computation_of):
    """..."""
    compute_nodes = _read_runtime("number-compute-nodes", to_parallelize, computation_of,
                                  default=1)
    tasks = _read_runtime("number-tasks-per-node", to_parallelize, computation_of,
                          default=1)
    return (compute_nodes, compute_nodes * tasks)
#+end_src

Here is an older version to read number of jobs.
#+begin_src jupyter-python

def read_njobs(to_parallelize, computation_of):
    """..."""
    if not to_parallelize:
        return (1, 1)

    try:
        q = computation_of.name
    except AttributeError:
        q = computation_of

    try:
        p = to_parallelize[q]
    except KeyError:
        if '/' in q:
            try:
                q0, q1 = q.split('/')
            except ValueError: #TODO: log something
                return (1, 1)
            else:
                try:
                    p0 = to_parallelize[q0]
                except KeyError:
                    return (1, 1)
                else:
                    try:
                        p = p0[q1]
                    except KeyError:
                        return (1, 1)
                    else:
                        pass
        else:
            return (1, 1)

    compute_nodes = p["number-compute-nodes"]
    tasks = p["number-tasks-per-node"]
    return (compute_nodes, compute_nodes * tasks)


#+end_src

In addition to the base configs, we will also write a description of the computation in it's workspace,
#+name: develop-parallelization-multinode-description
#+begin_src python
def write_description(of_computation, in_config, at_dirpath):
    """..."""
    computation_type, of_quantity = describe(of_computation)
    configured = deepcopy(parameterize(computation_type, of_quantity, in_config))
    configured["name"] = of_quantity

    LOG.info("Write setup description of computation %s in config %s\n \n\t at path %s",
             of_computation, pformat(configured), at_dirpath)

    return read_pipeline.write(configured, to_json=at_dirpath/"description.json")
#+end_src

These configs written in the workspace directory of the computation will be used in later stages of the setup.
** Setup of a compute node
We will write all the information that a compute node will need to run in the compute node's workdir.
#+name: develop-parallelization-multinode-setup-compute-node
#+begin_src python
def setup_compute_node(c, of_computation, with_inputs, using_configs, at_dirpath,
                       in_mode=None, slicing=None):
    """..."""
    assert not in_mode or in_mode in ("prod", "develop")

    from connsense.apps import APPS
    LOG.info("Configure compute-node %s (%s inputs) to %s slicing %s, with configs \n%s",
             c, len(with_inputs), of_computation, slicing or "none", using_configs)

    computation_type, of_quantity = describe(of_computation)
    for_compute_node = at_dirpath / f"compute-node-{c}"
    for_compute_node.mkdir(parents=False, exist_ok=True)
    configs = symlink_pipeline(using_configs, at_dirpath=for_compute_node)

    inputs_to_read = write_compute(batches=with_inputs, to_hdf=INPUTS, at_dirpath=for_compute_node)
    output_h5 = f"{for_compute_node}/connsense.h5"

    try:
        slurm_params = using_configs["slurm_params"]
    except KeyError as kerr:
            raise RuntimeError("Missing slurm params") from kerr
    of_executable = cmd_sbatch(APPS["main"], of_computation, config=slurm_params,
                               at_dirpath=for_compute_node)

    if "submission" not in with_inputs:
        launchscript = at_dirpath / "launchscript.sh"
    else:
        submission = with_inputs.submission.unique()
        assert len(submission) == 1,\
            f"A single compute node's inputs must be submitted together"
        launchscript = at_dirpath / f"launchscript-{submission[0]}.sh"

    run_mode = in_mode or "prod"
    command_lines = ["#!/bin/bash",
                     (f"########################## LAUNCH {computation_type} for chunk {c}"
                      f" of {len(with_inputs)} _inputs."
                     "#######################################"),
                     f"pushd {for_compute_node}",
                     f"sbatch {of_executable.name} run {computation_type} {of_quantity} \\",
                     "--configure=pipeline.yaml --parallelize=runtime.yaml \\",
                     None if not slicing else f"--slicing={slicing} \\",
                     f"--mode={run_mode} \\",
                     f"--input={inputs_to_read} \\",
                     f"--output={output_h5}",
                     "popd"]

    with open(launchscript, 'a') as to_launch:
        to_launch.write('\n'.join(l for l in command_lines if l) + "\n")

    setup = {"dirpath": for_compute_node, "sbatch": of_executable,
             "input": inputs_to_read, "output": output_h5}

    return read_pipeline.write(setup, to_json=for_compute_node/"setup.json")


def cmd_sbatch(executable, of_computation, config, at_dirpath):
    """..."""
    computation_type, _ = describe(of_computation)
    slurm_params = deepcopy(config)
    slurm_params.update({"name": computation_type, "executable": executable})
    slurm_config = SlurmConfig(slurm_params)
    return slurm_config.save(to_filepath=at_dirpath/f"{computation_type}.sbatch")


def write_compute(batches, to_hdf, at_dirpath):
    """..."""
    batches_h5, and_hdf_group = to_hdf
    batches.to_hdf(at_dirpath / batches_h5, key=and_hdf_group, format="fixed", mode='w')
    return at_dirpath / batches_h5

#+end_src

To write and read the multiple compute node setup,
#+name: develop-parallelization-multinode-setup-read-write
#+begin_src python
def write_multinode_setup(compute_nodes, inputs, at_dirpath):
    """..."""
    inputs_h5, dataset = INPUTS
    return read_pipeline.write({"compute_nodes": compute_nodes, "inputs": at_dirpath / inputs_h5},
                               to_json=at_dirpath/"setup.json")


def read_setup_compute_node(c, for_quantity):
    """..."""
    for_compute_node = for_quantity / f"compute-node-{c}"

    if not for_compute_node.exists():
        raise RuntimeError(f"Expected compute node directory {for_compute_node} created by the TAP run to collect")

    return read_setup(at_dirpath=for_quantity, compute_node=c)


def read_setup(at_dirpath, compute_node):
    """..."""
    setup_json = at_dirpath / f"compute-node-{compute_node}" / "setup.json"

    if not setup_json.exists():
        raise RuntimeError(f"No setup json found at {setup_json}")

    with open(setup_json, 'r') as f:
        return json.load(f)

    raise RuntimeError("Python execution must not have reached here.")

#+end_src

We will also ~symlink~ the configs,
#+name: develop-parallelization-multinode-setup-symlink-configs
#+begin_src python
def symlink_pipeline(configs, at_dirpath):
    """..."""
    to_base = symlink_pipeline_base(configs["base"], at_dirpath)
    return {"base": to_base}


def create_symlink(at_dirpath):
    """..."""
    def _to(config_at_path):
        """..."""
        it_is_a = at_dirpath / config_at_path.name
        _remove_link(it_is_a)
        it_is_a.symlink_to(config_at_path)
        return it_is_a

    return _to


def symlink_pipeline_base(configs, at_dirpath):
    """..."""
    symlink_to = create_symlink(at_dirpath)
    return {"pipeline": {fmt: symlink_to(config_at_path=p) for fmt, p in configs["pipeline"].items() if p},
            "runtime": {fmt: symlink_to(config_at_path=p) for fmt, p in configs["runtime"].items() if p}}

#+end_src

** Collect results
Once all the compute nodes have run and results saved to their local ~connsense.h5~, we will need to collect the results. Collection may depend on the computation's type,
#+name: develop-parallelization-multinode-collect-results
#+begin_src python
def collect_results(computation_type, setup, from_dirpath, in_connsense_store, slicing,
                    output_type=None):
    """..."""
    if computation_type == "extract-node-populations":
        assert not slicing, "Does not apply"
        return collect_node_population(setup, from_dirpath, in_connsense_store)

    if computation_type == "extract-edge-populations":
        assert not slicing, "Does not apply"

        return collect_edge_population(setup, from_dirpath, in_connsense_store)

    if computation_type in ("analyze-connectivity", "analyze-composition",
                            "analyze-node-types", "analyze-physiology"):
        return collect_analyze_step(setup, from_dirpath, in_connsense_store, slicing,
                                    output_type)

    raise NotImplementedError(f"INPROGRESS: {computation_type}")

#+end_src

To collect nodes,
#+name: develop-parallelization-multinode-collect-node-population
#+begin_src python
def collect_node_population(setup, from_dirpath, in_connsense_store):
    """..."""
    try:
        with open(from_dirpath/"description.json", 'r') as f:
            population = json.load(f)
    except FileNotFoundError as ferr:
        raise RuntimeError(f"NOTFOUND a description of the population extracted: {from_dirpath}")

    connsense_h5, group = in_connsense_store
    hdf_population = group + '/' + population["name"]

    def describe_output(of_compute_node):
        """..."""
        try:
            with open(Path(of_compute_node["dirpath"]) / "output.json", 'r') as f:
                output = json.load(f)
        except FileNotFoundError as ferr:
            LOG.info("No output configured for compute node %s\n%s", {pformat(of_compute_node)}, ferr)
            return None
        return output

    outputs = {c: describe_output(of_compute_node) for c, of_compute_node in setup.items()}
    LOG.info("Extract node populations %s reported outputs: \n%s", population["name"], pformat(outputs))

    def in_store(at_path, hdf_group=None):
        """..."""
        return matrices.get_store(at_path, hdf_group or hdf_population, pd.DataFrame)

    def move(compute_node, output):
        """..."""
        LOG.info("Get node population store for compute-node %s output %s", compute_node, output)
        h5, g = output
        return in_store(at_path=h5, hdf_group=g)

    return in_store(connsense_h5).collect({c: move(compute_node=c, output=o) for c, o in outputs.items()})

#+end_src

To collect edges,
#+name: develop-parallelization-multinode-collect-edge-population
#+begin_src python
def collect_edge_population(setup, from_dirpath, in_connsense_store):
    """..."""
    LOG.info("Collect edge population at %s using setup \n%s", from_dirpath, setup)

    try:
        with open(from_dirpath/"description.json", 'r') as f:
            population = json.load(f)
    except FileNotFoundError as ferr:
        raise RuntimeError(f"NOTFOUND a description of the population extracted: {at_basedir}") from ferr

    #p = population["name"]
    #hdf_edge_population = f"edges/populations/{p}"
    connsense_h5, group = in_connsense_store
    hdf_edge_population = group + '/' + population["name"]

    LOG.info("Collect edges with description \n%s", pformat(population))

    def describe_output(of_compute_node):
        """..."""
        try:
            with open(Path(of_compute_node["dirpath"]) / "output.json", 'r') as f:
                output = json.load(f)
        except FileNotFoundError as ferr:
            LOG.info("No output configured for compute node %s\n%s", {pformat(of_compute_node)}, ferr)
            return None
            #raise RuntimeError(f"No output configured for compute node {of_compute_node}") from ferr
        return output

    outputs = {c: describe_output(of_compute_node) for c, of_compute_node in setup.items()}
    LOG.info("Edge extraction reported outputs: \n%s", pformat(outputs))

    def collect_adjacencies(of_compute_node, output):
        """..."""
        LOG.info("Collect adjacencies compute-node %s output %s", of_compute_node, output)
        adj = read_toc_plus_payload(output, for_step="extract-edge-populations")
        return write_toc_plus_payload(adj, (connsense_h5, hdf_edge_population), append=True, format="table",
                                      min_itemsize={"values": 100})

    LOG.info("Collect adjacencies")
    for of_compute_node, output in outputs.items():
        collect_adjacencies(of_compute_node, output)

    LOG.info("Adjacencies collected: \n%s", len(outputs))

    return (in_connsense_store, hdf_edge_population)

#+end_src

To collect analyses
#+name: develop-parallelization-multinode-collect-analyze-step
#+begin_src python
def collect_analyze_step(setup, from_dirpath, in_connsense_store, slicing,
                         output_type=None):
    """..."""
    try:
        with open(from_dirpath/"description.json", 'r') as f:
            analysis = json.load(f)
    except FileNotFoundError as ferr:
        raise RuntimeError(f"NOTFOUND a description of the analysis extracted: {from_dirpath}") from ferr

    connsense_h5, group = in_connsense_store
    hdf_group = group + '/' + analysis["name"]
    if slicing:
        hdf_group = hdf_group + "/" + slicing
    output_type = output_type if output_type else analysis["output"]

    def describe_output(of_compute_node):
        """..."""
        try:
            with open(Path(of_compute_node["dirpath"]) / "output.json", 'r') as f:
                output = json.load(f)
        except FileNotFoundError as ferr:
            LOG.info("No output configured fo compute node %s: \n%s", pformat(of_compute_node), ferr)
            return None
            #raise RuntimeError(f"No output configured for compute node {of_compute_node}") from ferr
        return output

    outputs = {c: describe_output(of_compute_node) for c, of_compute_node in setup.items()}
    LOG.info("Analysis %s reported outputs: \n%s", analysis["name"], pformat(outputs))

    def in_store(at_path, hdf_group):
        """..."""
        return matrices.get_store(at_path, hdf_group or hdf_analysis, output_type)

    def move(compute_node, output):
        """..."""
        LOG.info("Get analysis store for compute-node %s output %s", compute_node, output)
        h5, g = output
        return in_store(at_path=h5, hdf_group=g)

    return (in_store(connsense_h5, hdf_group)
            .collect({c: move(compute_node=c, output=o) for c, o in outputs.items() if o}))

#+end_src

* Multiprocess on a single compute-node

We have developed a parallelization setup to distribute a computation over multiple compute-nodes. Now we can develop how a multiprocess run on a single compute-node that the launchscripts generated in the setup will invoke.
#+name: develop-parallelization-single-node-multiprocess
#+begin_src python
SERIAL_BATCHES = "serial-batches"
PARALLEL_BATCHES = "parallel-batches"

def run_multiprocess(of_computation, in_config, using_runtime, on_compute_node,
                     batching=SERIAL_BATCHES, slicing=None):
    """..."""
    import time
    from connsense.develop.topotap import HDFStore
    on_compute_node = run_cleanup(on_compute_node)

    run_in_progress = on_compute_node.joinpath(INPROGRESS)
    run_in_progress.touch(exist_ok=False)


    computation_type, of_quantity = describe(of_computation)
    parameters, execute, to_store_batch, to_store_one = (
        configure_execution(of_computation, in_config, on_compute_node, slicing=slicing)
    )
    assert to_store_batch or to_store_one
    assert not (to_store_batch and to_store_one)

    in_hdf = "connsense-{}.h5"

    circuit_args = input_circuit_args(of_computation, in_config,
                                      load_circuit=False, load_connectome=False, drop_nulls=False)
    circuit_kwargs = input_circuit_args(of_computation, in_config,
                                        load_circuit=True, load_connectome=False, drop_nulls=False)
    circuit_args_values = tuple(v for v in (circuit_kwargs.get("circuit"),
                                            circuit_kwargs.get("connectome")) if v)

    kwargs = load_kwargs(parameters, HDFStore(in_config), on_compute_node)

    timeout = kwargs.pop("timeout", None)

    inputs = generate_inputs(of_computation, in_config, slicing, circuit_args,
                             ,**circuit_kwargs)

    if not isinstance(inputs.index, pd.MultiIndex):
        inputs.index = pd.MultiIndex.from_arrays([inputs.index.values,], names=[inputs.index.name])

    collector = (plugins.import_module(parameters["collector"]) if "collector" in parameters
                 else None)

    def collect_batch(results):
        """..."""
        if not collector:
            return results

        _, collect = collector
        return collect(results)

    def execute_one(lazy_subtarget, bowl=None, index=None):
        """..."""
        subtarget_inputs = lazy_subtarget()
        LOG.info("Execute circuit args %s lazy subtarget %s, kwargs %s",
                 circuit_args_values, subtarget_inputs.keys(), kwargs.keys())
        #result = execute(*circuit_args_values, **subtarget_inputs)
        result = execute(circuit_kwargs, subtarget_inputs)
        if bowl:
            assert index
            bowl[index] = result
        return result

    def lazy_dataset(s):
        """..."""
        if callable(s): return s

        if isinstance(s, Mapping): return lambda: {var: value() for var, value in s.items()}

        raise ValueError(f"Cannot resolve lazy dataset of type {type(s)}")

    def serial_batch(of_input, *, index, in_bowl=None):
        """..."""
        LOG.info("Run %s batch %s of %s inputs args, and circuit %s, \n with kwargs %s slicing %s",
                 of_computation,  index, len(of_input), circuit_args_values, pformat(kwargs), slicing)

        def to_subtarget(s):
            """..."""
            r = execute_one(lazy_dataset(s))
            LOG.info("store one lazy subtarget %s result \n%s", s, r)

            if r is None:
                LOG.warning("THERE is no RESULT for subtarget index  %s", index)
                return None

            if r.empty:
                LOG.warning("Result is empty.")
            LOG.info("Result data types\n%s", r.describe())
            return to_store_one(in_hdf.format(index), result=r)

        if to_store_batch:
            results = of_input.apply(execute_one)
            try:
                results = results.droplevel("compute_node")
            except KeyError:
                pass
            result = to_store_batch(in_hdf.format(index), results=collect_batch(results))
        else:
            toc = of_input.apply(to_subtarget)
            try:
                toc = toc.droplevel("compute_node")
            except KeyError:
                pass

            tocna = toc[toc.isna()]
            LOG.info("TOC elements that were NA for subtarget index %s: \n%s", index, tocna)
            result = to_store_one(in_hdf.format(index), update=toc.dropna())

        if in_bowl is not None:
            in_bowl[index] = result
        return result

    n_compute_nodes,  n_total_jobs, _  = prepare_parallelization(of_computation,
                                                                 in_config, using_runtime)

    batches = load_input_batches(on_compute_node)
    n_batches = batches.batch.max() - batches.batch.min() + 1

    if n_compute_nodes == n_total_jobs:
        results = {}
        for batch, subtargets in batches.groupby("batch"):
            LOG.info("Run Single Node %s process %s / %s batches",
                     on_compute_node, batch, n_batches)
            results[batch] = serial_batch(inputs.loc[subtargets.index], index=batch)
        LOG.info("DONE Single Node connsense run.")
    else:
        if batching == SERIAL_BATCHES:
            manager = Manager()
            bowl = manager.dict()
            processes = []

            for b, subtargets in batches.groupby("batch"):
                LOG.info("Spawn Compute Node %s process %s / %s batches",
                         on_compute_node, b, n_batches)
                p = Process(target=serial_batch,
                            args=(inputs.loc[subtargets.index],),
                            kwargs={"index": b, "in_bowl": bowl})
                p.start()
                processes.append((b, p))

            LOG.info("LAUNCHED %s processes", n_batches)

            if timeout:
                start = time.time()
                in_run = pd.Series([p.is_alive() for _, p in processes], name="running",
                                   index=pd.Index([b for b, _ in processes], name="batch"))

                while (in_run.sum() > 0) and (time.time() - start < timeout):
                    in_run = pd.Series([p.is_alive() for _, p in processes], name="running",
                                       index=pd.Index([b for b, _ in processes],
                                                      name="batch"))
                    LOG.info("RUNNING PROCESSES (timeout at %s)", timeout)
                    LOG.info("at time %s processing still running: %s/%s\n%s",
                             time.time() - start, in_run.sum(), len(processes),
                             batches[in_run.reindex(batches.batch.values)
                                     .fillna(False).values]
                             .droplevel("compute_node").reset_index())

                    if in_run.sum() > 0:
                        time.sleep(60)
                    else:
                        break
                else:
                    LOG.info(("TIMEOUT: terminating %s alive of %s processes:"
                              " computation time %s exceeded."),
                             in_run.sum(), len(processes), timeout)
                    for _,p in processes:
                        p.terminate()

            for _,p in processes:
                p.join()

            LOG.info("Parallel computation %s results %s", of_computation, len(bowl))

            results = {key: value for key, value in bowl.items()}
            LOG.info("Computation %s results %s", of_computation, len(results))

        else:
            assert batching == PARALLEL_BATCHES, "No other is known."
            for batch, subtargets in batches.groupby("batch"):
                LOG.info("Run %s subtargets in parallel batch %s / %s batches.",
                         len(subtargets), batch, len(batches))

                manager = Manager()
                bowl = manager.dict()
                processes = []

                for i, s in enumerate(subtargets):
                    p = Process(target=execute_one,
                                args=(s,), kwargs={"index": i, "bowl": bowl})
                    p.start()
                    process.append(p)
                LOG.info("LAUNCHED %s process", i)

                for p in process:
                    p.join()
                LOG.info("Parallel computation for batch %s: %s", batch, len(bowl))
                values = pd.Series([v for v in bowl.values()], index=subtargets.index)
                hdf = in_hdf.format(batch)
                of_each_value = lambda: values.apply(lambda v: to_store_one(hdf, result=v))
                results = (to_store_batch(hdf, results=values) if to_store_batch else
                           to_store_one(hdf, update=of_each_value()))

    read_pipeline.write(results, to_json=on_compute_node/"batched_output.json")

    _, output_paths = read_pipeline.check_paths(in_config, step=computation_type)
    _, hdf_group = output_paths["steps"][computation_type]
    collected = collect_batches(of_computation, results, on_compute_node, hdf_group, slicing,
                                of_output_type=parameters["output"])
    read_pipeline.write(collected, to_json=on_compute_node/"output.json")

    run_in_progress.unlink()
    on_compute_node.joinpath(DONE).touch(exist_ok=False)
    return collected
#+end_src

** Circuit and connectome
We may need a circuit to run on. The current ~connsense-TAP~ can be configured with multiple circuits, which we could support later. However for now we enforce that only one circuit is configured, and that all the extractors provided work on single circuits as well.

The solution will be to specify the circuit at CLI. and update the pipeline's config that is passed to the parallelization methods developed here. This will allow configuration of more than one circuit, but each run will be for only one circuit that must be entered in the configuration.

Another is to require inputs for each computation in it's config:
#+name: develop-parallelization-single-node-multiprocess-circuit
#+begin_src python
def input_circuit(labeled, in_config):
    """..."""
    if not labeled:
        return None
    sbtcfg = SubtargetsConfig(in_config)
    circuit = sbtcfg.attribute_depths(circuit=labeled)

    return circuit


def input_connectome(labeled, in_circuit):
    """..."""
    if not labeled:
        return None

    from bluepy import Circuit
    assert isinstance(in_circuit, Circuit)

    if labeled == "local":
        return in_circuit.connectome
    return in_circuit.projection[labeled]


def input_circuit_args(computation, in_config,
                       load_circuit=True, load_connectome=False, *,
                       drop_nulls=True):
    """..."""
    computation_type, of_quantity = describe(computation)
    parameters = parameterize(computation_type, of_quantity, in_config)

    try:
        computation_inputs = parameters["input"]
    except KeyError as kerr:
        raise ValueError(f"No inputs configured for {computation}") from kerr

    input_circuits = computation_inputs.get("circuit", None)
    if input_circuits:
        assert len(input_circuits) == 1, f"NotImplemented processing more than one circuit"
        c = input_circuits[0]
    else:
        c = None
    circuit = input_circuit(c, in_config) if load_circuit else c

    input_connectomes = computation_inputs.get("connectome", None)
    if input_connectomes:
        assert len(input_connectomes) == 1, f"NotImplemented processing more than one connectome"
        x = input_connectomes[0]
    else:
        x = None
    connectome = input_connectome(x, in_circuit=c) if load_connectome else x
    circonn = {"circuit": circuit, "connectome": connectome}
    return {key: value for key, value in circonn.items() if value}


def subtarget_circuit_args(computation, in_config,
                           load_circuit=False, load_connectome=False):
    """..."""
    computation_type, of_quantity = describe(computation)
    parameters = parameterize(computation_type, of_quantity, in_config)

    try:
        subtarget = parameters["subtarget"]
    except KeyError as kerr:
        LOG.warning("No subtargets specified for %s", computation)
        return input_circuit_args(computation, in_config, load_circuit, load_connectome)

    c = subtarget.get("circuit", None)
    circuit = input_circuit(c, in_config) if load_circuit else c

    x = subtarget.get("connectome", None)
    return {"circuit": circuit, "connectome": input_connectome(x, circuit) if load_connectome else x}

#+end_src

** Pipeline dataset inputs

Inputs to run on a compute node, that are ~connsense-TAP~ datasets, are written in a computation's work-directory during the setup.
#+name: develop-parallelization-single-node-multiprocess-load-inputs
#+begin_src python

def load_input_batches(on_compute_node, inputs=None, n_parallel_tasks=None):
    """..."""
    store_h5, dataset = COMPUTE_NODE_SUBTARGETS

    assert inputs is None or inputs == on_compute_node / store_h5, (
        "inputs dont seem to be what was configured\n"
        f"Expected {inputs} to be {on_compute_node / store_h5} if setup by run_multinode(...)")

    inputs_read = pd.read_hdf(on_compute_node/store_h5, key=dataset)
    if not n_parallel_tasks:
        return inputs_read
    return inputs_read.assign(batch=pd.Series(np.arange(0, len(inputs_read))%n_parallel_tasks).to_numpy(int))

#+end_src

that assumes that the inputs were written on the compute nodes during setup.

** Keyword arguments

To load the key-word arguments
#+name: develop-parallelization-single-node-multiprocess-kwargs
#+begin_src python

def load_kwargs(parameters, to_tap, on_compute_node=None, consider_input=False):
    """..."""
    def load_if_dataset(variable, value):
        """..."""
        if not isinstance(value, Mapping):
            return value

        if "dataset" in value:
            return to_tap.pour_dataset(*value["dataset"], subset=value.get("subset", None))
            #return load_dataset(to_tap, variable, value)
        return value

    kwargs = parameters.get("kwargs", {})
    kwargs.update({var: load_if_dataset(var, value) for var, value in kwargs.items()
                   if var not in COMPKEYS})

    if consider_input:
        kwargs.update({var: value for var, value in parameters.get("input", {}).items()
                       if var not in ("circuit", "connectome") and (
                               not isinstance(value, Mapping) or "dataset" not in value)})

    try:
        workdir = kwargs["workdir"]
    except KeyError:
        return kwargs

    if isinstance(workdir, Path):
        return kwargs

    if isinstance(workdir, str):
        path = (Path(workdir)/on_compute_node.relative_to(to_tap._root.parent)
                if on_compute_node else Path(workdir))
        path.parent.mkdir(parents=True, exist_ok=True)

        if path.exists():
            LOG.warning("Compute node has been run before, leaving a workdir:\n%s", path)
            archive = path.parent / "history"
            archive.mkdir(parents=False, exist_ok=True)

            history = archive / path.name
            if history.exists():
                LOG.warning("Previous runs exist in %s history at \n%s", path.name, history)
            history.mkdir(parents=False, exist_ok=True)

            to_archive = history/ time.stamp(now=True)
            if to_archive.exists():
                LOG.warning("There is a previous run with the same time stamp as now!!!\n%s"
                            "\n It will be removed", to_archive)
                shutil.copytree(path, to_archive,
                                symlinks=True, ignore_dangling_symlinks=True, dirs_exist_ok=True)
                for filepath in path.glob('*'):
                    filepath.unlink()
            else:
                path.rename(to_archive)

        path.mkdir(parents=False, exist_ok=True)

        if on_compute_node:
            try:
                on_compute_node.joinpath("workdir").symlink_to(path)
            except FileExistsError as ferr:
                LOG.warn("Symlink to workdir compute-node %s already exists, most probably from a previous run"
                         " Please cleanup before re-run", str(on_compute_node))

        kwargs["workdir"] = path
        return kwargs

    if workdir is True:
        workdir = on_compute_node / "workdir"
        workdir.mkdir(parents=False, exist_ok=True)
        kwargs["workdir"] = workdir
        return kwargs

    raise NotImplementedError(f"What to do with workdir type {type(workdir)}")

#+end_src

** Execution of a unit-computation
The executable for a computation is provided in the ~pipeline-config~. The executable provided in the ~connsense-pipeline-config~ must apply to a single ~unit-computation~ of a ~subtarget~. However, we may want to consider the an analysis of slices of a ~subtarget~ as a /single computation/. This means that individual slices will not be ~unit-computations~. We will have to define a ~unit-computation~ that takes a  ~full-subtarget~, slices it, applies the /configured/ ~unit-computation~, and returns the result as a ~series~ or ~frame~ with labels for slices in its ~index~.

#+name: develop-parallelization-single-node-multiprocess-executable
#+begin_src python
def get_executable(computation, in_config, slicing=None):
    """..."""
    from connsense.develop.topotap import HDFStore

    computation_type, of_quantity = describe(computation)
    parameters = parameterize(computation_type, of_quantity, in_config)

    executable_type = EXECUTABLE[computation_type.split('-')[0]]
    try:
        executable = parameters[executable_type]
    except KeyError as err:
        raise RuntimeError(f"No {executable_type} defined for {computation_type}") from err

    _, ex = plugins.import_module(executable["source"], executable["method"])

    of_tap = HDFStore(in_config)

    analysis_kwargs = load_kwargs(parameters, of_tap)
    def execute_one(circuit_kwargs, subtarget_kwargs):
        LOG.info("Execute %s for a single subtarget", ex.__name__)
        return ex(**circuit_kwargs, **subtarget_kwargs, **analysis_kwargs)
#    def execute_one(*circuit_args, **subtarget_kwargs):
#        return ex(*circuit_args, **subtarget_kwargs, **analysis_kwargs)

    if not slicing or slicing == "full":
        return (execute_one, parameters)

    exclude = ("description", "do-full")
    cfg_slicings = {k: v for k, v in parameters["slicing"].items() if k not in exclude}
    if cfg_slicings[slicing].get("compute_mode", "EXECUTE") in ("datacall", "DATACALL"):
        return (execute_one, parameters)

    circuit_kwargs = input_circuit_args(computation, in_config, load_circuit=True)
    knives = load_slicing(cfg_slicings, of_tap, lazily=False, **circuit_kwargs)[slicing]

    def apply_sliced(circuit_kwargs, subtarget_kwargs):
        """..."""
        return (pd.Series([cut(subtarget_kwargs) for cut in knives],
                          index=knives.index)
                .apply(lambda xs: execute_one(circuit_kwargs, xs)))

    parameters["output"] = matrices.type_series_store(parameters["output"])
    return (apply_sliced, parameters)
#+end_src

that will compute the result for a ~unit-computation~. Each compute-nude will process several ~unit-computations~ in parallel. Each parallel batch's results will be stored in it's own ~connsense-TAP-store~ and collected into the ~compute-node~'s  ~connsense-TAP-store~. The results of a parallel batch can be written to the store per ~unit-computation~ or collected into a batch before storing. Whether a /single/ or a /batch/ store is used will depend on the computation's type.

#+name: develop-parallelization-single-node-multiprocess-stores
#+begin_src python
def store_node_properties_batch(of_population, on_compute_node, in_hdf_group):
    """...This will extract node properties for all subtargets as a single datasframe.
    NOT-IDEAL and needs hacks to gather differemt resuts into the same input dataframe.
    REPLACE by single subtarget store using matrices
    """
    def write_batch(connsense_h5, results):
        """..."""
        in_hdf = (on_compute_node/connsense_h5, in_hdf_group)
        LOG.info("Write %s  results %s ", in_hdf, len(results))
        return extract_nodes.write(results, of_population, in_hdf)

    return write_batch


def store_node_properties(of_population, on_compute_node, in_hdf_group):
    """..."""
    LOG.info("Store node properties of population %s on compute node %s in hdf %s, one subtarget at a time",
             of_population, on_compute_node, in_hdf_group)

    def write_hdf(at_path, *, result=None, update=None):
        """..."""
        assert not(result is None and update is None)
        assert result is not None or update is not None

        hdf_population = in_hdf_group+'/'+of_population
        store = matrices.get_store(on_compute_node/at_path, hdf_population, "pandas.DataFrame")

        if result is not None:
            return store.write(result)

        store.append_toc(store.prepare_toc(of_paths=update))
        return (at_path, hdf_population)

    return write_hdf


def store_edge_extraction(of_population, on_compute_node, in_hdf_group):
    """..."""
    def write_batch(connsense_h5, results):
        """..."""
        in_hdf = (on_compute_node/connsense_h5, f"{in_hdf_group}/{of_population}")
        LOG.info("Write %s batch results to %s", len(results), in_hdf)
        return extract_connectivity.write_adj(results, to_output=in_hdf, append=True,
                                              format="table", return_config=True)

    return write_batch


def store_matrix_data(of_quantity, parameters, on_compute_node, in_hdf_group, slicing):
    """..."""
    LOG.info("Store matrix data for %s", parameters)
    of_output = parameters["output"]
    hdf_group = f"{in_hdf_group}/{of_quantity}"
    if slicing:
        hdf_group = f"{hdf_group}/{slicing}"

    cached_stores = {}

    def write_hdf(at_path, *, result=None, update=None):
        """..."""
        assert at_path
        assert not(result is None and update is None)
        assert result is not None or update is not None

        p = on_compute_node/at_path
        if p not in cached_stores:
            cached_stores[p] = matrices.get_store(p, hdf_group, for_matrix_type=of_output)

        if result is not None:
            assert update is None, "No update allowed once result has been presented."
            LOG.info("Write a result to store %s with size %s", p, len(result))
            path = cached_stores[p].write(result)
            LOG.info("Wrote a result to store %s with size %s to path %s", p, len(result), path)
            return path

        cached_stores[p].append_toc(cached_stores[p].prepare_toc(of_paths=update))
        return (at_path, hdf_group)

    return write_hdf

#+end_src

To configure a multiprocess execution, we need the executable and the store to use for it's results

#+name: develop-parallelization-single-node-multiprocess-configure-execution
#+begin_src python
def configure_execution(computation, in_config, on_compute_node, slicing=None):
    """..."""
    computation_type, of_quantity = describe(computation)
    _, output_paths = read_pipeline.check_paths(in_config, step=computation_type)
    _, in_hdf_group = output_paths["steps"][computation_type]

    execute, parameters = get_executable(computation, in_config, slicing)

    if computation_type == "extract-node-populations":
        assert not slicing, "Does not apply"
        return (parameters, execute, None,  store_node_properties(of_quantity, on_compute_node,
                                                                  in_hdf_group))

    if computation_type == "extract-edge-populations":
        assert not slicing, "Does not apply"
        return (parameters, execute, store_edge_extraction(of_quantity, on_compute_node,
                                                           in_hdf_group), None)

    return (parameters, execute,
            None, store_matrix_data(of_quantity, parameters, on_compute_node,
                                    in_hdf_group, slicing))

#+end_src

** Collect parallel batches

To collect the results of parallel batches into a ~connsense-TAP-store~ at a ~compute-node~,
#+name: develop-parallelization-single-node-multiprocess-collect
#+begin_src python
def collect_batches(of_computation, results, on_compute_node, hdf_group, slicing,
                    of_output_type):
    """..."""
    LOG.info("Collect %s results of %s on compute node %s in group %s output type %s",
             len(results), of_computation, on_compute_node, hdf_group, of_output_type)
    computation_type, of_quantity = describe(of_computation)

    if computation_type == "extract-edge-populations":
        assert not slicing, "Does not apply"
        return collect_batched_edge_population(of_quantity, results,
                                               on_compute_node, hdf_group)

    if computation_type == "extract-node-populations":
        assert not slicing, "Does not apply"

    hdf_group = hdf_group+"/"+of_quantity
    if slicing:
        hdf_group = hdf_group+"/"+slicing
    in_connsense_h5 = on_compute_node / "connsense.h5"

    in_store = (matrices
                .get_store(in_connsense_h5, hdf_group, for_matrix_type=of_output_type))
    in_store.collect({batch: matrices.get_store(on_compute_node / batch_connsense_h5,
                                                hdf_group,
                                                for_matrix_type=of_output_type)
                      for batch, (batch_connsense_h5, group) in results.items()})
    return (in_connsense_h5, hdf_group)

#+end_src

For the extracting node populations we have a special case,
#+name: develop-parallelization-single-node-multiprocess-collect-batched-node-populations
#+begin_src python
def collect_batched_node_population(p, results, on_compute_node, hdf_group):
    """..."""
    from connsense.io.write_results import read as read_batch, write as write_batch

    LOG.info("Collect batched node populations of %s %s results on compute-node %s to %s", p,
             len(results), on_compute_node, hdf_group)

    in_connsense_h5 = on_compute_node / "connsense.h5"

    hdf_node_population = (in_connsense_h5, hdf_group+"/"+p)

    def move(batch, output):
        """..."""
        LOG.info("Write batch %s read from %s", batch, output)
        result = read_batch(output, "extract-node-populations")
        return write_batch(result, to_path=hdf_node_population, append=True, format="table")

    LOG.info("collect batched extraction of nodes at compute node %s", on_compute_node)
    for batch, output in results.items():
        move(batch, output)

    LOG.info("DONE collecting %s", results)
    return hdf_node_population

#+end_src

For extracting edge populations we have a special case,
#+name: develop-parallelization-single-node-multiprocess-collect-batched-edge-populations
#+begin_src python
def collect_batched_edge_population(p, results, on_compute_node, hdf_group):
    """..."""
    in_connsense_h5 = on_compute_node / "connsense.h5"

    hdf_edge_population = (in_connsense_h5, hdf_group+'/'+p)

    def move(batch, output):
        """.."""
        LOG.info("collect batch %s of adjacencies at %s output %s ", batch, on_compute_node, output)
        adjmats = read_toc_plus_payload(output, for_step="extract-edge-populations")
        return write_toc_plus_payload(adjmats, hdf_edge_population, append=True, format="table",
                                      min_itemsize={"values": 100})

    LOG.info("collect batched extraction of edges at compute node %s", on_compute_node)
    for batch, output in results.items():
        move(batch, output)

    LOG.info("DONE collecting %s", results)
    return hdf_edge_population

#+end_src

We have assumed that the stores invoked above are like the ~MatrixStore~ defined in ~connsense.analyze_connectivity.matrices~.
We do not have working version of a sparse matrices that we use to store adjacency.
Either we can implement such a store, or change the collection methods.

** Cleanup

And we will need to cleanup before rerunning a compute-node.
#+name: develop-parallelization-single-node-multiprocess-cleanup
#+begin_src python

def run_cleanup(on_compute_node):
    """..."""
    if on_compute_node.joinpath(INPROGRESS).exists() or on_compute_node.joinpath(DONE).exists():
        LOG.warning("Compute node has been run before: %s", on_compute_node)

        archive = on_compute_node.parent / "history"
        archive.mkdir(parents=False, exist_ok=True)

        history_compute_node = archive/on_compute_node.name
        if history_compute_node.exists():
            LOG.warning("Other than the existing run, there were previous ones too: \n%s",
                        list(history_compute_node.glob('*')))

        to_archive = history_compute_node/time.stamp(now=True)
        if to_archive.exists():
            LOG.warning("The last run archived at \n %s \n"
                        "must have been within the last minute of now (%s) and may be overwritten",
                        to_archive, time.stamp(now=True))
        shutil.copytree(on_compute_node, to_archive,
                        symlinks=False, ignore_dangling_symlinks=True, dirs_exist_ok=True)

    files_to_remove = ([on_compute_node / path for path in ("batched_output.json", "output.json",
                                                            INPROGRESS, DONE)]
                       + list(on_compute_node.glob("connsense*.h5")))
    LOG.info("On compute node %s, cleanup by removing files \n%s", on_compute_node.name, files_to_remove)
    for to_remove in files_to_remove:
        to_remove.unlink(missing_ok=True)

    return on_compute_node


#+end_src

* Putting it together

We can now list the code that can configure a multinode computation. which we do to keep the output Python code clean.
We will output into locations under the package ~connsense.pipeline.paralelization~.

#+header: :noweb yes :comments no :padline true
#+begin_src python  :tangle no
#+begin_src python :tangle ../pipeline/parallelization/__init__.py
"""Parallelize connsense-CRAP subtargets
"""
from .import parallelization
#+end_src

Within ~connsense.pipeline.parallelization~ we will have ~.parallelize_multinode~,

#+begin_src python :tangle no :noweb yes :comments org :padline true
,#+begin_src python :tangle "../pipeline/parallelization/parallelization.py" :noweb yes :comments org :padline true
from collections.abc import Mapping
from copy import deepcopy
import shutil
from pathlib import Path
from pprint import pformat

import json
import yaml

from multiprocessing import Process, Manager

import numpy as np
import pandas as pd

import bluepy

from connsense import extract_nodes,  plugins
from connsense.extract_connectivity import read_results
from connsense.extract_connectivity import extract as extract_connectivity
from connsense.pipeline import workspace
from connsense.pipeline import PARAMKEY, COMPKEYS
from connsense.io import logging, time, read_config as read_pipeline
from connsense.io.slurm import SlurmConfig
from connsense.io.write_results import read_toc_plus_payload, write_toc_plus_payload
from connsense.pipeline.workspace import find_base
from connsense.pipeline import ConfigurationError, NotConfiguredError
from connsense.pipeline.store.store import HDFStore
from connsense.define_subtargets.config import SubtargetsConfig
from connsense.analyze_connectivity import check_paths, matrices
from connsense.analyze_connectivity.analysis import SingleMethodAnalysisFromSource

# pylint: disable=locally-disabled, multiple-statements, fixme, line-too-long, too-many-locals, comparison-with-callable, too-many-arguments, invalid-name, unspecified-encoding, unnecessary-lambda-assignment

LOG = logging.get_logger("connsense pipeline")


def _remove_link(path):
    try:
        return path.unlink()
    except FileNotFoundError:
        pass
    return None


EXECUTABLE = {"define": "loader", "extract": "extractor", "sample": "generator", "analyze": "computation"}

BATCH_SUBTARGETS = ("subtargets.h5", "batch")
COMPUTE_NODE_SUBTARGETS = ("inputs.h5", "subtargets")
INPUTS = ("inputs.h5", "subtargets")
COMPUTE_NODE_ASSIGNMENT = ("subtargets.h5", "compute_node")

INPROGRESS = "INPROGRESS"
DONE = "DONE"

class IllegalParallelComputationError(ValueError):
    """..."""


<<parallelization-describe-computation>>

<<parallelization-process-multinode>>

<<parallelization-run-multinode-configs>>

<<parallelization-run-multinode-batched-inputs>>

<<parallelization-setup-compute-node>>

<<parallelization-write-multinode-setup>>

<<parallelization-collect-multinode-setup>>

<<parallelization-collect-edge-population>>

<<parallelization-collect-node-population>>

<<parallelization-collect-analyze-connectivity>>

<<parallelization-read-compute-node>>

<<parallelization-workspace>>

<<parallelization-write-configs>>

<<parallelization-write-configs-main>>

<<paralellization-write-configs-control>>

<<parallelization-write-configs-subgraphs>>

<<parallelization-write-description>>

<<parallelization-symlink-configs>>

<<parallelization-generate-inputs-of-inputs-generic>>

<<parallelization-pour-tap-subtarget>>

<<parallelization-parameterize-step>>

<<parallelization-configure-runtime-slurm>>

<<parallelization-configure-runtime-parallelization>>

<<parallelization-configure-runtime-batch-assignment>>

<<parallelization-configure-runtime-compute-nodes>>

<<parallelization-save-runtime-batch-run>>

<<parallelize-single-node>>

<<parallelize-single-nodel-circuit>>

<<parallelize-single-node-load-inputs>>

<<parallelize-single-node-run-batch-get-executable>>

<<parallelize-single-node-collect>>

<<parallelize-single-node-collect-batched-edge-populations>>

<<parallelize-single-node-collect-batched-node-populations>>

#+end_src

#+RESULTS:

For development we can use a develop version,

#+begin_src python :tangle "./parallelization.py" :noweb yes :comments org :padline true
from collections.abc import Mapping
from copy import deepcopy
import shutil
from pathlib import Path
from lazy import lazy
from pprint import pformat

import json
import yaml

import multiprocessing
from multiprocessing import Process, Manager

import numpy as np
import pandas as pd

from connsense import extract_nodes,  plugins
from connsense.extract_connectivity import read_results
from connsense.extract_connectivity import extract as extract_connectivity
from connsense.pipeline import workspace
from connsense.pipeline import PARAMKEY, COMPKEYS
from connsense.io import logging, time, read_config as read_pipeline
from connsense.io.slurm import SlurmConfig
from connsense.io.write_results import read_toc_plus_payload, write_toc_plus_payload
from connsense.pipeline.workspace import find_base
from connsense.pipeline import ConfigurationError, NotConfiguredError
#from connsense.pipeline.store.store import HDFStore
#from connsense.develop.topotap import HDFStore
from connsense.define_subtargets.config import SubtargetsConfig
from connsense.analyze_connectivity import check_paths, matrices
from connsense.analyze_connectivity.analysis import SingleMethodAnalysisFromSource

# pylint: disable=locally-disabled, multiple-statements, fixme, line-too-long, too-many-locals, comparison-with-callable, too-many-arguments, invalid-name, unspecified-encoding, unnecessary-lambda-assignment

LOG = logging.get_logger("connsense pipeline")


def _remove_link(path):
    try:
        return path.unlink()
    except FileNotFoundError:
        pass
    return None


EXECUTABLE = {"define": "loader", "extract": "extractor", "sample": "generator", "analyze": "computation"}

BATCH_SUBTARGETS = ("subtargets.h5", "batch")
COMPUTE_NODE_SUBTARGETS = ("inputs.h5", "subtargets")
INPUTS = ("inputs.h5", "subtargets")
COMPUTE_NODE_ASSIGNMENT = ("subtargets.h5", "compute_node")

INPROGRESS = "INPROGRESS"
DONE = "DONE"


class IllegalParallelComputationError(ValueError):
    """..."""


<<parallelization-describe-computation>>

<<parallelization-parameterize-step>>

<<develop-parallelization-batched-inputs>>

<<develop-parallelization-batched-inputs-parallel-groups>>

<<develop-parallelization-estimate-batched-inputs-load>>

<<develop-parallelization-batched-inputs-distribute-compute-nodes>>

<<develop-parallelization-batched-inputs-group-launchscripts>>

<<develop-parallelization-inputs-index>>

<<develop-parallelization-inputs-filter-datasets>>

<<develop-parallelization-inputs-dataset-lazy-load>>

<<develop-parallelization-inputs-variable-load-dataset>>

<<develop-parallelization-inputs-lazy-dataset-keywords>>

<<develop-parallelization-inputs-pour-datasets>>

<<develop-parallelization-workspace>>

<<develop-parallelization-multinode-configs>>

<<develop-parallelization-multinode-base-configs-write>>

<<develop-parallelization-multinode-base-configs-read>>

<<develop-parallelization-multinode-read-runtime>>

<<develop-parallelization-multinode-prepare-parallelization>>

<<develop-parallelization-multinode-description>>

<<develop-parallelization-inputs-reindex-datasets>>

<<develop-parallelization-multinode-datacall>>

<<develop-parallelization-multinode-generate-inputs>>

<<develop-parallelization-multinode-generate-inputs-apply-controls>>

<<develop-parallelization-multinode-generate-inputs-slice-controls>>

<<develop-parallelization-single-node-multiprocess-configure-execution-slices>>

<<develop-parallelization-multinode-configure-runtime-slurm>>

<<develop-parallelization-multinode-setup-compute-node>>

<<develop-parallelization-multinode-setup-read-write>>

<<develop-parallelization-multinode-setup-symlink-configs>>

<<develop-parallelization-multinode-setup>>

<<develop-parallelization-multinode-collect-results>>

<<develop-parallelization-multinode-collect-node-population>>

<<develop-parallelization-multinode-collect-edge-population>>

<<develop-parallelization-multinode-collect-analyze-step>>

<<develop-parallelization-single-node-multiprocess>>

<<develop-parallelization-single-node-multiprocess-circuit>>

<<develop-parallelization-single-node-multiprocess-load-inputs>>

<<develop-parallelization-single-node-multiprocess-kwargs>>

<<develop-parallelization-single-node-multiprocess-executable>>

<<develop-parallelization-single-node-multiprocess-stores>>

<<develop-parallelization-single-node-multiprocess-configure-execution>>

<<develop-parallelization-single-node-multiprocess-collect>>

<<develop-parallelization-single-node-multiprocess-collect-batched-node-populations>>

<<develop-parallelization-single-node-multiprocess-collect-batched-edge-populations>>

<<develop-parallelization-single-node-multiprocess-cleanup>>

<<develop-parallelization-inputs-control-datasets>>

<<develop-parallelization-inputs-subset-datasets>>

<<develop-parallelization-inputs-unit-computations>>
#+end_src
#+RESULTS:

* Runtime config
The runtime config provides parameters for parallelization each step in the ~connsense-TAP~.

#+name: runtime-config-init
#+begin_src yaml :tangle no :noweb yes :comments org :padline no
version: 1.0.0
date: 20220724
slurm:
  description: >-
    Configure default Slurm config.
  sbatch:
    account: "proj83"
    time: "8:00:00"
    venv: "/gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/load_env.sh"
#+end_src

** Define subtargets
Let us enter all the definitions by name, but no content to configure parallelization,
#+name: runtime-config-define-subtargets
#+begin_src yaml :tangle no :noweb yes :comments org :padline no
define-subtargets:
  description: >-
    Configure parallelization to run ~define-subtargets~.
  definitions:
    hexgrid-cells: null
    hexgrid-voxels: null
    pre-defined: null
#+end_src


** Extract voxels
#+name: runtime-config-extract-voxels
#+begin_src yaml :tangle no :noweb yes :comments org :padline no
extract-voxels:
  description: >-
    Configure parallelization to run ~extract-voxels~.
  annotations:
    layer: null
    depth: null
    flatmap: null
    orientation: null
#+end_src


** Extract node types
#+name: runtime-config-extract-node-types
#+begin_src yaml :tangle no :noweb yes :comments org :padline no
extract-node-types:
  description: >-
    Configure the extraction of node types.
  models:
    biophysical: null
#+end_src


** Extract node populations
We will extract nodes for each subtarget on it's own compute-node.

#+name: runtime-config-extract-node-populations
#+begin_src yaml :tangle no :noweb yes :comments org :padline no
extract-node-populations:
  description: >-
    Configure the extraction of node populations.
  populations:
    default:
      number-compute-nodes: 8
      number-tasks-per-node: 1
#+end_src


** Extract edge populations
We will extract nodes for each subtarget on it's own compute-node.

#+name: runtime-config-extract-edge-populations
#+begin_src yaml :tangle no :noweb yes :comments org :padline no
extract-edge-populations:
  description: >-
    Configure the extraction of edge populations.
  populations:
    local:
      number-compute-nodes: 8
      number-tasks-per-node: 1
#+end_src


** Analyze geometry
#+name: runtime-config-analyze-geometry
#+begin_src yaml :tangle no :noweb yes :comments org :padline no
analyze-geometry:
  description: >-
    Configure the analyses of a circuit subtarget geometry.
  analyses:
    layer_volume: null
    conicity: null
#+end_src


** Analyze composition
#+name: runtime-config-analyze-composition
#+begin_src yaml :tangle no :noweb yes :comments org :padline no
analyze-composition:
  description: >-
    Configure the analyses of a circuit subtarget composition.
  analyses:
    cell-count-by-layer: null
    cell-count-by-mtype: null
#+end_src


** Analyze connectivity
Edge properties may be need a lot of memory, crashing too many parallel jobs on a single node.
Let us try with 4 jobs in parallel on 1 node. For the 8 columnar subtargets this should be enough.

#+name: runtime-config-analyze-connectivity
#+begin_src yaml :tangle no :noweb yes :comments org :padline no
analyze-connectivity:
  description: >-
    Configure the analyses of a circuit subtarget connectivity.
  analyses:
    neuronal-convergence:
      number-compute-nodes: 1
      number-tasks-per-node: 4
    neuronal-divergence:
      number-compute-nodes: 1
      number-tasks-per-node: 4
    synaptic-convergence:
      number-compute-nodes: 1
      number-tasks-per-node: 4
    synaptic-divergence:
      number-compute-nodes: 1
      number-tasks-per-node: 4
#+end_src


** Results

#+begin_src yaml :tangle runtime.yaml :noweb yes :comments no :padline no
<<runtime-config-init>>
pipeline:
  <<runtime-config-define-subtargets>>
  <<runtime-config-extract-voxels>>
  <<runtime-config-extract-node-types>>
  <<runtime-config-extract-node-populations>>
  <<runtime-config-extract-edge-populations>>
  <<runtime-config-analyze-geometry>>
  <<runtime-config-analyze-composition>>
  <<runtime-config-analyze-connectivity>>
#+end_src

* Version 1
** Run a parallel process

A parallel process can be modeled as a ~map~ of a function over chunks of a collection, and then putting individual computation results back using a ~collect~ step. Parallel processes for large computations must be scheduled on the cluster, and not interactively. We have designed ~connsense-TAP~ parallelization to use multiple compute nodes, which are setup in a ~launchscript.sh~ following the scheme implied by the ~pipeline~ config.

The parallelized jobs can be launched using the script, and the results collected with another ~tap~ command. The scheme can be used for any ~pipeline~ step by providing methods for each ~computation-type~.

The first step is to setup the distributed computation on the disc in several ~compute-nodes~. The last step of collection will traverse the ~compute-nodes~ reading and gathering the computed results. The computation /batch/ information prescribed in the configuration is the same for the two, or any other step that must work with a parallelization. We can thus implement the processes of ~setup~ and ~collect~ as dispatches in the same main routine.


#+name: parallelization-process-multinode
#+begin_src python
def run_multinode(process_of, computation, in_config, using_runtime):
    """..."""
    _, to_stage = get_workspace(computation, in_config)

    using_configs = configure_multinode(process_of, computation, in_config,
                                        for_control=None, making_subgraphs=None, at_dirpath=to_stage)
    computation_type, of_quantity = describe(computation)

    inputs = generate_inputs_of(computation, in_config)
    n_compute_nodes,  n_parallel_jobs = prepare_parallelization(computation, in_config, using_runtime)
    batched = batch_multinode(process_of, inputs, computation, in_config,
                              at_path=to_stage, using_parallelization=(n_compute_nodes, n_parallel_jobs))

    if process_of == setup_compute_node:
        using_configs["slurm_params"] = configure_slurm(computation, in_config, using_runtime).get("sbatch", None)

        compute_nodes = {c: setup_compute_node(c, inputs, (computation_type, of_quantity, to_stage), using_configs)
                         for c, inputs in batched.groupby("compute_node")}
        return {"configs": using_configs,
                "number_compute_nodes": n_compute_nodes, "number_total_jobs": n_parallel_jobs,
                "setup": write_multinode_setup(compute_nodes, inputs,  at_dirpath=to_stage)}

    if process_of == collect_multinode:
        _, output_paths = read_pipeline.check_paths(in_config, step=computation_type)
        at_path = output_paths["steps"][computation_type]

        setup = {c: read_setup_compute_node(c, for_quantity=to_stage) for c,_ in batched.groupby("compute_node")}
        return collect_multinode(computation_type, setup, from_dirpath=to_stage, in_connsense_store=at_path)

    raise ValueError(f"Unknown {process_of} multinode")


#+end_src

Depending on the multinode process to run, we will write or read the configs.

#+name: parallelization-run-multinode-configs
#+begin_src python
def configure_multinode(process_of, computation, in_config, for_control, making_subgraphs, at_dirpath):
    """..."""
    if process_of == setup_compute_node:
        return write_configs_of(computation, in_config, at_dirpath,
                                with_random_shuffle=for_control, in_the_subtarget=making_subgraphs)

    if process_of == collect_multinode:
        return read_configs_of(computation, in_config, at_dirpath,
                               with_random_shuffle=for_control, in_the_subtarget=making_subgraphs)

    raise ValueError(f"Unknown {process_of} multinode")

#+end_src

The batched-inputs will be assigned compute nodes to run them on,

#+name: parallelization-run-multinode-batched-inputs
#+begin_src python
def batch_multinode(process_of, inputs, computation, in_config, at_path, using_parallelization,
                    max_launch_submissions=500):
    """..."""
    n_compute_nodes, n_parallel_jobs = using_parallelization

    if process_of == setup_compute_node:
        LOG.info("Assign batches to %s inputs", len(inputs))
        batches = assign_batches_to(inputs, upto_number=n_parallel_jobs)

        n_batches = batches.max() + 1
        LOG.info("Assign compute nodes to %s batches of %s inputs", len(batches), n_batches)
        compute_nodes = assign_compute_nodes(batches, upto_number=n_compute_nodes)

        assignment = pd.concat([batches, compute_nodes], axis=1)
        assignment_h5, dataset = COMPUTE_NODE_ASSIGNMENT

        if assignment.compute_node.nunique() <= max_launch_submissions:
            assignment.to_hdf(at_path/assignment_h5, key=dataset)
            return assignment

        submit = lambda compute_node: int(compute_node / max_launch_submissions)
        submissions = assignment.assign(submission=assignment.compute_node.apply(submit))
        submissions.to_hdf(at_path/assignment_h5, key=dataset)
        return submissions

    if process_of == collect_multinode:
        return read_compute_nodes_assignment(at_path)

    raise ValueError(f"Unknown {process_of} multinode")

#+end_src


** Setup computation

For setting up each compute node,
To ~setup_compute_node(c)~, we will need to generate inputs of the computation as described in the input config.

#+name: parallelization-setup-compute-node
#+begin_src python
def setup_compute_node(c, inputs, for_computation, using_configs):
    """..."""
    from connsense.apps import APPS
    LOG.info("Configure chunk %s with %s inputs to compute %s.", c, len(inputs), for_computation)

    computation_type, for_quantity, to_stage = for_computation

    for_compute_node = to_stage / f"compute-node-{c}"
    for_compute_node.mkdir(parents=False, exist_ok=True)
    configs = symlink_pipeline(configs=using_configs, at_dirpath=for_compute_node)

    inputs_to_read = write_compute(inputs, to_hdf=INPUTS, at_dirpath=for_compute_node)
    output_h5 = f"{for_compute_node}/connsense.h5"

    def cmd_sbatch(at_path, executable):
        """..."""
        try:
            slurm_params = using_configs["slurm_params"]
        except KeyError as kerr:
            raise RuntimeError("Missing slurm params") from kerr

        slurm_params.update({"name": computation_type, "executable": executable})
        slurm_config = SlurmConfig(slurm_params)
        return slurm_config.save(to_filepath=at_path/f"{computation_type}.sbatch")

    #of_executable = cmd_sbatch(at_path=for_compute_node, executable=APPS[computation_type])
    of_executable = cmd_sbatch(at_path=for_compute_node, executable=APPS["main"])

    def cmd_configs():
        """..."""
        return "--configure=pipeline.yaml --parallelize=runtime.yaml \\"

    def cmd_options():
        """..."""
        return None

    if "submission" not in inputs:
        launchscript = to_stage / "launchscript.sh"
    else:
        submission = inputs.submission.unique()
        assert len(submission) == 1
        launchscript = to_stage / f"launchscript-{submission[0]}.sh"

    command_lines = ["#!/bin/bash",
                     (f"########################## LAUNCH {computation_type} for chunk {c}"
                      f" of {len(inputs)} _inputs. #######################################"),
                     f"pushd {for_compute_node}",
                     f"sbatch {of_executable.name} run {computation_type} {for_quantity} \\",
                     cmd_configs(),
                     cmd_options(),
                     f"--input={inputs_to_read} \\",
                     f"--output={output_h5}",
                     "popd"]

    with open(launchscript, 'a') as to_launch:

        to_launch.write('\n'.join(l for l in command_lines if l) + "\n")

        # def write(aline):
        #     if aline:
        #         to_launch.write(aline + '\n')

        # write("#!/bin/bash")

        # write(f"########################## LAUNCH {computation_type} for chunk {c}"
        #      f" of {len(inputs)} _inputs. #######################################")
        # write(f"pushd {for_compute_node}")

        # sbatch = f"sbatch {of_executable.name} run {computation_type} {for_quantity} \\"
        # write(sbatch)
        # write(cmd_configs())
        # write(cmd_options())
        # write(f"--input={inputs_to_read} \\")
        # write(f"--output={output_h5}")

        # write("popd")

    setup = {"dirpath": for_compute_node, "sbatch": of_executable, "input": inputs_to_read, "output": output_h5}

    return read_pipeline.write(setup, to_json=for_compute_node/"setup.json")

#+end_src

#+name: parallelization-write-multinode-setup
#+begin_src python
def write_multinode_setup(compute_nodes, inputs, at_dirpath):
    """..."""
    inputs_h5, dataset = INPUTS
    #inputs.to_hdf(at_dirpath/inputs_h5, key=dataset)

    return read_pipeline.write({"compute_nodes": compute_nodes, "inputs": at_dirpath/inputs_h5},
                                to_json=at_dirpath/"setup.json")

#+end_src


** Collect results
The collected results must be written the ~connsense-TAP~ store,

#+name: parallelization-collect-multinode-setup
#+begin_src python
def collect_multinode(computation_type, setup, from_dirpath, in_connsense_store):
    """..."""
    #if not in_connsense_store.exists():
        #raise RuntimeError(f"NOTFOUND {in_connsense_h5_at_basedir}\n HDF5 for connsense in base dir must exist")

    if computation_type == "extract-node-populations":
        return collect_node_population(setup, from_dirpath, in_connsense_store)

    if computation_type == "extract-edge-populations":
        return collect_edge_population(setup, from_dirpath, in_connsense_store)

    if computation_type in ("analyze-connectivity", "analyze-composition",
                            "analyze-node-types", "analyze-physiology", "sample-edge-populations"):
        return collect_analyze_step(setup, from_dirpath, in_connsense_store)

    raise NotImplementedError(f"INPROGRESS: {computation_type}")

#+end_src

To collect extracted node populations

#+name: parallelization-collect-node-population
#+begin_src python
def collect_node_population(setup, from_dirpath, in_connsense_store):
    """..."""
    try:
        with open(from_dirpath/"description.json", 'r') as f:
            population = json.load(f)
    except FileNotFoundError as ferr:
        raise RuntimeError(f"NOTFOUND a description of the population extracted: {from_dirpath}")

    connsense_h5, group = in_connsense_store
    hdf_population = group + '/' + population["name"]

    def describe_output(of_compute_node):
        """..."""
        try:
            with open(Path(of_compute_node["dirpath"]) / "output.json", 'r') as f:
                output = json.load(f)
        except FileNotFoundError as ferr:
            raise RuntimeError(f"No output configured for compute node {of_compute_node}") from ferr
        return output

    outputs = {c: describe_output(of_compute_node) for c, of_compute_node in setup.items()}
    LOG.info("Extract node populations %s reported outputs: \n%s", population["name"], pformat(outputs))

    def in_store(at_path, hdf_group=None):
        """..."""
        return matrices.get_store(at_path, hdf_group or hdf_population, pd.DataFrame)

    def move(compute_node, output):
        """..."""
        LOG.info("Get node population store for compute-node %s output %s", compute_node, output)
        h5, g = output
        return in_store(at_path=h5, hdf_group=g)

    return in_store(connsense_h5).collect({c: move(compute_node=c, output=o) for c, o in outputs.items()})



def collect_node_population_single_dataframe(setup, from_dirpath, in_connsense_store):
    """...Deprecated"""
    from connsense.io.write_results import read as read_compute_node, write as write_compute_node
    LOG.info("Collect node population at %s using setup \n%s", from_dirpath, setup)

    try:
        with open(from_dirpath/"description.json", 'r') as f:
            population = json.load(f)
    except FileNotFoundError as ferr:
        raise RuntimeError(f"NOTFOUND a description of the population extracted: {from_dirpath}") from ferr

    def describe_output(of_compute_node):
        """..."""
        try:
            with open(Path(of_compute_node["dirpath"]) / "output.json", 'r') as f:
                output = json.load(f)
        except FileNotFoundError as ferr:
            raise RuntimeError(f"No output configured for compute node {of_compute_node}") from ferr
        return output

    #p = population["name"]
    #hdf_group = f"nodes/populations/{p}"
    connsense_h5, group = in_connsense_store

    def move(compute_node, from_path):
        """..."""
        LOG.info("Write batch %s read from %s", compute_node, from_path)
        compute_node_result = describe_output(from_path)
        result = read_compute_node(compute_node_result, "extract-node-populations")
        return write_compute_node(result, to_path=(connsense_h5, group+"/"+population["name"]),
                                  append=True, format="table")

    for compute_node, hdf_path in setup.items():
        move(compute_node, hdf_path)

    return (in_connsense_store, group+"/"+population["name"])

#+end_src

We store extracted edge population.
Assuming that the each compute node's results were collected in a dict that maps ~compute-node~ to
the path to it's HDF5 store, we can

#+name: parallelization-collect-edge-population
#+begin_src python
def collect_edge_population(setup, from_dirpath, in_connsense_store):
    """..."""
    LOG.info("Collect edge population at %s using setup \n%s", from_dirpath, setup)

    try:
        with open(from_dirpath/"description.json", 'r') as f:
            population = json.load(f)
    except FileNotFoundError as ferr:
        raise RuntimeError(f"NOTFOUND a description of the population extracted: {at_basedir}") from ferr

    #p = population["name"]
    #hdf_edge_population = f"edges/populations/{p}"
    connsense_h5, group = in_connsense_store
    hdf_edge_population = group + '/' + population["name"]

    LOG.info("Collect edges with description \n%s", pformat(population))

    def describe_output(of_compute_node):
        """..."""
        try:
            with open(Path(of_compute_node["dirpath"]) / "output.json", 'r') as f:
                output = json.load(f)
        except FileNotFoundError as ferr:
            raise RuntimeError(f"No output configured for compute node {of_compute_node}") from ferr
        return output

    outputs = {c: describe_output(of_compute_node) for c, of_compute_node in setup.items()}
    LOG.info("Edge extraction reported outputs: \n%s", pformat(outputs))

    def collect_adjacencies(of_compute_node, output):
        """..."""
        LOG.info("Collect adjacencies compute-node %s output %s", of_compute_node, output)
        adj = read_toc_plus_payload(output, for_step="extract-edge-populations")
        return write_toc_plus_payload(adj, (connsense_h5, hdf_edge_population), append=True, format="table",
                                      min_itemsize={"values": 100})
        #return write_toc_plus_payload(adj, (in_connsense_store, hdf_edge_population), append=True, format="table")

    LOG.info("Collect adjacencies")
    for of_compute_node, output in outputs.items():
        collect_adjacencies(of_compute_node, output)

    LOG.info("Adjacencies collected: \n%s", len(outputs))

    return (in_connsense_store, hdf_edge_population)

#+end_src

and for storing the results of analyses,

#+name: parallelization-collect-analyze-connectivity
#+begin_src python
def collect_analyze_step(setup, from_dirpath, in_connsense_store):
    """..."""
    try:
        with open(from_dirpath/"description.json", 'r') as f:
            analysis = json.load(f)
    except FileNotFoundError as ferr:
        raise RuntimeError(f"NOTFOUND a description of the analysis extracted: {from_dirpath}") from ferr

    #a = analysis["name"]
    #hdf_analysis = f"analyses/connectivity/{a}"
    connsense_h5, group = in_connsense_store
    hdf_analysis = group + '/' + analysis["name"]
    output_type = analysis["output"]

    def describe_output(of_compute_node):
        """..."""
        try:
            with open(Path(of_compute_node["dirpath"]) / "output.json", 'r') as f:
                output = json.load(f)
        except FileNotFoundError as ferr:
            raise RuntimeError(f"No output configured for compute node {of_compute_node}") from ferr
        return output

    outputs = {c: describe_output(of_compute_node) for c, of_compute_node in setup.items()}
    LOG.info("Analysis %s reported outputs: \n%s", analysis["name"], pformat(outputs))

    def in_store(at_path, hdf_group=None):
        """..."""
        return matrices.get_store(at_path, hdf_group or hdf_analysis, output_type)

    def move(compute_node, output):
        """..."""
        LOG.info("Get analysis store for compute-node %s output %s", compute_node, output)
        h5, g = output
        return in_store(at_path=h5, hdf_group=g)

    return in_store(connsense_h5).collect({c: move(compute_node=c, output=o) for c, o in outputs.items()})


#+end_src

the setup is read from the disc,

#+name: parallelization-read-compute-node
#+begin_src python
def read_setup_compute_node(c, for_quantity):
    """..."""
    for_compute_node = for_quantity / f"compute-node-{c}"

    if not for_compute_node.exists():
        raise RuntimeError(f"Expected compute node directory {for_compute_node} created by the TAP run to collect")

    return read_setup(at_dirpath=for_quantity, compute_node=c)


def read_setup(at_dirpath, compute_node):
    """..."""
    setup_json = at_dirpath / f"compute-node-{compute_node}" / "setup.json"

    if not setup_json.exists():
        raise RuntimeError(f"No setup json found at {setup_json}")

    with open(setup_json, 'r') as f:
        return json.load(f)

    raise RuntimeError("Python execution must not have reached here.")

#+end_src

The above distributes computations for individual subtargets over compute nodes.
Let us implement the methods used in ~configure_multinode~.

What might a ~computation~ look like? It can simply be a string read from the CLI arguments.
Consider ~computation="analyze-connectivity/degree"~, which should run analyses of degree of subtarget nodes
as specified in the configurcation. In general, following this convention, a computation will look like
~<pipelin   e-step>/<substep>~.


** Worspace for a computation
The location where a single computation, /i.e./ a computation on a single cluster node, is nested under the
~connsense~ pipeline's root.

#+name: parallelization-workspace
#+begin_src python
def get_workspace(for_computation, in_config, for_control=None, making_subgraphs=None, in_mode='r'):
    """..."""
    m = {'r': "test", 'w': "prod", 'a': "develop"}[in_mode]
    computation_type, of_quantity = describe(for_computation)
    rundir = workspace.get_rundir(in_config, computation_type, of_quantity, making_subgraphs, for_control, in_mode=m)
    basedir = workspace.find_base(rundir)
    return (basedir, rundir)

#+end_src


** Write configs: The different types of computations
There are as many different types of computations in the ~connsense~ pipeline as there are steps.
So we must provide methods used in ~configure_multinode~ for each of these steps.
However, most of these methods are the same. Let us see what the differences are by coding them.

Each computation will run in it's working folder, and thus have it's own configurations.
We write the pipeline config along with the computation's specific one's to the computation's working folder.

#+name: parallelization-write-configs
#+begin_src python
def write_configs_of(computation, in_config, at_dirpath, with_random_shuffle=None, in_the_subtarget=None):
    """..."""
    LOG.info("Write configs of %s at %s", computation, at_dirpath)
    return {"base": write_pipeline_base_configs(in_config, at_dirpath),
            "control": write_pipeline_control(with_random_shuffle, at_dirpath),
            "subgraphs": write_pipeline_subgraphs(in_the_subtarget, at_dirpath),
            "description": write_description(computation, in_config, at_dirpath)}

def read_configs_of(computation, in_config, at_dirpath, with_random_shuffle=None, in_the_subtarget=None):
    """..."""
    LOG.info("Read configs of %s at %s", computation, at_dirpath)
    return {"base": read_pipeline_base_configs(computation, in_config, at_dirpath),
            "control": read_pipeline_control(with_random_shuffle, at_dirpath),
            "subgraphs": read_pipeline_subgraphs(in_the_subtarget, at_dirpath)}
#+end_src

We have grouped ~connsense-TAP~ configs into three. The /base/ config are required, while the other two are placeholders
for features we have already implemented as part of ~connsense.analyze_connectivity~.
We can implement writing of these configs with arguments that use the config,

** The main config
We will symlink the pipeline and runtime configs,

#+name: parallelization-write-configs-main
#+begin_src python
def write_pipeline_base_configs(in_config, at_dirpath): #pylint: disable=unused-argument
    """..."""
    basedir = find_base(rundir=at_dirpath)
    LOG.info("CHECK BASE CONFIGS AT %s", basedir)
    def write_config(c):
        def write_format(f):
            filename = f"{c}.{f}"
            base_config = basedir / filename
            if base_config.exists():
                run_config = at_dirpath / filename
                _remove_link(run_config)
                run_config.symlink_to(base_config)
                return  run_config
            LOG.info("Not found config %s", base_config)
            return None
        return {f: write_format(f) for f in ["json", "yaml"] if f}
    return {c: write_config(c) for c in ["pipeline", "runtime", "config", "parallel"]}


def read_pipeline_base_configs(of_computation, in_config, at_dirpath): #pylint: disable=unused-argument
    """..."""
    LOG.info("Look for basedir of %s", at_dirpath)
    basedir = find_base(rundir=at_dirpath)
    LOG.info("CHECK BASE CONFIGS AT %s", basedir)
    def read_config(c):
        def read_format(f):
            filename = f"{c}.{f}"
            path_config = at_dirpath / filename
            if path_config.exists():
                LOG.warning("Pipeline config %s found at %s", filename, at_dirpath)
                if c in ("pipeline", "config"):
                    return read_pipeline.read(path_config)
                if c in ("runtime", "parallel"):
                    return read_runtime_config(path_config, of_pipeline=in_config)
                raise ValueError(f"NOT a connsense config: {filename}")
            LOG.warning("No pipeline config %s found at %s", filename, at_dirpath)
            return None

        return {f: read_format(f) for f in ["json", "yaml"] if f}
    return {c: read_config(c) for c in ["pipeline", "runtime", "config", "parallel"]}

#+end_src


** Controls
For analyses ~connsense~ can apply control algorihtms to the adjacency matrices that are
entered in the config, and available to ~configure_multinode~ method as argument ~for_control~
that should be an algorithm to shuffle the elements of a adjacency matrix.
The value ~for_control~ should be parsed by the pipeline setup CLI tool to an ~algorithm~.

#+name: paralellization-write-configs-control
#+begin_src python
def write_pipeline_control(algorithm, at_dirpath): #pylint: disable=unused-argument
    """..."""
    if not algorithm: return None

    if not at_dirpath.name.startswith("compute-node-"):
        control_json = at_dirpath / "control.json"
        description = deepcopy(algorithm.description)
        description["name"] = algorithm.name
        return read_pipeline.write(description, to_json=control_json)

    control_config = at_dirpath.parent / "control.json"
    if not control_config.exits():
        raise RuntimeError(f"InvalicComputeNode: {at_dirpath}. The directory's parent is missing a control config.")
    _remove_link(control_config)
    control_config.symlink_to(at_dirpath.parent / "control.json")
    return control_config

def read_pipeline_control(algorithm, at_dirpath): #pylint: disable=unused-argument
    """..."""
    if not algorithm: return None
    raise NotImplementedError("INRPOGRESS")

#+end_src


*** TODO  Develop a general approach to control

Adapted from ~connsense.analyze_connectivity~, the method to write a control will need testing
My concern is the random seed used by a given instance of the random shuffler.
The seed should be in the ~algorithm~. Test it.

But what is a control? We have applied control algorithms to the connectivity matrices before analyzing them.
This pairs an analysis and a control algorithm in the index for the results of analyzing a subtarget.

What would controlling the results of extraction of a edges be?
We do want to store randomized adjacencies of subtargets. Can we do that using controls?
Randomization of connectivity cannot be done while extracting edges -- the controls apply to the input
of a step.
Controlling inputs to edge extraction does have an interesting meaning.
Mathematically we can think of the adjacency matrix as a table of edges with a boolean value telling us if that
edge is a member of the edge population.
The inputs to edge detection are the node ~gids~ in the circuit, which mathematically are equivalent to a table
indexed by the ~gids~ and valued by booleans telling us if that ~node~ is a member of the population to consider.
Analogous to what an control algorithm does to edges, a control algorithm applied to nodes will do an equivalent thing,
that of moving them around the table.
The result of an analysis on a uniformly distributed a subarget-sized sample from the whole node population will
be a statistical control for that analysis on that subtarget.
However, within ~connsense-TAP~ we cannot sample from the whole population.
All of our analyses must apply only to a subtarget circuit extracted fromm the whole input circuit.
To make such controls possible, the input ~subtarget~ datatype should be a boolean 1D mask that represents a node's
membership in the subtarget.
That mask we can randomize.
So is there a value of pursing this at some point?

Using a 1D mask subtarget will be usefull for composition analyses.

Uniform shuffle is not very meaningfull. We should not shuffle the cells out of their position, layer, or mtype.
We should have invariants for a control.
It will be a toy.
We could randomize cell's positions given that they stay in the same layer.
Then we could extract edges. What edges would we extract?
This will show if a subtarget's nodes are less or more connected than an equivalent sample chosen randomly from
the whole population. Condition the control to keep cells in the same depth, layer, mtype, or any combination of
these to make a scientific case, and we can analyze the connectivity of the subtarget against a meaningful control.

Spatial shuffling. Any node shuffle will replace subtarget nodes with those outside the subtarget.
We could control for the replacement being at the same depth / layer and not too far from the subtarget's
/principal-axis/.
Let us say we double the thickness of a columnar subtarget. Shuffling the nodes will then give us a subtarget
with the same number of nodes but distributed in a column twice the thickness.

Consider an /in-silico/ experiment that we can do with a spatial shuffle of the sort sketched above.
We will need subtargets of several thicknesses, and the thickness scaling control applied to each.
There are two input parameters: subtarget thickness, and the thickness-scaling coefficient of the control.
The analyses results can be used illustrated using two dimensional graphic, like a /heatmap/ or a /contour-plot/,


** Subgraphs
We have nothing for subgraphs to configure. In our current setup, subgraph information is passed
by CLI arguments, while the directory layout is determined during the execution of ~configure_multinode~ method
by ~get_workspace~ method.

#+name: parallelization-write-configs-subgraphs
#+begin_src python
def write_pipeline_subgraphs(in_the_subtarget, at_dirpath): #pylint: disable=unused-argument
    """..."""
    return None


def read_pipeline_subgraphs(algorithm, at_dirpath): #pylint: disable=unused-argument
    """..."""
    if not algorithm: return None
    raise NotImplementedError("INRPOGRESS")
#+end_src


** Description of the computation

#+name: parallelization-write-description
#+begin_src python
def write_description(computation, in_config, at_dirpath):
    """..."""
    computation_type, of_quantity = describe(computation)
    configured = parameterize(computation_type, of_quantity, in_config)
    configured["name"] = of_quantity
    return read_pipeline.write(configured, to_json=at_dirpath / "description.json")
#+end_src


** Symlink in the compute node directory
Configs should be written in a ~computation~'s  ~rundir~, but ~symlinked~ to by ~compute-nodes~.

#+name: parallelization-symlink-configs
#+begin_src python
def symlink_pipeline(configs, at_dirpath):
    """..."""
    to_base = symlink_pipeline_base(configs["base"], at_dirpath)
    to_control = symlink_pipeline_control(configs["control"], at_dirpath)
    to_subgraphs = symlink_pipeline_subgraphs(configs["subgraphs"], at_dirpath)
    return {"base": to_base, "control": to_control, "subgraphs": to_subgraphs}


def create_symlink(at_dirpath):
    """..."""
    def _to(config_at_path):
        """..."""
        it_is_a = at_dirpath / config_at_path.name
        _remove_link(it_is_a)
        it_is_a.symlink_to(config_at_path)
        return it_is_a

    return _to


def symlink_pipeline_base(configs, at_dirpath):
    """..."""
    symlink_to = create_symlink(at_dirpath)
    return {"pipeline": {fmt: symlink_to(config_at_path=p) for fmt, p in configs["pipeline"].items() if p},
            "runtime": {fmt: symlink_to(config_at_path=p) for fmt, p in configs["runtime"].items() if p}}


def symlink_pipeline_control(to_config, at_dirpath):
    """..."""
    return create_symlink(at_dirpath)(to_config) if to_config else None


def symlink_pipeline_subgraphs(to_config, at_dirpath):
    """..."""
    return create_symlink(at_dirpath)(to_config) if to_config else None

#+end_src


** Inputs

The inputs to a ~computation~ will also depend on the pipeline step that the ~copmutation~ is at.
If the computation is to extract an edge population, the inputs will be subtargets.

If the computation is to analyze connectivity, the inputs will be the edges and nodes that apply, /i.e/ the network.
The edge population is part of the argued ~computation~, and their source and target node populations are in
the configuration.

We can add other computation types when it is time to run them, and collect them in an interface.

** A generic generator of inputs

The ~generate_inputs_of(computation, in_config)~ method above may need a lot of cases for individual pipeline steps.
Let us try to devise a more generic version that will rely on the ~connsense.pipeline.store.HDFStore~ to fetch data
using the config.

#+name: parallelization-generate-inputs-of-inputs-generic
#+begin_src python

def input_units(computation, to_tap):
    """..."""
    parameters = parameterize(*describe(computation), to_tap._config)
    index_vars = parameters.get("index", parameters["input"])
    if len(index_vars) > 1:
        return pd.MultiIndex.from_product([to_tap.subset_index(var, values) for var, values in index_vars.items()])

    var, values = next(iter(index_vars.items()))
    return pd.Index(to_tap.subset_index(var, values))


def filter_input_datasets(described):
    """..."""
    return {var: val for var, val in described.items() if (var not in ("circuit", "connectome")
                                                           and isinstance(val, Mapping) and "dataset" in val)}


def generate_inputs_of(computation, in_config, on_compute_node=None, by_subtarget=False):
    """..."""
    LOG.info("Generate inputs for %s %s %s", computation,
             "by subtarget" if by_subtarget else "", on_compute_node if on_compute_node else "")

    computation_type, of_quantity = describe(computation)
    described = parameterize(computation_type, of_quantity, in_config)

    tap = HDFStore(in_config)

    inputs_h5, dataset = INPUTS
    input_batches = pd.read_hdf(on_compute_node/inputs_h5, dataset) if on_compute_node else None

    variables = filter_input_datasets(described["input"])

    unit_computations = input_units(computation, tap)
    LOG.info("There will be %s unit computations with index %s", len(unit_computations), unit_computations.names)
    unit_slices = [unit_computations[s:s+1] for s in range(0, len(unit_computations))]

    #input_datasets = pd.DataFrame({var: tap.pour_dataset(var, values["dataset"]).loc[unit_computations]
                                   #for var, values in variables.items()})
    dataflow = pour(tap, variables)
    input_datasets = pd.Series([dataflow(s) for s in unit_slices], index=unit_computations)
    return (lambda s: input_datasets.loc[s]) if by_subtarget else input_datasets


    #input_datasets = unit_computations.apply(dataflow, axis=1) #causes stop iteration for some cases!
    #input_datasets = pd.Series([dataflow(row) for i, row in unit_computations.iterrows()],
                               #index=pd.MultiIndex.from_frame(unit_computations))

    return get_subtarget(input_datasets) if by_subtarget else input_datasets


#+end_src

Data for each subtarget will be poured from the ~HDFStore tap~ as follows,

#+name: parallelization-pour-tap-subtarget
#+begin_src python
def pour(tap, variables):
    """.."""
    #input_datasets = {var: tap.pour_dataset(var, vals["dataset"]) for var, vals in variables.items()}

    LOG.info("Pour tap variables %s", pformat(variables))
    print("Pour tap variables %s"%pformat(variables))

    def unpack(value):
        """..."""
        try:
            get = value.get_value
        except AttributeError:
            return value
        return get()

    def group_properties(var):
        """..."""
        properties = variables[var].get("properties", None)
        print("Group properties for variable %s: %s"%(var, properties))

        def apply(subtarget):
            """..."""
            if not properties:
                return subtarget

            return lambda index: subtarget[properties].loc[index]

        return apply

    def load_dataset(var, values):
        """..."""
        LOG.info("To pour, load %s dataset ", var)
        print("To pour, load %s dataset "%var)
        dataset = tap.pour_dataset(var, values["dataset"]).apply(unpack)
        if not isinstance(dataset.index, pd.MultiIndex):
            dataset.index = pd.MultiIndex.from_tuples([(x,) for x in dataset.index.values],
                                                      names=[dataset.index.name,])

        if not "reindex" in values:
            return dataset

        original = dataset.apply(lambda subtarget: tap.reindex(subtarget, variables=values["reindex"]))
        return pd.concat(original.values, axis=0, keys=original.index.values, names=original.index.names)

    LOG.info("Input datasets for variables: %s", variables.keys())
    print("Input datasets for variables: %s"%variables.keys())
    input_datasets = {variable: load_dataset(variable, values).apply(group_properties(variable))
                      for variable, values in variables.items()}
    LOG.info("\t: %s", input_datasets.keys())
    print("\t: %s"%input_datasets.keys())
    LOG.info("Pour tap to get elements of \n%s", pformat(input_datasets))

    def loc(subtarget):
        """..."""
        def get(variable):
            """..."""
            try:
                value_subtarget = input_datasets[variable].loc[subtarget]
            except KeyError as kerr:
                LOG.warning("Subtarget %s\n not found in input datasets %s", subtarget, variable)
                return None
            return value_subtarget

        subtarget_input_dataset = {}

        for variable in input_datasets:
            values = get(variable)
            LOG.info("Input values for %s: \n%s", variable, values.describe())
            print("Input values for %s: \n%s"%(variable, values.describe()))
            if values is None:
                return None
            subtarget_input_dataset[variable] = values

        return subtarget_input_dataset

    def loc_0(subtarget):
        """..."""
        LOG.info("Locate subtarget \n%s ", subtarget)

        def get_dataset(var, value):
            """..."""
            LOG.info("To pour on \n%s\n, get dataset %s, %s", subtarget, var, value)
            #dataset = tap.pour_subtarget(value["dataset"], subset=lookup)
            dataset = tap.pour_dataset(var, value["dataset"])
            try:
                value = dataset.get_value
            except AttributeError:
                return dataset
            return value()

        def get_transformation(value):
            """..."""
            return {k: v for k, v in value.items() if k != "dataset"}

        return pd.Series({var: vals.loc[subtarget] for var, vals in input_datasets.items()})

        return pd.Series({var: evaluate(get_transformation(value), get_dataset(var, value))
                          for var, value in variables.items()})

    def evaluate(transformation, of_dataset):
        """..."""
        transform = resolve(transformation)
        return transform(of_dataset)

    def resolve(transformation):
        """..."""
        if not transformation:
            return lambda x: x

        try:
            _, transform = plugins.import_module(transformation)
        except plugins.ImportFailure:
            pass
        else:
            return transform

        to_filter = get_filter(transformation)
        to_type = get_properties(transformation)

        return lambda dataset: to_type(to_filter(dataset))

    return lazily(to_evaluate=loc)


def get_filter(transformation):
    """..."""
    if "filter" not in transformation:
        return lambda dataset: dataset

    def apply(dataset):
        """..."""
        raise NotImplementedError

    return apply


def get_properties(transformation):
    """..."""
    g = transformation.get("properties", None)

    if isinstance(g, (list, tuple)) and len(g) == 1:
        g = g[0]

    def apply(dataset):
        """..."""
        def to_node(ids):
            """..."""
            return dataset[g].loc[ids].reset_index(drop=True) if g is not None else ids
        return to_node
    return apply


    return lazily(loc)


def lazily(to_evaluate):
    """..."""
    LOG.info("Evaluate %s lazily", to_evaluate.__name__)
    return lambda subtarget: lambda: to_evaluate(subtarget)

#+end_src


#+name: abandoned-parallelization-pour-tap-subtarget-frozen
#+begin_src python
def index_units_pre_create_index(computation, in_config):
    """..."""
    computation_type, of_quantity = describe(computation)
    parameters = parameterize(computation_type, of_quantity, in_config)

    def check_input():
        """..."""
        try:
            inputs = parameters["input"]
        except KeyError:
            return {}
        indices = {"circuit": inputs.get("circuit", None), "connectome": inputs.get("connectome", None),
                   "dataset": inputs.get("subtarget", {}).get("dataset", None)}
        return {key: value for key, value in indices.items() if value}

    indices = parameters.get("index", {}); indices.update(check_input())

    if not indices:
        raise NotConfiguredError(f"MISSING unit of computation for {computation}")

    if "dataset" not in indices:
        raise IllegalParallelComputationError("WITHOUT a dataset to iterate")

    return indices


def input_units_pre_create_index(computation, to_tap):
    """..."""
    indices = index_units(computation, to_tap._config)
    return to_tap.index_contents(indices)



def pour(tap, variables):
    """.."""
    def tap_dataset(value):
        """..."""
        dataset = tap.pour_subtarget(value["dataset"])
        #LOG.info("in dataset index \%s", dataset.index.names)
        return dataset

    def loc(subtarget):
        """..."""
        #LOG.info("Locate subtarget \n%s ", subtarget)
        def lookup(to_dataset):
            """..."""
            index = tuple(subtarget[var] for var in subtarget.index if var in to_dataset.index.names)
            #LOG.info("Look up %s in dataset indexed by %s", index, to_dataset.index.names)
            return index

        def get_dataset(var, value):
            """..."""
            LOG.info("To pour on \n%s\n, get dataset %s, %s", subtarget, var, value)
            #dataset = tap.pour_subtarget(value["dataset"], subset=lookup)
            dataset = tap.pour_dataset(var, value["dataset"])
            try:
                value = dataset.get_value
            except AttributeError:
                return dataset
            return value()

        def get_transformation(value):
            """..."""
            return {k: v for k, v in value.items() if k != "dataset"}

        return pd.Series({var: evaluate(get_transformation(value), get_dataset(var, value))
                          for var, value in variables.items()})

    def evaluate(transformation, of_dataset):
        """..."""
        transform = resolve(transformation)
        return transform(of_dataset)

    def resolve(transformation):
        """..."""
        if not transformation:
            return lambda x: x

        try:
            _, transform = plugins.import_module(transformation)
        except plugins.ImportFailure:
            pass
        else:
            return transform

        to_filter = get_filter(transformation)
        to_type = get_properties(transformation)

        return lambda dataset: to_type(to_filter(dataset))

    return lazily(to_evaluate=loc)


def get_filter(transformation):
    """..."""
    if "filter" not in transformation:
        return lambda dataset: dataset

    def apply(dataset):
        """..."""
        raise NotImplementedError

    return apply


def get_properties(transformation):
    """..."""
    g = transformation.get("properties", None)

    if isinstance(g, (list, tuple)) and len(g) == 1:
        g = g[0]

    def apply(dataset):
        """..."""
        def to_node(ids):
            """..."""
            return dataset[g].loc[ids].reset_index(drop=True) if g is not None else ids
        return to_node
    return apply


    return lazily(loc)


def lazily(to_evaluate):
    """..."""
    LOG.info("Evaluate %s lazily", to_evaluate.__name__)
    return lambda subtarget: lambda: to_evaluate(subtarget)

#+end_src

#+name: abandoned-parallelization-pour-tap-subtarget-in-deep-freeze
#+begin_src python
def index_subtargets(variable, value, in_config):
    """..."""
    circuit_kwargs = subtarget_circuit_args(value["dataset"], in_config)
    circuit_args = tuple(circuit_kwargs[k] for k in ["circuit", "connectome"] if circuit_kwargs[k])

    def add_circuit_args(subtarget):
         """..."""
         return (*circuit_args, subtarget)

    return add_circuit_args


def pour_frozen(tap, variables, in_config):
    """..."""
    def for_subtarget(s):
        """..."""
        def args(variable, value):
            """..."""
            subtarget_index = index_subtargets(variable, value, in_config)(s)
            return ({k: v for k, v in value.items() if k != "dataset"}, get_dataset(value, subtarget_index))
        return pd.Series({variable: evaluate(*args(variable, value)) for variable, value in variables.items()})

    def get_dataset(value, subtarget_index):
        """..."""
        data = tap.pour_subtarget(value["dataset"], subtarget_index)
        try:
            return data.get_value()
        except AttributeError:
            pass
        return data

    def evaluate(transformation, of_dataset):
        """..."""
        transform = resolve(transformation)
        return transform(of_dataset)

    def resolve(transformation):
        """..."""
        if not transformation:
            return lambda x: x

        try:
            _, transform = plugins.import_module(transformation)
        except plugins.ImportFailure:
            pass
        else:
            return transform

        to_filter = get_filter(transformation)
        to_type = get_properties(transformation)

        return lambda dataset: to_type(to_filter(dataset))

    return lazily(to_evaluate=for_subtarget)


def get_filter(transformation):
    """..."""
    if "filter" not in transformation:
        return lambda dataset: dataset

    def apply(dataset):
        """..."""
        raise NotImplementedError

    return apply


def get_properties(transformation):
    """..."""
    g = transformation.get("properties", None)

    if isinstance(g, (list, tuple)) and len(g) == 1:
        g = g[0]

    def apply(dataset):
        """..."""
        def to_node(ids):
            """..."""
            return dataset[g].loc[ids].reset_index(drop=True) if g is not None else ids
        return to_node
    return apply


def lazily(to_evaluate):
    """..."""
    return lambda subtarget: lambda: to_evaluate(subtarget)
#+end_src

To use this new method we will need to refactor ~define-subtargets~.  We will code the refactor somewhere else,
but we introduce the configuration that the refactor will handle here:

What we have realized is that  it is the subtarget index that defines a ~connsense-CRAP~,
within which we can define several families of subtargets

#+begin_src yaml
define-subtargets:
  members:
    - "S1DZO"
    - "S1DZ"
    - "S1FL"
    - "S1HL"
    - "S1J"
    - "S1Sh"
    - "S1Tr"
    - "S1ULp"
  defintions:
    mosaic:
      description: >-
        All the nodes as one subtarget
      input:
        circuit: "Bio_M"
      loader:
        source: connsense.define_subargets.bluepy
        method: target_mosaic
      output: pandas.Series
    central-columns:
      input:
        circuit: "Bio_M"
      loader:
        source: connsense.define_subargets.bluepy
        method: target_central_column
      output: pandas.Series
#+end_src

then extract nodes,

#+begin_src  yaml
extract-node-populations:
  populations:
    default/mosaic:
      circuit: "Bio_M"
      properties: <<properties-to-extract>>
      input:
        subtargets:
          dataset: ["define-subtargets", "mosaic"]
      extractor:
        source: connsense.extract_nodes.bluepy
        method: extract_nodes
      output: pandas.DataFrame
    default/central-columns:
      properties: <<properties-to-extract>>
      input:
        circuit: "Bio_M"
        subtargets:
          dataset: ["define-subtargets", "central-columns"]
      extractor:
        source: connsense.extract_nodes.bluepy
        method: extract_nodes
      output: pandas.DataFrame

#+end_src

and extract edges,

#+begin_src yaml
extract-edge-populations:
  populations:
    local/target-central-columns:
      circuit: "Bio_M"
      connectome: "local"
      input:
        source_nodes:
          dataset: ["define-subtargets", "mosaic"]
        target_nodes:
          dataset: ["define-subtargets", "central-columns"]
      extractor:
        source: connsense.extract_connectivity.bluepy
        method: extract_edge_population
      output:
        adj: scipy.sparse.spmatrix
        props: pandas.DataFrame
#+end_src

and analyses
#+begin_src yaml
analyze-connectivity:
  analyses:
    neuron-mosaic-convergence:
      description: >-
        Count the number of edges coming in from the whole circuit to nodes among a central-column.
      input:
        adjacency:
          dataset: ["extract-edge-populations", "local/target-central-columns"]
        from_source:
          dataset: ["extract-node-populations", "local/mosaic"]
          groupy: Cell.MTYPE
        to_target:
          dataset: ["extract-node-populations", "local/central-columns"]
      output: pandas.DataFrame
#+end_src


** Abandoned old implementations

#+name: abandoned-parallelization-inputs
#+begin_src python
def input_subtargets(in_config):
    """..."""
    _, output_paths = read_pipeline.check_paths(in_config, "define-subtargets")
    path_subtargets = output_paths["steps"]["define-subtargets"]
    LOG.info("Read subtargets from %s", path_subtargets)

    subtargets = read_results(path_subtargets, for_step="define-subtargets")
    LOG.info("Read %s subtargets", len(subtargets))

    #subset for testing
    # from bluepy import Cell
    # circuit = input_circuit("Bio_M", in_config)
    # def filter_l1(gids):
    #     layers = circuit.cells.get(gids, Cell.LAYER)
    #     return list(layers[layers==1].index.values)
    # return subtargets.apply(filter_l1)

    return subtargets


def generate_inputs_of(computation, in_config, by_subtarget=False, on_compute_node=None):
    """..."""
    LOG.info("Generate inputs for  %s ", computation)

    def get_subtarget(input_dataset):
        """..."""
        return lambda s: input_dataset.loc[s]

    computation_type, of_quantity = describe(computation)
    if computation_type == "extract-edge-populations":
        input_dataset = input_subtargets(in_config)
        return get_subtarget(input_dataset) if by_subtarget else input_dataset

    input_paths, _ = read_pipeline.check_paths(in_config, step=computation_type)

    parameters = parameterize(computation_type, of_quantity, in_config)

    if computation_type == "extract-node-types":
        circuit = input_circuit(parameters["input"], in_config)
        if not circuit:
            raise RuntimeError("MIssing circuit to extract-node-types")

        if parameters["input"]:
            raise RuntimeError(f"UNADMISSABLE arguments {parameters['input']} to extract-node-types")

        extractor = parameters["extractor"]
        _, extract = plugins.import_module(extractor["source"], extractor["method"])
        input_dataset = extract(circuit)
    else:
        cfg_inputs = parameters["input"].items()
        inputs = [load_connsense_input(dset, in_config, with_name=arg) for arg, dset in cfg_inputs
                  if arg not in ("circuit", "connectome")]
        if len(inputs) == 1:
            input_dataset = inputs[0]
        else:
            input_dataset = pd.concat(inputs, axis=1)

    return get_subtarget(input_dataset) if by_subtarget else input_dataset

#+end_src


What kind of inputs may a computation have? Either a circuit, or the hdf-path of dataset, or both!

#+name: abandoned-parallelization-load-inputs
#+begin_src python
def load_connsense_input(computation, in_config, with_name):
    """..."""
    from ..io.write_results import read
    computation_type, of_quantity = describe(computation)
    LOG.info("Load connsense input %s %s", computation_type, of_quantity)

    input_paths, _ = read_pipeline.check_paths(in_config, step=computation_type)
    hdf_path, group = input_paths["steps"][computation_type]

    if computation_type == "define-subtargets":
        assert not of_quantity or of_quantity == ""
        subtargets = read((hdf_path, group), for_step=computation_type)

        # subset for testing
        # from bluepy import Cell
        # circuit = input_circuit("Bio_M", in_config)
        # def filter_l1(gids):
        #     layers = circuit.cells.get(gids, Cell.LAYER)
        #     return list(layers[layers==1].index.values)
        # return subtargets.apply(filter_l1)


        return subtargets

    key = f"{group}/{of_quantity}"

    if computation_type in ("extract-node-types", "extract-node-populations"):
        return read((hdf_path, key), for_step=computation_type)

    if computation_type == "extract-edge_populations":

        if dataset.endswith("/adj"):
            return read_toc_plus_payload((hdf_path, key)).rename(with_name)

        if dataset.endswith("/props"):
            return (matrices.get_store(hdf_path, key, for_matrix_type="pandas.DataFrame", in_mode='r').toc
                    .rename(with_name))

        raise RutimeError(f"Unknown dataset for results of extract-edge-population: {dataset}")

    parameters = parameterize(computation_type, of_quantity, in_config)

    if computation_type.startswith("analyze-"):
        return (matrices.get_store(hdf_path, key, for_matrix_type=parameters["output"], in_mode='r').toc.
                rename(with_name))

    raise NotImplementedError(f"computation type of {computation_type}")

#+end_src


** Configure runtime
The results of ~configure_multinode~ will be written to a Slurm configuration and listed in a launchscript.
The Slurm configuration of a computation can be read from the runtimr config.

** Configure Slurm
#+name: parallelization-configure-runtime-slurm
#+begin_src python
def configure_slurm(computation, in_config, using_runtime):
    """..."""
    computation_type, quantity = describe(computation)
    pipeline_config = in_config if isinstance(in_config, Mapping) else read_pipeline.read(in_config)
    from_runtime = (read_runtime_config(for_parallelization=using_runtime, of_pipeline=pipeline_config)
                    if not isinstance(using_runtime, Mapping) else using_runtime)

    params = from_runtime["pipeline"].get(computation_type, {})
    try:
        configured = params[quantity]
    except KeyError:
        quantity, component = quantity.split('/')
        configured = params[quantity][component]
    return configured

#+end_src

We will submit one Slurm job per compute-node,


** Parallelization
To configure parallelization of a ~connsense-TAP~ step.
Each ~connsense-TAP~ step should be configured in the runtime config providing the number of compute nodes,
and the number of tasks per node.

#+name: parallelization-configure-runtime-parallelization
#+begin_src python
def read_njobs(to_parallelize, computation_of):
    """..."""
    if not to_parallelize:
        return (1, 1)

    try:
        q = computation_of.name
    except AttributeError:
        q = computation_of

    try:
        p = to_parallelize[q]
    except KeyError:
        if '/' in q:
            try:
                q0, q1 = q.split('/')
            except ValueError: #TODO: log something
                return (1, 1)
            else:
                try:
                    p0 = to_parallelize[q0]
                except KeyError:
                    return (1, 1)
                else:
                    try:
                        p = p0[q1]
                    except KeyError:
                        return (1, 1)
                    else:
                        pass
        else:
            return (1, 1)

    compute_nodes = p["number-compute-nodes"]
    tasks = p["number-tasks-per-node"]
    return (compute_nodes, compute_nodes * tasks)


def read_runtime_config(for_parallelization, of_pipeline=None, return_path=False):
    """..."""
    assert not of_pipeline or isinstance(of_pipeline, Mapping), of_pipeline

    if not for_parallelization:
        return (None, None) if return_path else None

    try:
        path = Path(for_parallelization)
    except TypeError:
        assert isinstance(for_parallelization, Mapping)
        path = None
        config = for_parallelization
    else:
        if path.suffix.lower() in (".yaml", ".yml"):
            with open(path, 'r') as fid:
                config = yaml.load(fid, Loader=yaml.FullLoader)
        elif path.suffix.lower() == ".json":
            with open(path, 'r') as fid:
                config = json.load(fid)
        else:
            raise ValueError(f"Unknown config type {for_parallelization}")

    if not of_pipeline:
        return (path, config) if return_path else config

    from_runtime = config["pipeline"]
    default_sbatch = lambda : deepcopy(config["slurm"]["sbatch"])

    def configure_slurm_for(computation_type):
        """..."""
        LOG.info("Configure slurm for %s", computation_type)
        try:
            cfg_computation_type = of_pipeline["parameters"][computation_type]
        except KeyError:
            return None

        paramkey = PARAMKEY[computation_type]
        try:
            quantities_to_configure = cfg_computation_type[paramkey]
        except KeyError:
            LOG.warning("No quantities to configure for %s". computation)
            return None

        try:
            runtime = from_runtime[computation_type]
        except KeyError:
            LOG.warning("No runtime configured for computation type %s", computation_type)
            return None

        configured = runtime[paramkey]

        def decompose_quantity(q):
            """..."""
            return [var for var in quantities_to_configure[q].keys() if var not in COMPKEYS]

        def configure_quantity(q):
            """..."""
            q_cfg = deepcopy(configured.get(q) or {})
            if "sbatch" not in q_cfg:
                q_cfg["sbatch"] = default_sbatch()
            if "number-compute-nodes" not in q_cfg:
                q_cfg["number-compute-nodes"] = 1
            if "number-tasks-per-node" not in q_cfg:
                q_cfg["number-tasks-per-node"] = 1

            def configure_component(c):
                """..."""
                cfg = deepcopy(configured.get(q, {}).get(c, {}))
                if "sbatch" not in cfg:
                    cfg["sbatch"] = q_cfg["sbatch"]
                if "number-compute-nodes" not in cfg:
                    cfg["number-compute-nodes"] = q_cfg["number-compute-nodes"]
                if "number-tasks-per-node" not in cfg:
                    cfg["number-tasks-per-node"] = q_cfg['number-tasks-per-node']

                return cfg

            for c in decompose_quantity(q):
                q_cfg[c] = configure_component(c)

            return q_cfg

        return {q: configure_quantity(q) for q in quantities_to_configure if q != "description"}

    runtime_pipeline = {c: configure_slurm_for(computation_type=c) for c in of_pipeline["parameters"]}
    config = {"version": config["version"], "date": config["date"], "pipeline": runtime_pipeline}
    return (path, config) if return_path else config


def prepare_parallelization(computation, in_config, using_runtime):
    """.."""
    computation_type, quantity = describe(computation)
    from_runtime = (read_runtime_config(for_parallelization=using_runtime, of_pipeline=in_config)
                    if not isinstance(using_runtime, Mapping) else using_runtime)
    LOG.info("prepare parallelization %s using runtime \n%s", computation, pformat(from_runtime))
    configured = from_runtime["pipeline"].get(computation_type, {})
    LOG.info("\t Configured \n%s", configured)
    return read_njobs(to_parallelize=configured, computation_of=quantity)

#+end_src

*** Batch assignement

We will assign every input subtarget a batch that will be queued on a compute node. The pipeline distinguishes between input data and it's index. The index may be defined inbdependently of the input --- which will be the case when a subtarget is reindexed to have additional levels such as source and target ~mtype-ids~. The mtypes should be defined independently of the subtarget itself --- and certain mtypes or pathways may not show up in some subtarget. In that case input-data will not contain the queried index entry. In the following we will filter such index entries out.

#+name: parallelization-configure-runtime-batch-assignment
#+begin_src python
def assign_batches_to(inputs, upto_number, return_load=False):
    """..."""
    def estimate_load(input_data):
        """Needs improvement.."""
        LOG.info("Estimate load for type %s", type(input_data))
        if input_data is None:
            return None

        if callable(input_data):
            return estimate_load(input_data())

        if isinstance(input_data, Mapping):
            if not input_data:
                return 1.
            first = next(v for v in input_data.values())
            return estimate_load(first)

        if isinstance(input_data, pd.Series):
            return input_data.apply(estimate_load).sum()

        try:
            shape = input_data.shape
        except AttributeError:
            pass
        else:
            return np.prod(shape)

        try:
            return len(input_data)
        except TypeError:
            pass

        return 1.

    if isinstance(inputs, pd.Series):
        weights = inputs.apply(estimate_load).sort_values(ascending=True).rename("estimated_load")
    elif isinstance(inputs, pd.DataFrame):
        weights = inputs.apply(estimate_load, axis=1).sort_values(ascending=True).rename("estimated_load")
    else:
        raise TypeError(f"Unhandled type of input: {inputs}")

    nan_weights = weights[weights.isna()]
    if len(nan_weights) > 0:
        LOG.warning("No input data for %s / %s inputs:\n%s", len(nan_weights), len(weights), pformat(nan_weights))
        weights = weights.dropna()

    computational_load = (np.cumsum(weights) / weights.sum()).rename("estimated_load")
    n = np.minimum(upto_number, len(weights))
    batches = (n * (computational_load - computational_load.min())).apply(int).rename("batch")

    LOG.info("Load balanced batches for %s inputs: \n %s", len(inputs), batches.value_counts())
    return batches if not return_load else pd.concat([batches, weights/weights.sum()], axis=1)
    #return batches.loc[inputs.index]

#+end_src

#+RESULTS: parallelization-configure-runtime-batch-assignment
: None

*** Compute nodes
To run a multi-compute-node copmutation we will assign compute nodes,

#+name: parallelization-configure-runtime-compute-nodes
#+begin_src python
def assign_compute_nodes(batches, upto_number):
    """..."""
    LOG.info("Assign compute nodes to batches \n%s", batches)
    _, dataset = COMPUTE_NODE_ASSIGNMENT

    assignment = pd.Series(np.linspace(0, upto_number - 1.e-6, batches.max() + 1, dtype=int)[batches.values],
                           name=dataset, index=batches.index)
    return assignment


def read_compute_nodes_assignment(at_dirpath):
    """..."""
    assignment_h5, dataset = COMPUTE_NODE_ASSIGNMENT

    if not (at_dirpath/assignment_h5).exists():
        raise RuntimeError(f"No compute node assignment saved at {at_dirpath}")

    return pd.read_hdf(at_dirpath / assignment_h5, key=dataset)

#+end_src

*** Batch run
Method ~configure_multinode~ will only write the configurations each of which willl be used to
run a single node computation. When distributed overl multiple compute nodes, each compute node will get
only a chunk of the inputs. We will need to save the batch of inputs to be sent to a compute node in that
compute node's rundir.

#+name: parallelization-save-runtime-batch-run
#+begin_src python
def write_compute(batches, to_hdf, at_dirpath):
    """..."""
    batches_h5, and_hdf_group = to_hdf
    batches.to_hdf(at_dirpath / batches_h5, key=and_hdf_group, format="fixed", mode='w')
    return at_dirpath / batches_h5

#+end_src



** Single node parallelization
The parallelization setup we have discussed will run several on compute nodes. For each compute node, the executable
is loaded from ~connsense.apps.APPS~. Parallelization of each compute-node's job is expected to be implemneted in
the executable. We can provide a wrapper that runs a ~multiprocess~ job on each compute node.
The input stored on each ~compute-node~ contains batch index for each subtarget. We just need a loop,

#+name: parallelize-single-node
#+begin_src python

def load_kwargs(parameters, to_tap, on_compute_node, consider_input=False):
    """..."""
    def load_dataset(value):
        """..."""
        return (to_tap.read_dataset(value["dataset"]) if isinstance(value, Mapping) and "dataset" in value
                else value)

    kwargs = parameters.get("kwargs", {})
    kwargs.update({var: load_dataset(value) for var, value in kwargs.items() if var not in COMPKEYS})

    if consider_input:
        kwargs.update({var: value for var, value in parameters.get("input", {}).items()
                       if var not in ("circuit", "connectome") and (
                               not isinstance(value, Mapping) or "dataset" not in value)})

    try:
        workdir = kwargs["workdir"]
    except KeyError:
        return kwargs

    if isinstance(workdir, Path):
        return kwargs

    if isinstance(workdir, str):
        path = Path(workdir)/on_compute_node.relative_to(to_tap._root.parent)
        path.parent.mkdir(parents=True, exist_ok=True)

        if path.exists():
            LOG.warning("Compute node has been run before, leaving a workdir:\n%s", path)
            archive = path.parent / "history"
            archive.mkdir(parents=False, exist_ok=True)

            history = archive / path.name
            if history.exists():
                LOG.warning("Previous runs exist in %s history at \n%s", path.name, history)
            history.mkdir(parents=False, exist_ok=True)

            to_archive = history/ time.stamp(now=True)
            if to_archive.exists():
                LOG.warning("There is a previous run with the same time stamp as now!!!\n%s"
                            "\n It will be removed", to_archive)
                shutil.copytree(path, to_archive,
                                symlinks=True, ignore_dangling_symlinks=True, dirs_exist_ok=True)
                for filepath in path.glob('*'):
                    filepath.unlink()
            else:
                path.rename(to_archive)

        path.mkdir(parents=False, exist_ok=True)

        try:
            on_compute_node.joinpath("workdir").symlink_to(path)
        except FileExistsError as ferr:
            LOG.warn("Symlink to workdir compute-node %s already exists, most probably from a previous run"
                        " Please cleanup before re-run", str(on_compute_node))

        kwargs["workdir"] = path
        return kwargs

    if workdir is True:
        workdir = on_compute_node / "workdir"
        workdir.mkdir(parents=False, exist_ok=True)
        kwargs["workdir"] = workdir
        return kwargs

    raise NotImplementedError(f"What to do with workdir type {type(workdir)}")


def run_cleanup(on_compute_node):
    """..."""
    if on_compute_node.joinpath(INPROGRESS).exists() or on_compute_node.joinpath(DONE).exists():
        LOG.warning("Compute node has been run before: %s", on_compute_node)

        archive = on_compute_node.parent / "history"
        archive.mkdir(parents=False, exist_ok=True)

        history_compute_node = archive/on_compute_node.name
        if history_compute_node.exists():
            LOG.warning("Other than the existing run, there were previous ones too: \n%s",
                        list(history_compute_node.glob('*')))

        to_archive = history_compute_node/time.stamp(now=True)
        if to_archive.exists():
            LOG.warning("The last run archived at \n %s \n"
                        "must have been within the last minute of now (%s) and may be overwritten",
                        to_archive, time.stamp(now=True))
        shutil.copytree(on_compute_node, to_archive,
                        symlinks=False, ignore_dangling_symlinks=True, dirs_exist_ok=True)

    files_to_remove = ([on_compute_node / path for path in ("batched_output.json", "output.json",
                                                            INPROGRESS, DONE)]
                       + list(on_compute_node.glob("connsense*.h5")))
    LOG.info("On compute node %s, cleanup by removing files \n%s", on_compute_node.name, files_to_remove)
    for to_remove in files_to_remove:
        to_remove.unlink(missing_ok=True)

    return on_compute_node


def run_multiprocess(of_computation, in_config, using_runtime, on_compute_node, inputs=None):
    """..."""
    on_compute_node = run_cleanup(on_compute_node)

    run_in_progress = on_compute_node.joinpath(INPROGRESS)
    run_in_progress.touch(exist_ok=False)

    execute, to_store_batch, to_store_one = configure_execution(of_computation, in_config, on_compute_node)

    assert to_store_batch or to_store_one
    assert not (to_store_batch and to_store_one)

    computation_type, of_quantity = describe(of_computation)
    parameters = parameterize(computation_type, of_quantity, in_config)

    in_hdf = "connsense-{}.h5"

    circuit_kwargs = input_circuit_args(of_computation, in_config, load_circuit=True)
    circuit_args = tuple(k for k in ["circuit", "connectome"] if circuit_kwargs[k])
    circuit_args_values = tuple(v for v in (circuit_kwargs["circuit"], circuit_kwargs["connectome"]) if v)
    circuit_args_names = tuple(v for v in ((lambda c: c.variant if c else None)(circuit_kwargs["circuit"]),
                                           circuit_kwargs["connectome"]) if v)

    kwargs = load_kwargs(parameters, HDFStore(in_config), on_compute_node)

    subset_input = generate_inputs_of(of_computation, in_config, on_compute_node, by_subtarget=True)

    collector = plugins.import_module(parameters["collector"]) if "collector" in parameters else None

    def collect_batch(results):
        """..."""
        if not collector:
            return results

        _, collect = collector
        return collect(results)

    def execute_one(lazy_subtarget):
        """..."""
        def unpack(value):
            if isinstance(value, pd.Series):
                assert len(value) == 1
                LOG.info("Execute one single lazy subtarget %s", value.index[0])
                return value.iloc[0]
            return value

        return execute(*circuit_args_values, **{var: unpack(value) for var, value in lazy_subtarget().items()},
                       ,**kwargs)

    def run_batch(of_input, *, index, in_bowl=None):
        """..."""
        LOG.info("Run %s batch %s of %s inputs args, and circuit %s, \n with kwargs %s ", of_computation,
                 index, len(of_input), circuit_args_values, pformat(kwargs))

        def to_subtarget(s):
            """..."""
            r = execute_one(lazy_subtarget=s)
            LOG.info("store one lazy subtarget %s result \n%s", s, r)
            LOG.info("Result data types %s", r.describe())
            return to_store_one(in_hdf.format(index), result=r)

        if to_store_batch:
            results = of_input.apply(execute_one)
            result = to_store_batch(in_hdf.format(index), results=collect_batch(results))
            #framed = pd.concat([results], axis=0, keys=connsense_index.values, names=connsense_index.names)
            #result = to_store_batch(in_hdf.format(index), results=collect_batch(framed))
        else:
            result = to_store_one(in_hdf.format(index), update=of_input.apply(to_subtarget))

        if in_bowl is not None:
            in_bowl[index] = result
        return result

    n_compute_nodes,  n_total_jobs = prepare_parallelization(of_computation, in_config, using_runtime)

    #batches = load_input_batches(on_compute_node, inputs, n_parallel_tasks=int(n_jobs/n_compute_nodes))
    batches = load_input_batches(on_compute_node)
    n_batches = batches.batch.max() - batches.batch.min() + 1

    if n_compute_nodes == n_total_jobs:
        bowl = {}
        for batch, subtargets in batches.groupby("batch"):
            LOG.info("Run Single Node %s process %s / %s batches", on_compute_node, batch, n_batches)
            bowl[batch] = run_batch(subset_input(subtargets.index), index=batch)
        LOG.info("DONE Single Node connsense run.")
    else:
        manager = Manager()
        bowl = manager.dict()
        processes = []

        for batch, subtargets in batches.groupby("batch"):
            LOG.info("Spawn Compute Node %s process %s / %s batches", on_compute_node, batch, n_batches)
            p = Process(target=run_batch,
                        args=(subset_input(subtargets.index),), kwargs={"index": batch, "in_bowl": bowl})
            p.start()
            processes.append(p)

        LOG.info("LAUNCHED %s processes", n_batches)

        for p in processes:
            p.join()

        LOG.info("Parallel computation %s results %s", of_computation, len(bowl))

    results = {key: value for key, value in bowl.items()}
    LOG.info("Computation %s results %s", of_computation, len(results))

    read_pipeline.write(results, to_json=on_compute_node/"batched_output.json")

    _, output_paths = read_pipeline.check_paths(in_config, step=computation_type)
    _, hdf_group = output_paths["steps"][computation_type]
    of_output_type = parameters["output"]

    collected = collect_batches(of_computation, results, on_compute_node, hdf_group, of_output_type)
    read_pipeline.write(collected, to_json=on_compute_node/"output.json")

    run_in_progress.unlink()
    on_compute_node.joinpath(DONE).touch(exist_ok=False)

    return collected

#+end_src

We may need a circuit to run on. The current ~connsense-TAP~ can be configured with multiple circuits,
which we could support later. However for now we enforce that only one circuit is configured, and that
all the extractors provided work on single circuits as well.

The solution will be to specify the circuit at CLI. and update the pipeline's config that is passed to
the parallelization methods developed here. This will allow configuration of more than one circuit,
but each run will be for only one circuit that must be entered in the configuration.

Another is to require inputs for each computation in it's config:

#+name: parallelize-single-nodel-circuit
#+begin_src python
def input_circuit(labeled, in_config):
    """..."""
    if not labeled:
        return None
    sbtcfg = SubtargetsConfig(in_config)
    circuit = sbtcfg.attribute_depths(circuit=labeled)

    return circuit


def input_connectome(labeled, in_circuit):
    """..."""
    if not labeled:
        return None

    from bluepy import Circuit
    assert isinstance(in_circuit, Circuit)

    if labeled == "local":
        return in_circuit.connectome
    return in_circuit.projection[labeled]


def input_circuit_args(computation, in_config, load_circuit=True, load_connectome=False):
    """..."""
    computation_type, of_quantity = describe(computation)
    parameters = parameterize(computation_type, of_quantity, in_config)

    try:
        computation_inputs = parameters["input"]
    except KeyError as kerr:
        raise ValueError(f"No inputs configured for {computation}") from kerr

    input_circuits = computation_inputs.get("circuit", None)
    if input_circuits:
        assert len(input_circuits) == 1, f"NotImplemented processing more than one circuit"
        c = input_circuits[0]
    else:
        c = None
    circuit = input_circuit(c, in_config) if load_circuit else c

    input_connectomes = computation_inputs.get("connectome", None)
    if input_connectomes:
        assert len(input_connectomes) == 1, f"NotImplemented processing more than one connectome"
        x = input_connectomes[0]
    else:
        x = None
    connectome = input_connectome(x, in_circuit) if load_connectome else x
    return {"circuit": circuit, "connectome": connectome}


def subtarget_circuit_args(computation, in_config, load_circuit=False, load_connectome=False):
    """..."""
    computation_type, of_quantity = describe(computation)
    parameters = parameterize(computation_type, of_quantity, in_config)

    try:
        subtarget = parameters["subtarget"]
    except KeyError as kerr:
        LOG.warning("No subtargets specified for %s", computation)
        return input_circuit_args(computation, in_config, load_circuit, load_connectome)

    c = subtarget.get("circuit", None)
    circuit = input_circuit(c, in_config) if load_circuit else c

    x = subtarget.get("connectome", None)
    return {"circuit": circuit, "connectome": input_connectome(x, circuit) if load_connectome else x}

#+end_src
Inputs to run on a compute node are written in its work-directory during the setup,

#+name: parallelize-single-node-load-inputs
#+begin_src python
def load_input_batches(on_compute_node, inputs=None, n_parallel_tasks=None):
    """..."""
    store_h5, dataset = COMPUTE_NODE_SUBTARGETS

    assert inputs is None or inputs == on_compute_node / store_h5, (
        "inputs dont seem to be what was configured\n"
        f"Expected {inputs} to be {on_compute_node / store_h5} if setup by run_multinode(...)")

    inputs_read = pd.read_hdf(on_compute_node/store_h5, key=dataset)
    if not n_parallel_tasks:
        return inputs_read
    return inputs_read.assign(batch=pd.Series(np.arange(0, len(inputs_read))%n_parallel_tasks).to_numpy(int))


#+end_src

that assumes that the inputs were written on the compute nodes during setup.

If the result of computing a single input entry is huge, the second method to compute batch that
computes the entire batch before writing may be limited by memory. In that case we should prefer the
first method that computes and writes the results of each entry before processing the next.

For a single batch of subtargets, the computation will run either on the entire batch and its results saved together,
or the computation will run on one subtarget at a time with results written before the next one is processed.

#+name: parallelize-single-node-run-batch-get-executable
#+begin_src python
def get_executable(computation_type, parameters):
    """..."""
    executable_type = EXECUTABLE[computation_type.split('-')[0]]

    try:
        executable = parameters[executable_type]
    except KeyError as err:
        raise RuntimeError(f"No {executable_type} defined for {computation_type}") from err

    _, execute = plugins.import_module(executable["source"], executable["method"])

    return execute


def configure_execution(computation, in_config, on_compute_node):
    """..."""
    computation_type, of_quantity = describe(computation)
    parameters = parameterize(computation_type, of_quantity, in_config)
    _, output_paths = read_pipeline.check_paths(in_config, step=computation_type)
    _, at_path = output_paths["steps"][computation_type]

    execute = get_executable(computation_type, parameters)

    if computation_type == "extract-node-populations":
        return (execute, None,  store_node_properties(of_quantity, on_compute_node, at_path))
        #return (execute, store_node_properties(of_quantity, on_compute_node, at_path), None)

    if computation_type == "extract-edge-populations":
        return (execute, store_edge_extraction(of_quantity, on_compute_node, at_path), None)

    return (execute, None, store_matrix_data(of_quantity, parameters, on_compute_node, at_path))


def store_node_properties_batch(of_population, on_compute_node, in_hdf_group):
    """...This will extract node properties for all subtargets as a single datasframe.
    NOT-IDEAL and needs hacks to gather differemt resuts into the same input dataframe.
    REPLACE by single subtarget store using matrices
    """
    def write_batch(connsense_h5, results):
        """..."""
        in_hdf = (on_compute_node/connsense_h5, in_hdf_group)
        LOG.info("Write %s  results %s ", in_hdf, len(results))
        return extract_nodes.write(results, of_population, in_hdf)

    return write_batch


def store_node_properties(of_population, on_compute_node, in_hdf_group):
    """..."""
    LOG.info("Store node properties of population %s on compute node %s in hdf %s, one subtarget at a time",
             of_population, on_compute_node, in_hdf_group)

    def write_hdf(at_path, *, result=None, update=None):
        """..."""
        assert not(result is None and update is None)
        assert result is not None or update is not None

        hdf_population = in_hdf_group+'/'+of_population
        store = matrices.get_store(on_compute_node/at_path, hdf_population, "pandas.DataFrame")

        if result is not None:
            return store.write(result)

        store.append_toc(store.prepare_toc(of_paths=update))
        return (at_path, hdf_population)

    return write_hdf


def store_edge_extraction(of_population, on_compute_node, in_hdf_group):
    """..."""
    def write_batch(connsense_h5, results):
        """..."""
        in_hdf = (on_compute_node/connsense_h5, f"{in_hdf_group}/{of_population}")
        LOG.info("Write %s batch results to %s", len(results), in_hdf)
        return extract_connectivity.write_adj(results, to_output=in_hdf,  append=True, format="table",
                                              return_config=True)

    return write_batch


def store_matrix_data(of_quantity, parameters, on_compute_node, in_hdf_group):
    """..."""
    LOG.info("Store matrix data for %s", parameters)
    of_output = parameters["output"]
    hdf_quantity = f"{in_hdf_group}/{of_quantity}"

    cached_stores = {}

    def write_hdf(at_path, *, result=None, update=None):
        """..."""
        assert at_path
        assert not(result is None and update is None)
        assert result is not None or update is not None

        p = on_compute_node/at_path
        if p not in cached_stores:
            cached_stores[p] = matrices.get_store(p, hdf_quantity, for_matrix_type=of_output)

        if result is not None:
            return cached_stores[p].write(result)

        cached_stores[p].append_toc(cached_stores[p].prepare_toc(of_paths=update))
        return (at_path, hdf_quantity)

    return write_hdf

#+end_src

#+name: parallelize-single-node-collect
#+begin_src python
def collect_batches(of_computation, results, on_compute_node, hdf_group, of_output_type):
    """..."""
    LOG.info("Collect bactched %s results of %s on compute node %s in group %s output type %s",
             of_computation, len(results), on_compute_node, hdf_group, of_output_type)
    computation_type, of_quantity = describe(of_computation)


    #if computation_type == "extract-node-populations":
        #return collect_batched_node_population(of_quantity, results, on_compute_node, hdf_group)

    if computation_type == "extract-edge-populations":
        return collect_batched_edge_population(of_quantity, results, on_compute_node, hdf_group)

    hdf_quantity = hdf_group+"/"+of_quantity
    in_connsense_h5 = on_compute_node / "connsense.h5"
    in_store = matrices.get_store(in_connsense_h5, hdf_quantity, for_matrix_type=of_output_type)

    batched = results.items()
    in_store.collect({batch: matrices.get_store(on_compute_node / batch_connsense_h5, hdf_quantity,
                                                for_matrix_type=of_output_type)
                      for batch, (batch_connsense_h5, group) in batched})
    return (in_connsense_h5, hdf_quantity)
#+end_src

For the extracting node populations we have a special case,

#+name: parallelize-single-node-collect-batched-node-populations
#+begin_src python
def collect_batched_node_population(p, results, on_compute_node, hdf_group):
    """..."""
    from connsense.io.write_results import read as read_batch, write as write_batch

    LOG.info("Collect batched node populations of %s %s results on compute-node %s to %s", p,
             len(results), on_compute_node, hdf_group)

    in_connsense_h5 = on_compute_node / "connsense.h5"

    hdf_node_population = (in_connsense_h5, hdf_group+"/"+p)

    def move(batch, output):
        """..."""
        LOG.info("Write batch %s read from %s", batch, output)
        result = read_batch(output, "extract-node-populations")
        return write_batch(result, to_path=hdf_node_population, append=True, format="table")

    LOG.info("collect batched extraction of nodes at compute node %s", on_compute_node)
    for batch, output in results.items():
        move(batch, output)

    LOG.info("DONE collecting %s", results)
    return hdf_node_population

#+end_src

For the extracting edge populations we have a special case,

#+name: parallelize-single-node-collect-batched-edge-populations
#+begin_src python
def collect_batched_edge_population(p, results, on_compute_node, hdf_group):
    """..."""

    in_connsense_h5 = on_compute_node / "connsense.h5"

    hdf_edge_population = (in_connsense_h5, hdf_group+'/'+p)

    def move(batch, output):
        """.."""
        LOG.info("collect batch %s of adjacencies at %s output %s ", batch, on_compute_node, output)
        adjmats = read_toc_plus_payload(output, for_step="extract-edge-populations")
        return write_toc_plus_payload(adjmats, hdf_edge_population, append=True, format="table",
                                      min_itemsize={"values": 100})

    LOG.info("collect batched extraction of edges at compute node %s", on_compute_node)
    for batch, output in results.items():
        move(batch, output)

    LOG.info("DONE collecting %s", results)
    return hdf_edge_population

#+end_src

We have assumed that the stores invoked above are like the ~MatrixStore~ defined in ~connsense.analyze_connectivity.matrices~.
We do not have working version of a sparse matrices that we use to store adjacency.
Either we can implement such a store, or change the collection methods.
