#+PROPERTY: header-args:jupyter-python :session ~/Library/Jupyter/runtime/active-83-ssh.json
#+PROPERTY: header-args:jupyter :session ~/Library/Jupyter/runtime/active-83-ssh.json

#+STARTUP: overview
#+STARTUP: logdrawer
#+STARTUP: hideblocks

Let us setup an interactive ~Python~ session where we can run the code developed here.
#+begin_src jupyter
print("Welcome to EMACS Jupyter")
#+end_src

#+RESULTS:
: Welcome to EMACS Jupyter

#+title: Topotap

* Setup :noexport:
In our discussion we will develop scientific concepts to measure the circuit, and implement Python functions to compute them. Here we setup a notebook template to test and explore, and the structure of a ~Python~ package for our methods.


#+RESULTS:
: Welcome to EMACS Jupyter

** Introduction
#+name: notebook-init
#+begin_src jupyter-python
from importlib import reload
from collections.abc import Mapping
from collections import OrderedDict
from pprint import pprint, pformat
from pathlib import Path

import numpy as np
import pandas as pd

import matplotlib

reload(matplotlib)
from matplotlib import pylab as plt
import seaborn as sbn
GOLDEN = (1. + np.sqrt(5.))/2.

from IPython.display import display

from bluepy import Synapse, Cell, Circuit

print("We will plot golden aspect ratios: ", GOLDEN)
#+end_src
** Workspaces
We have run ~connsense-CRAP~ for the SSCx dissemination variant /Bio-M/, extracting data that we will use to compute the factology. Here is a list of workspaces we will need to generate factsheets.
#+name: notebook-workspaces-0
#+begin_src jupyter-python
from connsense.pipeline import pipeline
from connsense.develop import parallelization as devprl

from connsense.pipeline.store import store as tap_store
from connsense.develop import topotap as devtap

ROOTSPACE = Path("/")
PROJSPACE = ROOTSPACE / "gpfs/bbp.cscs.ch/project/proj83"
SOODSPACE = PROJSPACE / "home/sood"
CONNSPACE = SOODSPACE / "topological-analysis-subvolumes/test/v2"
#+end_src

#+RESULTS: notebook-workspaces-0

We have another ~connsense-TAP~ project defined in,
#+name: notebook-workspaces
#+begin_src jupyter-python :noweb yes
<<notebook-workspaces-0>>

PORTALSPACE = (SOODSPACE / "portal" / "develop" / "factology-v2" / "analyses/connsense"
               / "redefine-subtargets/create-index/morphology-mtypes")
EXPTLSPACE = PORTALSPACE / "experimental"
#+end_src
#+RESULTS: notebook-workspaces

While test-developing it will be good to have direct access to the ~connsense-TAP-store~ we will use,

We can collect the code above in a ~Pyhton~ template file that can be used to generate notebooks,

** ~connsense~ Modules

#+name: notebook-connsense-tap
#+begin_src jupyter-python
topaz = pipeline.TopologicalAnalysis(CONNSPACE/"pipeline.yaml", CONNSPACE/"runtime.yaml")
tap = tap_store.HDFStore(topaz._config)
circuit = tap.get_circuit("Bio_M")

topotap = devtap.HDFStore(CONNSPACE/"pipeline.yaml")
print("Available analyses: ")
pprint(topotap.analyses)
#+end_src

#+RESULTS: notebook-connsense-tap
:  2022-10-25 14:37:15,281: Configure slurm for create-index
:  2022-10-25 14:37:15,283: No runtime configured for computation type create-index
:  2022-10-25 14:37:15,284: Configure slurm for define-subtargets
:  2022-10-25 14:37:15,285: Configure slurm for extract-node-populations
:  2022-10-25 14:37:15,285: Configure slurm for extract-edge-populations
:  2022-10-25 14:37:15,286: Configure slurm for analyze-connectivity
:  2022-10-25 14:37:15,288: Load circuit Bio_M
: Available analyses:
: {'connectivity': {'model-params-dd2': <connsense.develop.topotap.TapDataset object at 0x7ffec6bc2970>,
:                   'simplex-counts': <connsense.develop.topotap.TapDataset object at 0x7ffed04aea90>}}

** Notebook template
Finally, here is a template that we can use to start test-developing. We will deposit the code in a sub-directory, of the directory holding this file.

#+begin_src jupyter-python :tangle develop_topotap.py :comments no :noweb yes :padline yes
# %% [markdown]
"""# Test Develop a Circuit Factology
"""

# %% [code]
<<notebook-init>>

<<notebook-workspaces>>

<<notebook-connsense-tap>>

<<notebook-reloads>>


#+end_src

#+RESULTS:
#+begin_example
 2022-11-29 14:51:13,922: Configure slurm for create-index
 2022-11-29 14:51:13,923: No runtime configured for computation type create-index
 2022-11-29 14:51:13,923: Configure slurm for define-subtargets
 2022-11-29 14:51:13,924: Configure slurm for extract-node-populations
 2022-11-29 14:51:13,924: Configure slurm for extract-edge-populations
 2022-11-29 14:51:13,925: Configure slurm for analyze-connectivity
 2022-11-29 14:51:13,927: Load circuit Bio_M
We will plot golden aspect ratios:  1.618033988749895
Available analyses:
{'connectivity': {'model-params-dd2': <connsense.develop.topotap.TapDataset object at 0x7ffe0bff6520>,
                  'simplex-counts': <connsense.develop.topotap.TapDataset object at 0x7ffed51e5760>}}
#+end_example

* Introduction

 While the ~Python-environment~ setup above loads the other packages that we will need, we will be most interested in discussing a layer for topological-analyses.

** DevNotes
We will test-develop our implementations, using a notebook,

#+name: notebook-connsense-topotap
#+begin_src jupyter-python :tangle develop_topotap.py
# %% [markdown]
"""Load a connsense-TAP to analyze topology of a circuit
"""
# %% [code]

from connsense.develop import topotap as topotap_store
reload(topotap_store)
topotap = topotap_store.HDFStore(CONNSPACE/"pipeline.yaml")
print("Available analyses: ")
pprint(topotap.analyses)
#+end_src

#+RESULTS: notebook-connsense-topotap
: Available analyses:
: {'connectivity': {'model-params-dd2': <connsense.develop.topotap.TapDataset object at 0x7ffe2e3bf250>,
:                   'simplex-counts': <connsense.develop.topotap.TapDataset object at 0x7ffe2e3bfa30>}}

Let us analyze degrees of the subtargets. To get to the data about circuit edges / adjacency, we start by looking at the ~circuit-subtargets~ that we are analyzed,

#+name: notebook-connsense-subtargets
#+begin_src jupyter-python :tangle develop_topotap.py
# %% [markdown]
"""## Subtargets in connsense-TAP
"""
# %% [code]

topotap.subtargets
#+end_src

#+RESULTS: notebook-connsense-subtargets
:RESULTS:
:  2022-10-31 15:23:12,272: Load dataset ('define-subtargets', 'flatmap-columns'):
: ('Hexaongal prism like columns oriented along cortical layers, from '
:  'white-matter to pia. The data is loaded from an NRRD file that maps each '
:  'circuit voxel to a subtarget ids corresponding to a flatmap column.The '
:  'subtarget ids should be mapped to the subtargets they refer to in a '
:  'dataframe provided as the input `info`.')
#+begin_example
             subtarget  flat_i  flat_j        flat_x  flat_y
subtarget_id
1               R18;C0     -27      27  3.802528e-13  6210.0
2               R19;C0     -28      29  1.991858e+02  6555.0
3               R18;C1     -26      28  3.983717e+02  6210.0
4               R19;C1     -27      30  5.975575e+02  6555.0
5               R16;C0     -24      24  3.380025e-13  5520.0
...                ...     ...     ...           ...     ...
236             R4;C12       6      18  4.780460e+03  1380.0
237             R9;C15       2      29  6.174761e+03  3105.0
238            R15;C13      -9      36  5.378018e+03  5175.0
239             R3;C11       7      16  4.581274e+03  1035.0
240            R15;C15      -7      38  6.174761e+03  5175.0

[240 rows x 5 columns]
#+end_example
:END:

This is the information that was provided in the configured definition of subtargets. In the earlier version of ~connsense-TAP~ this information was part of each ~connsense-dataset~'s index. However it does not need to. We can get the data from ~connsense-TAP~ store for analyses that need it.

For each subtarget, ~connsense-TAP~ saves the ~circuit-cell-gids~ as a list. We do not expect each user to need this list --- ~connsense-TAP~ hides that information. However the ~gids~ are avaialble among the node properties.

#+name: notebook-connsense-nodes
#+begin_src jupyter-python :tangle develop_topotap.py
# %% [markdown]
"""## Nodes in connsense-TAP
"""
# %% [code]

topotap.nodes.dataset
#+end_src

#+RESULTS: notebook-connsense-nodes
:RESULTS:
:  2022-10-31 15:23:27,560: Load dataset ('extract-node-populations', 'default'):
: ('The default population will be that of neurons in the SSCx. To extract the '
:  'neurons we will use a `connsense` method that uses ~bluepy~.')
:  2022-10-31 15:23:27,562: Initialize a DataFrameStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/test/connsense.h5 / nodes/populations/default
#+begin_example
subtarget_id  circuit_id
1             0             <connsense.analyze_connectivity.matrices.BeLaz...
2             0             <connsense.analyze_connectivity.matrices.BeLaz...
3             0             <connsense.analyze_connectivity.matrices.BeLaz...
4             0             <connsense.analyze_connectivity.matrices.BeLaz...
5             0             <connsense.analyze_connectivity.matrices.BeLaz...
                                                  ...
235           0             <connsense.analyze_connectivity.matrices.BeLaz...
236           0             <connsense.analyze_connectivity.matrices.BeLaz...
237           0             <connsense.analyze_connectivity.matrices.BeLaz...
238           0             <connsense.analyze_connectivity.matrices.BeLaz...
239           0             <connsense.analyze_connectivity.matrices.BeLaz...
Length: 239, dtype: object
#+end_example
:END:

The contents for each ~circuit-subtarget~ entry are a mysterious object ~BeLazy~ which is nothing more than an instruction to load the data,

#+name: notebook-connsense-nodes-load-lazy
#+begin_src jupyter-python :tangle develop_topotap.py
# %% [markdown]
"""Contents of nodes
"""
# %% [code]

topotap.nodes.dataset.iloc[0].get_value().info()
#+end_src

#+RESULTS: notebook-connsense-nodes-load-lazy
#+begin_example
 2022-10-31 15:23:35,938: Initialize a DataFrameStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/test/connsense.h5 / nodes/populations/default
<class 'pandas.core.frame.DataFrame'>
Int64Index: 4570 entries, 0 to 4569
Data columns (total 11 columns):
 #   Column         Non-Null Count  Dtype
---  ------         --------------  -----
 0   gid            4570 non-null   int64
 1   region         4570 non-null   category
 2   layer          4570 non-null   int64
 3   x              4570 non-null   float64
 4   y              4570 non-null   float64
 5   z              4570 non-null   float64
 6   synapse_class  4570 non-null   category
 7   mtype          4570 non-null   category
 8   etype          4570 non-null   category
 9   morphology     4570 non-null   category
 10  depth          4570 non-null   float64
dtypes: category(5), float64(4), int64(2)
memory usage: 3.0 MB
#+end_example

Lazy-data is necessary to track all the subtargets in a dataset as each can be big. However we can interact with ~connsense-TAP~ without having to know about laziness of the data,

#+name: notebook-connsense-nodes-subtarget-circuit
#+begin_src jupyter-python :tangle develop_topotap.py
# %% [markdown]
"""Contents of nodes
"""
# %% [code]

topotap.nodes(subtarget="R19;C0", circuit="Bio_M").info()
#+end_src

#+RESULTS: notebook-connsense-nodes-subtarget-circuit
#+begin_example
 2022-10-31 15:23:48,974: Initialize a DataFrameStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/test/connsense.h5 / nodes/populations/default
<class 'pandas.core.frame.DataFrame'>
Int64Index: 1823 entries, 0 to 1822
Data columns (total 11 columns):
 #   Column         Non-Null Count  Dtype
---  ------         --------------  -----
 0   gid            1823 non-null   int64
 1   region         1823 non-null   category
 2   layer          1823 non-null   int64
 3   x              1823 non-null   float64
 4   y              1823 non-null   float64
 5   z              1823 non-null   float64
 6   synapse_class  1823 non-null   category
 7   mtype          1823 non-null   category
 8   etype          1823 non-null   category
 9   morphology     1823 non-null   category
 10  depth          1823 non-null   float64
dtypes: category(5), float64(4), int64(2)
memory usage: 2.9 MB
#+end_example


We don't have to provide the circuit,

#+name: notebook-connsense-nodes-subtarget
#+begin_src jupyter-python :tangle develop_topotap.py
# %% [markdown]
"""Nodes of a subtarget
"""
# %% [code]

topotap.nodes(subtarget="R19;C0").info()
#+end_src

#+RESULTS: notebook-connsense-nodes-subtarget
#+begin_example
 2022-10-31 15:24:03,195: Initialize a DataFrameStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/test/connsense.h5 / nodes/populations/default
<class 'pandas.core.frame.DataFrame'>
Int64Index: 1823 entries, 0 to 1822
Data columns (total 11 columns):
 #   Column         Non-Null Count  Dtype
---  ------         --------------  -----
 0   gid            1823 non-null   int64
 1   region         1823 non-null   category
 2   layer          1823 non-null   int64
 3   x              1823 non-null   float64
 4   y              1823 non-null   float64
 5   z              1823 non-null   float64
 6   synapse_class  1823 non-null   category
 7   mtype          1823 non-null   category
 8   etype          1823 non-null   category
 9   morphology     1823 non-null   category
 10  depth          1823 non-null   float64
dtypes: category(5), float64(4), int64(2)
memory usage: 2.9 MB
#+end_example

We can access the adjacencies,

#+name: notebook-connsense-adjacency
#+begin_src jupyter-python :tangle develop_topotap.py
# %% [markdown]
"""## Adjacency datasets
"""
# %% [code]
topotap.adjacency["local"].dataset
#+end_src

#+RESULTS: notebook-connsense-adjacency
:RESULTS:
:  2022-10-31 15:24:30,624: Load dataset ('extract-edge-populations', 'local'):
: None
:  2022-10-31 15:24:30,626: Load dataset ('extract-edge-populations', 'long-range'):
: 'Add connections from two connectomes in section input/connectome'
#+begin_example
subtarget_id  circuit_id  connectome_id
1             0           0                <connsense.io.write_results.LazyMatrix object ...
2             0           0                <connsense.io.write_results.LazyMatrix object ...
3             0           0                <connsense.io.write_results.LazyMatrix object ...
4             0           0                <connsense.io.write_results.LazyMatrix object ...
5             0           0                <connsense.io.write_results.LazyMatrix object ...
                                                                 ...
235           0           0                <connsense.io.write_results.LazyMatrix object ...
236           0           0                <connsense.io.write_results.LazyMatrix object ...
237           0           0                <connsense.io.write_results.LazyMatrix object ...
238           0           0                <connsense.io.write_results.LazyMatrix object ...
239           0           0                <connsense.io.write_results.LazyMatrix object ...
Length: 239, dtype: object
#+end_example
:END:

That behaves similarly to ~topotap.nodes~ with an additional level for connectome. We have only one connectome that allows us to get adjacencies,

#+name: notebook-connsense-adjacency-load
#+begin_src jupyter-python :tangle develop_topotap.py
# %% [markdown]
""" Adjacency of a subtarget
"""
# %% [code]
topotap.adjacency["local"].dataset.iloc[0].get_value()
#+end_src

#+RESULTS: notebook-connsense-adjacency-load
: <4570x4570 sparse matrix of type '<class 'numpy.int64'>'
: 	with 431358 stored elements in Compressed Sparse Row format>


#+begin_src jupyter-python :tangle develop_topotap.py
topotap.adjacency["local"](subtarget="R19;C0")
#+end_src

#+RESULTS:
: <1823x1823 sparse matrix of type '<class 'numpy.int64'>'
: 	with 88675 stored elements in Compressed Sparse Row format>

And we have simplex-counts
#+name: notebook-connsense-analyses
#+begin_src jupyter-python :tangle develop_topotap.py
# %% [markdown]
"""## Analyses
"""
# %% [code]
pprint(topotap.analyses)
#+end_src

#+RESULTS: notebook-connsense-analyses
: {'connectivity': {'model-params-dd2': <connsense.develop.topotap.TapDataset object at 0x7ffe2e3bf250>,
:                   'simplex-counts': <connsense.develop.topotap.TapDataset object at 0x7ffe2e3bfa30>}}

that we can access using the same indexing scheme,

#+name: notebook-connsense-analyses-load
#+begin_src jupyter-python :tangle develop_topotap.py
# %% [markdown]
"""Simplex counts
"""
# %% [code]
simplex_counts = topotap.analyses["connectivity"]["simplex-counts"]
simplex_counts.dataset
#+end_src

#+RESULTS: notebook-connsense-analyses-load
:RESULTS:
:  2022-10-31 15:25:06,047: Pour analyses for analyze-connectivity
:  2022-10-31 15:25:06,048: Initialize a SeriesStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/test/connsense.h5 / analyses/connectivity/simplex-counts
#+begin_example
subtarget_id  circuit_id  connectome_id
1             0           0                <connsense.analyze_connectivity.matrices.BeLaz...
2             0           0                <connsense.analyze_connectivity.matrices.BeLaz...
3             0           0                <connsense.analyze_connectivity.matrices.BeLaz...
4             0           0                <connsense.analyze_connectivity.matrices.BeLaz...
5             0           0                <connsense.analyze_connectivity.matrices.BeLaz...
                                                                 ...
235           0           0                <connsense.analyze_connectivity.matrices.BeLaz...
236           0           0                <connsense.analyze_connectivity.matrices.BeLaz...
237           0           0                <connsense.analyze_connectivity.matrices.BeLaz...
238           0           0                <connsense.analyze_connectivity.matrices.BeLaz...
239           0           0                <connsense.analyze_connectivity.matrices.BeLaz...
Length: 239, dtype: object
#+end_example
:END:

That also responds to calls,

#+name: notebook-connsense-simplex-counts-load
#+begin_src jupyter-python :tangle develop_topotap.py
# %% [markdown]
"""Simplex counts
"""
# %% [code]
simplex_counts = topotap.analyses["connectivity"]["simplex-counts"]
simplex_counts("R19;C0")
#+end_src

#+RESULTS: notebook-connsense-simplex-counts-load
:RESULTS:
:  2022-10-31 15:25:10,376: Pour analyses for analyze-connectivity
:  2022-10-31 15:25:10,377: Initialize a SeriesStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/test/connsense.h5 / analyses/connectivity/simplex-counts
: dim
: 0      1823
: 1     88675
: 2    276930
: 3     85837
: 4      3495
: 5        21
: Name: simplex_count, dtype: int64
:END:

#+RESULTS:
:RESULTS:
:  2022-10-11 14:26:40,429: Pour analyses for analyze-connectivity
:  2022-10-11 14:26:40,431: Initialize a SeriesStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/connsense.h5 / analyses/connectivity/simplex-counts
: dim
: 0      1823
: 1     88675
: 2    276930
: 3     85837
: 4      3495
: 5        21
: Name: simplex_count, dtype: int64
:END:

* HDFStore
The long-range connectivity in the SSCx circuit is based on a topographical mapping connections between subregions.
The mapping projects each voxel in the circuit atlas to a /pixel/ in the circuit's /flatmap/. This ~voxel-->pixel~ map, from the circuit's physical space to it's ~flatmap~ space, is used to compute neighborhoods of /intra-SSCx/ white-matter (WM) projections. WM projections are expected to enter the cortex from under layer 6 and proceed upwards along cortical layers. Thalamo-cortical (TC) projections follow similar trajectories. We want to analyze local connectivity in such cortical columns.

We want an interface to a ~connsense-TAP~ instance developed for topological network analyses of a brain circuit. Here we implement o replacement of ~connsense.pipeline.store.store.HDFStore~ adding methods for simpler interaction with the pipeline's data.
** Imports
#+name: tap-imports
#+begin_src python
"""Interface to the HD5-store where the pipeline stores it's data.
"""
from collections.abc import Iterable, Mapping
from collections import OrderedDict, defaultdict
from copy import deepcopy
from pprint import pformat
from lazy import lazy
from pathlib import Path
import h5py

import numpy as np
import pandas as pd

from bluepy import Direction, Synapse, Cell

from connsense import plugins
from connsense.define_subtargets.config import SubtargetsConfig
from connsense import analyze_connectivity as anzconn
from connsense.analyze_connectivity import matrices
from connsense.io import read_config
from connsense.io.write_results import (read as read_dataset,
                                        read_subtargets,
                                        read_node_properties,
                                        read_toc_plus_payload)
from connsense.io import logging
from connsense.pipeline import ConfigurationError, NotConfiguredError, COMPKEYS
from .import parallelization as prl

LOG = logging.get_logger(__name__)
#+end_src

Paths are specified in ~connsense-TAP~ condiguration, using which we can locate the H5 file with the data that results from running ~connsense-TAP~. The configuration provides paths to the H5 file, and the keys in the data-store for each of the computations / steps in the configuration. An HDFStore interface will need these paths,

** Loaders
#+name: tap-locate
#+begin_src python
def locate_store(config, in_connsense_h5=None):
    """..."""
    if not in_connsense_h5:
        return Path(config["paths"]["input"]["store"])
    return Path(in_connsense_h5)


def group_steps(config):
    """..."""
    inputs = config["paths"]["input"]["steps"]
    return {step: group for step, (_, group) in inputs.items()}

#+end_src

~connsense-TAP~ store data with integer IDs in the index, while saving the names for the entries in H5. The names for IDs used are,
#+name: tap-connsense-index
#+begin_src python
SUBTARGET_ID = "subtarget_id"
CIRCUIT_ID = "circuit_id"
CONNECTOME_ID = "connectome_id"
MTYPE_ID = "mtype_id"
MORPHOLOGY_ID = "morphology_id"

from connsense.pipeline import COMPKEYS, PARAMKEY, ConfigurationError, NotConfiguredError
#+end_src

Each individual configured of computation is entered in a list under a key that depends on it's computation type. Here is a list of these parameter keys for each computation type that ~connsense-TAP~ knows about,
#+begin_src python
PARAMKEY = {"define-subtargets": "definitions",
            "extract-voxels": "annotations",
            "extract-node-types": "modeltypes",
            "extract-edge-types": "models",
            "create-index": "variables",
            "extract-node-populations": "populations",
            "extract-edge-populations": "populations",
            "sample-edge-populations": "analyses",
            "randomize-connectivity": "algorithms",
            "configure-inputs": "analyses",
            "analyze-geometry": "analyses",
            "analyze-node-types": "analyses",
            "analyze-composition": "analyses",
            "analyze-connectivity": "analyses",
            "analyze-physiology": "analyses"}
#+end_src

We can instantiate an HDFStore interface instance with a path to the ~pipeline~ config, or the ~config~ itself. The ~config~ should contain a path to the H5 file that contains ~connsense-TAP~ data, or we can pass one as a second argument,
** HDFStore

#+name: tap-connsense-hdfstore-init
#+begin_src python
class HDFStore:
    """An interface to the H5 data extracted by connsense-TAP.
    """
    def __init__(self, config, in_connsense_h5=None):
        """Initialize an instance of connsense-TAP HDFStore.

        config: Path to a YAML / JSON file that configures the pipeline,
        or a Mapping resulting from reading such a config file.

        in_consense_h5: Path to the connsense-TAP H5 store if different

        from the one configured This can be used for testing the data produced in
        individual compute-nodes during a pipeline run.
        """
        self._config = read_config.read(config) if not isinstance(config, Mapping) else config
        self._root = locate_store(self._config, in_connsense_h5)
        self._groups = group_steps(self._config)
        self._circuits = {}

        self._external = locate_store(self._config, "external.h5")
#+end_src

*** Parameters

Once we have an object to interface with a ~connsense-TAP~, we will want to load datasets to further analyze them. Information about the configured computations are in the section ~parameters~,

#+name: tap-parameters-0
#+begin_src python
@lazy
def parameters(tap):
    """Section `parameters` of the config, loaded without `create-index`.
    """
    return {param: config for param, config in tap._config["parameters"].items() if param != "create-index"}

#+end_src

Each parameters entry is for a ~computation-type~ that may have multiple quantities under it. Each ~(computation-type, of_quantity)~ is a dataset that ~connsense-TAP~ can provide usWe can ask ~connsense-TAP~ to describe these computations. The quantities for a ~parameters~ entry are provided under a key,

#+name: tap-paramkey
#+begin_src python
def get_paramkey(tap, computation_type):
    """..."""
    return PARAMKEY[computation_type]

#+end_src

Here we have assumed that the computations are valid, /i.e/ they have a ~paramkey~ entry known to ~connsense-TAP~. We should check the configured ~computation-types~ against ~connsense-TAP~ when ~HDFStore~ is initialized (TODO).

Some analyses may have components, in which case a simple lookup of parameters by ~computation-type~ and ~quantity~ keys will not work. We can provide a method to handle components,

#+name: tap-parameters
#+begin_src python :noweb yes
<<tap-parameters-0>>

def read_parameters(tap, computation_type, quantity, slicing=None):
    """..."""
    from copy import deepcopy
    from connsense.analyze_connectivity import matrices

    pkey = tap.get_paramkey(computation_type)
    if not slicing or slicing == "full":
        if '/' not in quantity:
            configured = deepcopy(tap.parameters[computation_type][pkey][quantity])
        else:
            group, quantity = quantity.split('/')
            configured = deepcopy(tap.parameters[computation_type][pkey][group][quantity])

        if slicing == "full":
            configured.pop("slicing")

        return configured

    params = tap.read_parameters(computation_type, quantity)

    try:
        cfg_slicings = params.pop("slicing")
    except KeyError:
        raise ConfigurationError(f"No slicings were configured: \n{pformat(params)}")

    try:
        slicing_params = cfg_slicings[slicing]
    except KeyError:
        raise ConfigurationError(f"slicing {slicing} not among configure:\n{pformat(cfg_slicings)}")

    if slicing_params.get("compute_mode", ("EXECUTE")) in ("execute", "EXECUTE"):
        params["output"] = matrices.type_series_store(params["output"])

    return params
#+end_src

*** Descriptions
We want *rich* descriptions from ~connsense-TAP~ about the ~config~ as well as the extracted data.
#+name: tap-describe
#+begin_src python
def describe(tap, computation_type=None, of_quantity=None):
    """...Describe the dataset associated with a `(computation_type, of_quantity)`.

    computation_type: should be an entry in the configuration section parameters,
    ~                 if not provided, all computation-types
    of_quantity: should be an entry under argued `computation_type`
    ~            if not provided, all quantities under `computation_type`
    """
    if not computation_type:
        assert not of_quantity, "because a quantity without a computation-type does not make sense."
        return {c: tap.describe(computation_type=c) for c in tap.parameters}

    try:
        config = tap.parameters[computation_type]
    except KeyError as kerr:
        LOG.error("computation-type %s not configured! Update the config, or choose from \n%s",
                  computation_type, pformat(tap.parameters.keys()))
        raise NotConfiguredError(computation_type) from kerr

    paramkey = tap.get_paramkey(computation_type)
    try:
        config = config[paramkey]
    except KeyError as kerr:
        LOG.error("Missing %s entries in %s config.", paramkey, computation_type)
        raise ConfigurationError(f"{paramkey} entries for {computation_type}")

    def describe_quantity(q):
        if '/' not in q:
            config_q = {"description": config[q].get("description", "NotAvailable"),
                        "dataset": (computation_type, q)}
            for k, v in config[q].items():
                if k != "description":
                    config_q[k] = v
            return config_q

        try:
            g, qq = q.split('/')
        except ValueError:
            try:
                g, qq, qqq = q.split('/')
            except Exception as err:
                LOG.warning("Unable to parse computation %s, %s", computation_type, quantity)
                raise err
            #qq = '/'.join([qq, qqq])


        config_g = {"description": config[g].get("description", "NotAvailable")}
        config_g[q] = {"description": config[g].get("description", "NotAvailable"),
                       "dataset": (computation_type, f"{g}/{qq}")}
        for k, v in config[g][qq].items():
            if k != "description":
                config_g[q][k] = v
        return config_g


    if not of_quantity:
        return [describe_quantity(q) for q in config]

    return describe_quantity(q=of_quantity)


#+end_src

*** Circuits
We can use a method to load a circuit by their label,
#+name: tap-connsense-hdfstore-circuits
#+header:  :comments org :noweb yes :padline no :exports code :tangle no
#+begin_src jupyter-python
def get_circuit(self, labeled):
    """..."""
    if labeled not in self._circuits:
        sbtcfg = SubtargetsConfig(self._config)
        circuit = sbtcfg.input_circuit[labeled]
        circuit.variant = labeled
        self._circuits[labeled] = circuit
    return self._circuits[labeled]

@lazy
def circuits(self):
    return self.index_variable("circuit")

@lazy
def connectomes(self):
    return self.index_variable("connectome")
#+end_src

*** TAP datasets

To load the data from ~connsense-TAP-HDFStore~ we will need to infer path to the ~dataset~ from the method's arguments. Data formats used by ~connsense-TAP~ may differ between ~computation-types~, and we keep each ~computation-type~ in it's own ~hdf-group~. If we think of each ~computation-type~ as a ~phenomenon~, we may need to measure several quantities to study it. Each ~connsense-analysis~ is then that of a ~computation-type/method~, or ~phenomenon/quantity~. Essentially there are two levels of grouping in ~connsense-TAP-HDFStore~ that we will refer to as ~computation-type/quantity~ in our code.

We can ick up path to ~computation-type~' ~HDFStore~ from the project's ~pipeline.yaml~,
#+name: tap-pour-dataset-0
#+begin_src python
def get_path(tap, computation_type):
    """..."""
    return (tap._root, tap._groups[computation_type])
#+end_src

As we have developed our ~connsense~, we have learnt that a simplistic hierarchy such as the two level ~phenomenon/quantity~ is not sufficient to structure the results of our study. Consider our configuration of ~extract-node-types~
#+begin_src yaml
  extract-node-types:
    description: >-
      Extract node-type data
    cell-models: #TODO make this synonymous with modeltypes in TAP-interface
      biophysical:
        description: >-
          The biophysical nodes...
        mtype:
          input:
            circuit: "Bio_M"
          extractor:
            source: connsense.extract_node_types.bluepy
            method: extract_mtypes
          output: "pandas.Series"
        morphology:
          input:
            circuit: "Bio_M"
          extractor:
            source: connsense.extract_node_types.bluepy
            method: extract_morphologies_by_mtype
          output: "pandas.DataFrame"
          collector:
            source: connsense.extract_node_types.bluepy
            method: collect_modeltype
        etype:
          input:
            circuit: "Bio_M"
          extractor:
            source: connsense.extract_node_types.bluepy
            method: extract_etypes
          output: "pandas.Series"
        electrophysiology:
          input:
            circuit: "Bio_M"
          extractor:
            source: connsense.extract_node_types.bluepy
            method: extract_electrophysiologies
          output: "pandas.DataFrame"
          collector:
            source: connsense.extract_node_types.bluepy
            method: collect_modeltype

#+end_src

In a computation of ~extract-node-types~ we enter the /types/ of ~cell-models~ we will analyze, /i.e./ these are a part of the subjects in our study. The cells we use to build circuits at BBP cannot be packaged as a single piece of code that we can call a /cell-model and give a name to. The cell models that we have are themselves built from individual /sub-components/. We provide an additional level of hierarchy in the ~connsense-TAP~ configuration to allow for a ~composite~ ~phneomonon/quantity~.

We extend the ~connsense-TAP~'s ~group-hierarchy~ beyond the basic ~computation-type/method~ in a different way when an ~analysis~ needs computation of the main input's slices. We have developed ~slicing~ in ~parallelization.org~. We can configure as many ~slicings~ for an analysis. The results are stored at ~connsense-h5/computation-type/quantity/slicing~ for a given ~slicing~, while the full inputs are in ~connsense-h5/computation-type/quantity/full~. The /full/ input includes controls if configured.

To interface with the ~slicing~ datasets of an analysis we will use a ~kwarg~,
#+name: tap-pour-dataset-1
#+begin_src python :noweb yes
<<tap-pour-dataset-0>>

def pour_dataset(tap, computation_type, of_quantity, slicing=None,
                 ,*, subset=None):
    """..."""
    def slice_subset(dataset):
        """..."""
        return dataset.loc[subset] if subset is not None else dataset

    if (circuit_dataset:=tap.check_circuit_dataset(computation_type, of_quantity)):
        LOG.info("Circuit dataset %s: %s",
                 (computation_type, of_quantity), circuit_dataset[0])
        assert slicing is None,\
            "We do not apply slicing to circuit datasets, only analyses"
        return slice_subset(circuit_dataset[0])

    cnsh5, hdf_group = tap.get_path(computation_type)
    dataset = '/'.join([hdf_group, of_quantity] if not slicing
                       else [hdf_group, of_quantity, slicing])

    with h5py.File(tap._root, 'r') as hdf:
        if "data" in hdf[dataset]:
            dataset = '/'.join([dataset, "data"])

    if computation_type == "extract-node-populations":
        assert slicing is None,\
            "We do not apply slicing to extracted node populations, only analyses"
        return slice_subset(matrices
                            .get_store(cnsh5, dataset, pd.DataFrame, in_mode='r').toc)

    if computation_type == "extract-edge-populations":
        assert slicing is None,\
            "We do not apply slicing to extracted edge populations, only analyses"
        return slice_subset(read_toc_plus_payload((cnsh5, dataset),
                                                  "extract-edge-populations"))

    if computation_type.startswith("analyze-"):
        return slice_subset(tap.pour_analyses(computation_type, of_quantity, slicing))

    return slice_subset(read_dataset((cnsh5, dataset), computation_type))


def pour(tap, dataset):
    """For convenience, allow queries with tuples (computation_type, of_quantity).
    """
    return tap.pour_dataset(*dataset)

#+end_src

We can pour whole circuit datasets from ~tap~,
#+name: tap-pour-dataset
#+begin_src python :noweb yes
<<tap-pour-dataset-1>>

def assign_subtargets(tap, to_report_ids=False):
    """..."""
    def assign_subtargets_for_circuit(_id):
        def series(of_gids):
            return pd.Series(of_gids, name="gid",
                             index=pd.RangeIndex(0, len(of_gids), 1, name="node_id"))

        circuit_subtarget_gids = tap.subtarget_gids.xs(_id, level="circuit_id")
        assignment = (pd.concat([series(gids) for gids in circuit_subtarget_gids],
                                keys=circuit_subtarget_gids.index)
                      .reset_index().set_index("gid"))
        if to_report_ids: return assignment

        subtargets = tap.subtargets.subtarget.loc[assignment.subtarget_id.values].values
        return pd.Series(subtargets, name="subtarget", index=assignment.index)

    circuit_ids = tap.subtarget_gids.index.get_level_values("circuit_id").unique()

    return pd.Series([assign_subtargets_for_circuit(c) for c in circuit_ids],
                     name="subtargets", index=circuit_ids)


def check_circuit_dataset(tap, computation_type, of_quantity):
    """..."""
    if not computation_type.startswith("circuit-"):
        return False

    if computation_type == "circuit-node-populations":
        from connsense.develop import parallelization
        circuits = (tap.create_index("circuit")
                    .apply(lambda c: parallelization.input_circuit(c, tap._config)))
        try:
            extraction = tap.describe("extract-node-populations", of_quantity)
        except KeyError as kerr:
            raise KeyError("No extraction for node population %s was defined."
                           "Circuit datasets may be defined for corresponding "
                           "extractions"%(of_quantity,))

        of_cells = extraction.get("kwargs", {}).get("properties", None)
        def get_cells(circuit):
            cells = circuit.cells.get(properties=of_cells)
            cells.index.rename(Cell.ID, inplace=True)
            return cells
        circuit_nodes = circuits.apply(get_cells).rename("cell_properties")
        subtargets = tap.assign_subtargets()
        dataset = (pd.merge(circuit_nodes, subtargets,
                            left_index=True, right_index=True)
                   .apply(lambda c: c.cell_properties.join(c.subtargets), axis=1))
        return (dataset,)

    raise NotImplementedError(f"Circuit data {computation_type}")
#+end_src

#+RESULTS: tap-pour-dataset
: None


*** TAP analyses

Analyses ~computation-type~ should be of the form ~analyze-phenomenon~. This allows us to have a method to ~pour-analyses~,
#+name: tap-pour-analyses
#+begin_src python

def decompose(self, computation_type, of_quantity):
    """Some computations may have components.
    We need to strip computation keys from the config, and return the resulting dict.
    """
    parameters = prl.parameterize(computation_type, of_quantity, self._config)
    return {var: val for var, val in parameters.items() if var not in COMPKEYS}


def pour_analyses(tap, computation_type, quantity, slicing=None):
    """Pour the results of running an analysis computation.
    """
    LOG.info("Pour analyses for %s quantity %s", computation_type, quantity)
    cnsh5, hdf_group = tap.get_path(computation_type)

#    if quantity == "psp/traces":
#        return pd.read_hdf(cnsh5, '/'.join([hdf_group, quantitye))


    def pour_component(c, parameters):
        """..."""
        LOG.info("Pour %s %s component %s: \n%s\n from store %s",
                 computation_type, quantity, c, pformat(parameters),
                 (cnsh5, '/'.join([hdf_group, c])))

        dataset = '/'.join([hdf_group, quantity, c] if not slicing
                           else [hdf_group, quantity, c, slicing])
        store = matrices.get_store(cnsh5, dataset, parameters["output"], in_mode='r')
        return store.toc if store else None

    components = tap.decompose(computation_type, quantity)
    if not components:
        dataset = '/'.join([hdf_group, quantity] if not slicing
                           else [hdf_group, quantity, slicing])
        parameters = tap.read_parameters(computation_type, quantity, slicing)
        store = matrices.get_store(cnsh5, dataset, parameters["output"], in_mode='r')
        return store.toc if store else None

    return {'/'.join([quantity, c]): pour_component(c, parameters)
            for c, parameters in components.items()}
#+end_src

*** TAP Indices

With methods to pour datasets from a ~connsense-TAP~, we can provide some convenient interfaces to get subtargets, nodes, adjacencies, analyses. In its H5 data, ~connsense-TAP~ will index the computations using the configuration entry for ~parameters/create-index~,

#+name: tap-create-index-create
#+begin_src python
def create_index(tap, variable):
    """..."""
    described = tap._config["parameters"]["create-index"]["variables"][variable]

    if isinstance(described, pd.Series):
        values = described.values
    elif isinstance(described, Mapping):
        try:
            dataset = described["dataset"]
        except KeyError as kerr:
            LOG.error("Cannot create an index for %s of no dataset in config.", variable)
            raise ConfigurationError("No create-index %s dataset", variable)
        return tap.pour(dataset)
    elif isinstance(described, Iterable):
        values = list(described)
    else:
        raise ConfigurationError(f"create-index %s using config \n%s", pformat(described))

    return pd.Series(values, name=variable, index=pd.RangeIndex(0, len(values), 1, name=f"{variable}_id"))


#+end_src

We might want want to index ids of a variable,
#+name: tap-create-index
#+begin_src python :noweb yes
<<tap-create-index-create>>

def index_variable(tap, name, value=None):
    """..."""
    import numpy as np

    index = tap.create_index(variable=name)

    if value is not None and not isinstance(value, (list, np.ndarray)):
        idx = index.index.values[index == value]
        return idx[0] if len(idx) == 1 else idx

    reverse = pd.Series(index.index.values, name=index.name, index=pd.Index(index.values, name=index.name))
    return reverse.reindex(value) if value is not None else reverse

#+end_src

*** TAP Subtargets

#+name: tap-subtargets
#+begin_src python
def pour_subtargets(tap, dataset):
    """..."""
    return tap.pour(("define-subtargets", dataset))
@lazy
def subtargets(tap):
    """Subtargets in connsense-TAP."""
    definitions = tap.describe("define-subtargets")

    if len(definitions) == 0:
        LOG.warning("No subtargets configured!")
        return None

    def of_(definition):
        """..."""
        LOG.info("Load dataset %s: \n%s", definition["dataset"], pformat(definition["description"]))
        _, group = definition["dataset"]
        subtargets = tap.pour_subtargets(f"{group}/name")
        try:
            info = tap.pour_subtargets(f"{group}/info")
        except KeyError:
            return subtargets
        return pd.concat([subtargets, info], axis=1)

    if len(definitions) == 1:
        return of_(definitions[0])
    return {definition["dataset"][1]: of_(definition) for definition in definitions}


@lazy
def subtarget_gids(tap):
    """..."""
    definitions = tap.describe("define-subtargets")

    if len(definitions) == 0:
        LOG.warning("No subtargets configured!")
        return None

    def of_(definition):
        """..."""
        LOG.info("Load dataset %s: \n%s", definition["dataset"], pformat(definition["description"]))
        _, group = definition["dataset"]
        return tap.pour_subtargets(f"{group}/data")

    if len(definitions) == 1:
        return of_(definitions[0])
    return {definition["dataset"][1]: of_(definition) for definition in definitions}
#+end_src

*** TAP Nodes
#+name: tap-nodes
#+begin_src python
@lazy
def nodes(tap):
    """Nodes in connsense-TAP
    """
    populations = tap.describe("extract-node-populations")

    if len(populations) == 0:
        LOG.warning("No populations configured!")
        return None

    def of_(population):
        """..."""
        LOG.info("Load dataset %s: \n%s", population["dataset"], pformat(population["description"]))
        return TapDataset(tap, population["dataset"])

    if len(populations) == 1:
        return of_(populations[0])
    return {population["dataset"][1]: of_(population) for population in populations}


#+end_src

*** TAP Dataset
The ~HDFStore~ will rely on ~TapDataset~ to provide clean interfaces to the data computed by ~connsense-TAP~. With the large datasets in store we should be able to look around a ~TapDataset~'s contents without loading any data. This will require indices,
#+name: tap-dataset-0
#+begin_src python

class TapDataset:
    """A dataset computed by connsense-TAP.
    """
    def __init__(self, tap, dataset, belazy=True, transform=None):
        """..."""
        self._tap = tap
        self._dataset = dataset
        self._phenomenon, self._quantity = dataset
        self._belazy = belazy
        self._transform = transform

    def load(self):
        """.."""
        return TapDataset(self._tap, self._dataset, belazy=False, transform=self._transform)

    def index_ids(self, variable):
        """..."""
        try:
            series = self._tap.create_index(variable)
        except KeyError:
            LOG.warn("No values for %s in TAP at %s", variable, self._tap._root)
            return None

        return pd.Series(series.index.values, name=f"{series.name}_id",
                         index=pd.Index(series.values, name=series.name))

    @lazy
    def parameters(self):
        """Configure parameters for this TapDataset."""
        description = self._tap.describe(self._phenomenon, self._quantity)
        return description[self._quantity] if '/' in self._quantity else description

    @lazy
    def id_subtargets(self):
        """..."""
        return self.index_ids("subtarget")
    @lazy
    def id_circuits(self):
        """..."""
        return self.index_ids("circuit")
    @lazy
    def id_connectomes(self):
        """..."""
        return self.index_ids("connectome")

    @lazy
    def id_mtypes(self):
        """..."""
        return self.index_ids("mtype")

    def index(self, subtarget, circuit=None, connectome=None):
        """Get `connsense-TAP`index for the arguments.
        """
        subtarget_id = self.id_subtargets.loc[subtarget]

        if not circuit:
            assert not connectome, f"connectome must be of a circuit"
            return (subtarget_id,)

        circuit_id = self.id_circuits.loc[circuit]

        if not connectome:
            return (subtarget_id, circuit_id)

        connectome_id = self.id_connectomes.loc[connectome]
        return (subtarget_id, circuit_id, connectome_id)

    @lazy
    def toc(self):
        """TAP Table of Contents"""
        primary_ids = ["subtarget_id", "circuit_id", "connectome_id"]
        secondary_ids = [var_id for var_id in self.dataset.index.names if var_id not in primary_ids]


#+end_src

A ~TapDataset~ can use ~self._tap: HDFStore~ to pour datasets, taking care of each ~slicing~ as separate from ~full~,
#+name: tap-dataset-1
#+begin_src python :noweb yes
<<tap-dataset-0>>

    @lazy
    def dataset(self):
        """..."""
        from connsense.develop.parallelization import DataCall

        def call(dataitem):
            """..."""
            return DataCall(dataitem, self._transform)

        def load_component(c):
            """..."""
            raise NotImplementedError("INPROGRESS")

        def load_slicing(s):
            """..."""
            slicing_cfg = self.parameters["slicing"][s]
            try:
                lazydset = self._tap.pour_dataset(self._phenomenon, self._quantity, slicing=s)
            except KeyError as kerr:
                LOG.warning("No computation results of %s %s for input slicing %s: \n%s",
                            self._phenomenon, self._quantity, s, kerr)
                return None

            if self._belazy:
                return lazydset

            dataset = lazydset.apply(lambda l: l.get_value())
            slices = prl.parse_slices(slicing_cfg)

            if slicing_cfg["compute_mode"] in ("execute", "EXECUTE"):
                return pd.concat(dataset.values, keys=dataset.index)

            slicing_args = list(prl.flatten_slicing(next(slices)).keys())
            sliced = pd.concat([g.droplevel(slicing_args) for _,g in dataset.groupby(slicing_args)], axis=1,
                                keys=[g for g,_ in dataset.groupby(slicing_args)], names=slicing_args)
            return (sliced if isinstance(dataset, pd.Series) else
                    sliced.reorder_levels(dataset.columns.names + slicing_args, axis=1))

        if not "slicing" in self.parameters:
            lazydset = self._tap.pour(self._dataset).sort_index()
            if not isinstance(lazydset, pd.Series):
                raise TypeError(f"Unexpected type of TAP-dataset {type(lazydset)}.\n"
                                "If you defined this TapDataset for measurement of a phenomenon/quantity,\n"
                                "we are still figuring out how to handle that. We may remove such a "
                                "possibility and define a TapDatasetGroup.\n "
                                "Thus we will keep TapDataset to contain only a single type of data"
                                "i.e. data that originates from a single TAP-computation")

            lazycalls = lazydset.apply(call)
            return lazycalls if self._belazy else lazycalls.apply(lambda l: l())

        slicings = self.parameters["slicing"]
        dataset = {s: load_slicing(s) for s in slicings if s not in ("description", "do-full")}
        try:
            lazyfull = self._tap.pour_dataset(self._phenomenon, self._quantity, slicing="full")
        except KeyError as kerr:
            LOG.warning("No computation results for the full input of %s %s: \n%s", self._phenomenon, self._quantity, kerr)
            dataset["full"] = None
        else:
            dataset["full"] = (lazyfull if self._belazy else lazyfull.apply(lambda l: l.get_value()))
        return dataset

    def summarize(self, method):
        """..."""
        if callable(method):
            return TapDataset(self._tap, self._dataset, belazy=self._belazy, transform=method)

        if isinstance(method, (list, str)):
            return TapDataset(self._tap, self._dataset, belazy=self._belazy,
                              transform=lambda measurement: measurement.agg(method))

        raise NotImplementedError(f"Method to handle of type {type(method)}")

#+end_src

While its ~dataset~ provides a ~dict~ (/i.e./ ~[]~) interface to ~connsense-TAP-datasets~, the loaded data is indexed by ~ids~ and not ~subtarget~ names. We can provide a ~call~ (/i.e./ ~()~) interface to ~TapDataset~ easier to use,
#+name: tap-dataset-2
#+begin_src python :noweb yes
<<tap-dataset-1>>

    def check_reindexing(self):
        """Figure out other variables (than subtarget, circuit, connectome) in the index and name them."""
        from connsense.develop import parallelization as prl

        datasets = {variable: config["dataset"] for variable, config in self.parameters["input"].items()
                    if isinstance(config, Mapping) and "dataset" in config}
        try:
            return datasets["reindex"]
        except KeyError:
            pass

        for _, variable_dataset in datasets.items():
            variable_inputs = prl.parameterize(*variable_dataset, self._tap._config)["input"]
            for _, input_dataset in variable_inputs.items():
                try:
                    return input_dataset["reindex"]
                except KeyError:
                    pass
        return None

    def name_index(self, result, variable_id, index_only_variable=False):
        """..."""
        varindex = result.index.get_level_values(variable_id)

        if variable_id and variable_id.endswith("_id"):
            variable = variable_id.strip("_id")
            return pd.Series(self._tap.create_index(variable).loc[varindex].values, name=variable,
                             index=(varindex if index_only_variable else result.index))

        return pd.Series(varindex.values, name=variable_id, index=result.index)

    def name_reindex_variables(self, result):
        """..."""
        assert ("subtarget_id"  not in result.index.names
                and "circuit_id" not in result.index.names
                and "connectome_id" not in result.index.names)

        of_indices = pd.DataFrame({variable:(self._tap.create_index(variable)
                                             .loc[result.index.get_level_values(f"{variable}_id").values]
                                             .values)
                                   for variable in self.check_reindexing()})

        named_index = pd.MultiIndex.from_frame(of_indices)

        if isinstance(result, pd.Series):
            return pd.Series(result.values, index=named_index)

        if isinstance(result, pd.DataFrame):
            return result.set_index(named_index)

        raise TypeError("Unexpect type %s of result", type(result))

    def name_index_variables(self, result):
        """..."""
        assert ("subtarget_id"  not in result.index.names
                and "circuit_id" not in result.index.names
                and "connectome_id" not in result.index.names)

        varnames = pd.concat([self.name_index(result, var) for var in result.index.names],
                             axis=1)
        variables = pd.MultiIndex.from_frame(varnames)

        assert isinstance(result, pd.Series), f"Illegal type {type(result)}"
        return pd.Series(result.values, name=result.name, index=variables)

    def __call__(self, subtarget, circuit=None, connectome=None, *, control=None, slicing=None):
        """Call to get data using the names for (subtarget, circuit, connectome).
        """
        idx = self.index(subtarget, circuit, connectome)

        if "slicing" not in self.parameters:
            result = self.dataset.loc[idx]

            try:
                evaluate = result.get_value
            except AttributeError:
                pass
            else:
                return evaluate()

            if len(result) == 1:
                return result.iloc[0].get_value()

            return self.name_index_variables(result[~result.index.duplicated(keep="first")])

        slicings = {key for key in self.parameters["slicing"] if key not in ("do-full", "description")}

        if not slicing:
            if "full" not in self.dataset:
                LOG.info("TapDataset %s was configured with slicings, but not full."
                         "\n Please provide a `slicing=<value>`.", self._dataset)
                raise ValueError("TapDataset %s was configured with slicings, but not full."
                                 "\n Please provide a `slicing=<value>`."%(self._dataset,))
            return self.dataset["full"].loc[idx]

        if slicing not in slicings:
            LOG.warning("Slicing %s was not among those configured: \n%s", slcicing, slicings)
            raise ValueError("Slicing %s was not among those configured: \n%s"%(slcicing, slicings))

        return self.dataset[slicing].loc[idx]

    @lazy
    def variable_ids(self):
        """..."""
        if not isinstance(self.dataset, Mapping):
            return self.dataset.index.names
        return {component: (dset.index.names if dset is not None else None)
                for component, dset in self.dataset.items()}


    def frame_component(self, c=None, name_indices=True):
        """..."""
        LOG.info("Frame TapDataset (%s/%s) component %s",
                 self._phenomenon, self._quantity, c)

        if isinstance(c, str):
            assert isinstance(self.dataset, Mapping)
            component = self.dataset[c]
            variable_ids = self.variable_ids[c]
        else:
            assert c is None and not isinstance(self.dataset, Mapping), c
            component = self.dataset
            variable_ids = self.variable_ids

        if component is None:
            LOG.warning("No dataset found for component %s", c)
            return None

        def cleanup_index(r):
            try:
                return r.droplevel(self.variable_ids)
            except (IndexError, KeyError):
                return r

        if name_indices:
            index = pd.concat([self.name_index(component, varid)
                               for varid in variable_ids], axis=1)

            if isinstance(component, pd.Series):
                s = pd.Series(component.values, index=pd.MultiIndex.from_frame(index))
                s = s[~s.index.duplicated(keep="last")]
                return (pd.concat([cleanup_index(v) for v in s.values], keys=s.index)
                        if not self._belazy else s)

            assert isinstance(component, pd.DataFrame), f"Invalid type {type(component)}"
            return component.set_index(pd.MultiIndex.from_frame(index))

        if isinstance(component, pd.Series):
            component = component[~component.index.duplicated(keep="last")]
            return pd.concat(component.values, keys=component.index)

        assert isinstance(component, pd.DataFrame), f"Invalid type {type(component)}"
        return component

    @lazy
    def frame(self):
        """..."""
        if isinstance(self.dataset, Mapping):
            return {c: self.frame_component(c) for c in self.dataset}
        return self.frame_component()
        #index = pd.concat([self.name_index(self.dataset, varid) for varid in self.variable_ids], axis=1)
        #series = pd.Series(self.dataset.values, index=pd.MultiIndex.from_frame(index))
        #series = series[~series.index.duplicated(keep="first")]
        #return pd.concat(series.values, keys=series.index)

    def frame_fun(self, subtarget, circuit, connectome, summarize=None):
        """..."""
        data = self(subtarget, circuit, connectome)
        try:
            data = data.apply(lambda d: d())
        except TypeError:
            pass
        frame = pd.concat(data.values, keys=data.index)
        return frame.groupby(data.index.names).agg(summarize) if summarize else frame
#+end_src

Some analyses may have controls. We do not save the controlled inputs during the execution of the pipeline. While the original input can be loaded from the HDFStore, we will need to generate the controlled input in order to provide a controlled input. Let us implement a ~TapDataset~ method that returns the input of the ~pipeline-step~ associated with the ~TapDataset~,
#+name: tap-dataset-3
#+begin_src python :noweb yes
<<tap-dataset-2>>

    def name_indices(self, component=None):
        """..."""
        if isinstance(component, str):
            assert isinstance(self.dataset, Mapping)
            data = self.dataset[component]
            variable_ids = self.variable_ids[component]
            others = None
        elif component is None:
            assert component is None and not isinstance(self.dataset, Mapping), component
            data = self.dataset
            variable_ids = self.variable_ids
            others = None
        else:
            assert isinstance(component, (pd.Series, pd.DataFrame))
            data = component
            variable_ids = [varid for varid in component.index.names if varid.endswith("_id")]
            other_ids = [varid for varid in component.index.names if not varid.endswith("_id")]
            others = component.index.to_frame()[other_ids]

        named = pd.concat([self.name_index(data, varid) for varid in variable_ids], axis=1)
        return named.join(others) if others is not None else named

    @lazy
    def inputs(self):
        """..."""
        from connsense.develop import parallelization as devprl
        inputs = devprl.generate_inputs(self._dataset, self._tap._config)
        return inputs

    def input(self, subtarget, circuit=None, connectome=None, *, controls=None):
        """..."""
        from connsense.develop import parallelization as devprl
        toc_idx = self.index(subtarget, circuit, connectome)
        inputs = self.inputs.loc[toc_idx]
        if not isinstance(inputs.index, pd.MultiIndex):
            inputs.index = pd.MultiIndex.from_tuples([(v,) for v in inputs.index.values],
                                                     names=[inputs.index.name])

        if not controls:
            return inputs.apply(lambda l: l()) if self._belazy else inputs

        try:
            configured = self.parameters["controls"]
        except KeyError as kerr:
            Log.warning("No controls have been set for the TapDataset %s", self._dataset)
            raise kerr

        controls_configured = devprl.load_control(configured)

        argued = [c for c, _, _ in controls_configured if c.startswith(f"{controls}-")]

        controlled = pd.concat([inputs.xs(c, level="control") for c in argued], axis=0,
                               keys=[c.replace(controls, '')[1:] for c in argued],
                               names=[controls])
        return controlled.apply(lambda l: l()) if not self._belazy else controlled
#+end_src

Additional, planned and requested features.
#+name: tap-dataset-4
#+begin_src jupyter-python :noweb yes
<<tap-dataset-3>>
    @lazy
    def controls(self):
        try:
            control_type = self.inputs.index.get_level_values("control")
        except KeyError as err:
            LOG.error("No control configured ?")
            raise err

        controls = self.inputs[control_type != "original"]
        idxframe = (self.name_indices(component="full") if isinstance(self.dataset, Mapping)
                    else self.name_indices())
        return pd.Series(controls.values,
                         index=pd.MultiIndex.from_frame(idxframe.loc[controls.index]))


    def apply_control(self, algorithm=None, seed=None, *, subtarget, circuit=None, connectome=None,
                      datacall=False):
        """...Get controlled input (datacalls).
        Note: This method makes sense only if this TapDataset is of an analysis.
        Returned datatype depends on the arguments.
        If a subtarget is provided, the randomization algorithm will be run by calling
        the datacall -- so you will get the randomized inputs to this analysis TapDataset.

        Parameters
        ------------
        algorithm: The name of the algorithm to load randomizations for
        seed: One, and only one, of the seeds in the connsense-pipeline
        subtarget: Name of the subtarget to control
        circuit: Name of the circuit whose inputs should be controled.
        ~        If None, and there is only one configured circuit, the circuit level will be removed.
        connectome: Name if the connectome whose inputs should be controlled
        ~        If None, and there is only one configured connectome, the connectome level will be removed.
        """
        from .import parallelization as prl
        controls = self.controls.loc[subtarget]

        if circuit:
            controls = controls.loc[circuit]
        else:
            if len(self._tap.circuits) == 1:
                controls = controls.droplevel("circuit")

        if connectome:
            controls = controls.loc[connectome]
        else:
            if len(self._tap.connectomes) == 1:
                controls = controls.droplevel("connectome")

        if not algorithm:
            return controls

        try:
            configured = self.parameters["controls"]
        except KeyError as kerr:
            Log.warning("No controls have been set for the TapDataset %s", self._dataset)
            raise kerr
        controls_configured = prl.load_control(configured)

        algorithm_index = [c for c, _, _ in controls_configured if c.startswith(f"{algorithm}-")]
        controls = controls.loc[algorithm_index]

        if seed is None:
            return controls

        _datacall = controls.loc[f"{algorithm}-{seed}"]

        return _datacall if datacall else _datacall()

#+end_src

#+RESULTS: eap-dataset
: None

Just like controls, we can have attributes on a ~TapDataset~ that handle the slices,

#+name: tap-dataset
#+begin_src jupyter-python :noweb yes
<<tap-dataset-4>>
    @lazy
    def slicings(self):
        """Slicings for this (analysis) TapDataset, read from connsense-pipeline.yaml
        """
        try:
            configured = self.parameters["slicing"]
        except KeyError:
            raise ConfigurationError(f"Slicing  not configured for {self._dataset}")

        return {key: value for key, value in configured.items()
                if key.lower() not in ("do-full", "do_full", "description")}

    @lazy
    def sliced_inputs(self):
        """..."""
        from .import parallelization as prl

        def slice(s):
            inputs = prl.generate_inputs(self._dataset, self._tap._config, slicing=s,
                                         datacalls_for_slices=True)
            idxframe = self.name_indices(inputs)
            return pd.Series(inputs.values,
                             index=pd.MultiIndex.from_frame(idxframe.loc[inputs.index]))

        return {s: slice(s) for s in self.slicings}

    def slice(self, s, value=None, *, subtarget,  circuit=None, connectome=None,
              datacall=False):
        """..."""
        from .import parallelization as prl
        slices = self.slices[s]

        if circuit:
            slices = slices.loc[circuit]
        else:
            if len(self._tap.circuits) == 1:
                slices = slices.droplevel("circuit")

        if connectome:
            slices = slices.loc[connectome]
        else:
            if len(self._tap.connectomes) == 1:
                slices = slices.droplevel("connectome")

        if not value:
            return slices

        return slices.loc[value]

    def load_adjacency_controls(self, subtargets, control_names, belazy=False):
        """...Load adjacency and control them by the provided name.
        Return pandas Series for the controls, each with an adjacency matrix.
        """
        raise NotImplementedError("INPROGRESS")
#+end_src

*** TAP Adjacency

#+name: tap-adjacency
#+begin_src python
@lazy
def adjacency(tap):
    """Adjacency matrices of subtargets in connsense-TAP
    """
    populations = tap.describe("extract-edge-populations")

    if len(populations) == 0:
        LOG.warning("No populations configured!")
        return None

    def of_(population):
        """..."""
        LOG.info("Load dataset %s: \n%s", population["dataset"], pformat(population["description"]))
        return TapDataset(tap, population["dataset"])

    if len(populations) == 1:
        return of_(populations[0])
    return {population["dataset"][1]: of_(population) for population in populations}

#+end_src

*** TAP Analyses

For analyses we have an additional level, of phenomenon.

#+name: tap-analyses
#+begin_src python
def get_phenomenon(tap, computation_type):
    """..."""
    analysis = computation_type.split('-')
    if analysis[0] != "analyze":
        LOG.warn("%s is not an analysis", computation_type)
        return None


    return '-'.join(analysis[1:])

def find_analyses(tap, phenomenon=None):
    """Find all analyses of phenomenon in the config.
    """

    if phenomenon:
        analyzed = tap.parameters[f"analyze-{phenomenon}"]
        return analyzed["analyses"]

    return {p: tap.find_analyses(phenomenon=p) for p in tap.phenomena}

@property
def phenomena(tap):
    """The analyze phenomena.
    """
    return [tap.get_phenomenon(computation_type=c) for c in tap.parameters if c.startswith("analyze-")]

def describe_analyses(tap, phenomenon=None):
    """..."""
    analyze = "analyze-{}".format
    if phenomenon:
        return tap.describe(analyze(phenomenon))
    return {p: tap.describe(analyze(p)) for p in tap.phenomena}

@lazy
def analyses(tap):
    """..."""
    analyses = tap.describe_analyses()
    return {phenomenon: {q["dataset"][1]: TapDataset(tap, q["dataset"]) for q in quantities}
            for phenomenon, quantities in analyses.items()}

def get_analyses(tap, phenomenon, quantity, control=None, slicing=None):
    """..."""
    dataset = tap.analyses[phenomenon][quantity].load().dataset
    print("get analyses dataset", dataset.keys())
    return dataset[slicing] if slicing else dataset

def load_controls(tap, phenomenon, quantity, label=None, subtargets=None):
    """..."""
    pass

def load_adjacency_controls(tap, analysis, subtargets, control_name):
    """..."""
    pass


#+end_src

#+RESULTS: tap-analyses

We want to get the datasets without a knowledge of what is in the config. We can etpose the common computation types as ~tap-attributes~, with helpful logging and error-messages. All the configured computations follow a convention that allows us to define a ~TapDataset~,

#+name: tap-nodes
#+begin_src python
@lazy
def nodes(tap):
    """Nodes that were extracted
    """
    return TapDataset(self, "extract-node-populations")
#+end_src

*** TAP controls on demand
Add a method
#+name: tap-controls
#+begin_src python
def get_controls(tap, analysis, controls=None):
    """..."""
    from .import parallelization as prl
    ctrls = prl.control_inputs(of_computation=analysis, in_config=tap._config, using_tap=tap)
    return ctrls.reorder_levels([l for l in ctrls.index.names if l!="control"]+["control"])
#+end_src

#+RESULTS: tap-controls
: None

*** TAP external
We may register datas into a ~connsens-TAP-store~ as external to the ~connsense-pipeline~. The data will be saved in a HDF5 file ~external.h5~ in the same location as ~TAP's~ ~connsense.h5~.
#+name: tap-external
#+begin_src jupyter-python :noweb yes
def append_external(tap, dataset, data):
    """Append an external dataset to a TAP instance.
    dataset: [computation_type, quantity] that can be used to determine the h5/group to save.
    data: the actual dataframe to save.
    """
    external_h5 = Path(tap._root).parent/"external.h5"

    computation_type, of_quantity = dataset
    _, group = tap.get_path(computation_type)
    dset = f"{group}/{of_quantity}"

    data.to_hdf(external_h5, key=dset)
    return (external_h5, dset)

def read_external(tap, dataset):
    """Read the external dataset that has been previously registered."""
    external_h5 = Path(tap._root).parent/"external.h5"

    computation_type, of_quantity = dataset
    _, group = tap.get_path(computation_type)
    dset = f"{group}/{of_quantity}"

    return pd.read_hdf(external_h5, key=dset)
#+end_src

#+RESULTS: tap-external

** Results

Finally, let us collect the code in a module,

#+begin_src python :tangle topotap.py :comments org :padline yes :noweb yes
<<tap-imports>>

<<tap-locate>>

<<tap-connsense-index>>

<<tap-dataset>>

<<tap-connsense-hdfstore-init>>

    <<tap-parameters>>

    <<tap-paramkey>>

    <<tap-describe>>

    <<tap-connsense-hdfstore-circuits>>

    <<tap-pour-dataset>>

    <<tap-pour-analyses>>

    <<tap-create-index>>

    <<tap-subtargets>>

    <<tap-nodes>>

    <<tap-adjacency>>

    <<tap-analyses>>

    <<tap-controls>>

    <<tap-external>>
#+end_src

and also the notebook,

#+begin_src jupyter-python :tangle develop_topotap.py :comments no :noweb yes :padline yes
<<notebook-connsense-topotap>>

<<notebook-connsense-subtargets>>

<<notebook-connsense-nodes>>

<<notebook-connsense-nodes-load-lazy>>

<<notebook-connsense-nodes-subtarget-circuit>>

<<notebook-connsense-nodes-subtarget>>

<<notebook-connsense-adjacency>>

<<notebook-connsense-adjacency-load>>

<<notebook-connsense-analyses>>

<<notebook-connsense-analyses-load>>

<<notebook-connsense-simplex-counts-load>>

#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
 2022-10-11 14:30:33,388: Load dataset ('define-subtargets', 'flatmap-columns'):
('Hexaongal prism like columns oriented along cortical layers, from '
 'white-matter to pia. The data is loaded from an NRRD file that maps each '
 'circuit voxel to a subtarget ids corresponding to a flatmap column.The '
 'subtarget ids should be mapped to the subtargets they refer to in a '
 'dataframe provided as the input `info`.')
 2022-10-11 14:30:33,403: Load dataset ('extract-node-populations', 'default'):
('The default population will be that of neurons in the SSCx. To extract the '
 'neurons we will use a `connsense` method that uses ~bluepy~.')
 2022-10-11 14:30:33,404: Initialize a DataFrameStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/connsense.h5 / nodes/populations/default
 2022-10-11 14:30:33,413: Initialize a DataFrameStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/connsense.h5 / nodes/populations/default
Available analyses:
{'connectivity': {'simplex-counts': <connsense.develop.topotap.TapDataset object at 0x7ffe18be9a60>}}
 2022-10-11 14:30:33,637: Initialize a DataFrameStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/connsense.h5 / nodes/populations/default
<class 'pandas.core.frame.DataFrame'>
Int64Index: 4570 entries, 0 to 4569
Data columns (total 11 columns):
 #   Column         Non-Null Count  Dtype
---  ------         --------------  -----
 0   gid            4570 non-null   int64
 1   region         4570 non-null   category
 2   layer          4570 non-null   int64
 3   x              4570 non-null   float64
 4   y              4570 non-null   float64
 5   z              4570 non-null   float64
 6   synapse_class  4570 non-null   category
 7   mtype          4570 non-null   category
 8   etype          4570 non-null   category
 9   morphology     4570 non-null   category
 10  depth          4570 non-null   float64
dtypes: category(5), float64(4), int64(2)
memory usage: 3.0 MB
 2022-10-11 14:30:33,904: Initialize a DataFrameStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/connsense.h5 / nodes/populations/default
<class 'pandas.core.frame.DataFrame'>
Int64Index: 1823 entries, 0 to 1822
Data columns (total 11 columns):
 #   Column         Non-Null Count  Dtype
---  ------         --------------  -----
 0   gid            1823 non-null   int64
 1   region         1823 non-null   category
 2   layer          1823 non-null   int64
 3   x              1823 non-null   float64
 4   y              1823 non-null   float64
 5   z              1823 non-null   float64
 6   synapse_class  1823 non-null   category
 7   mtype          1823 non-null   category
 8   etype          1823 non-null   category
 9   morphology     1823 non-null   category
 10  depth          1823 non-null   float64
dtypes: category(5), float64(4), int64(2)
memory usage: 2.9 MB
 2022-10-11 14:30:34,133: Load dataset ('extract-edge-populations', 'local'):
None
 2022-10-11 14:30:34,200: Pour analyses for analyze-connectivity
 2022-10-11 14:30:34,200: Initialize a SeriesStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/connsense.h5 / analyses/connectivity/simplex-counts
 2022-10-11 14:30:34,211: Pour analyses for analyze-connectivity
 2022-10-11 14:30:34,212: Initialize a SeriesStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/connsense.h5 / analyses/connectivity/simplex-counts
<class 'pandas.core.frame.DataFrame'>
Int64Index: 1823 entries, 0 to 1822
Data columns (total 11 columns):
 #   Column         Non-Null Count  Dtype
---  ------         --------------  -----
 0   gid            1823 non-null   int64
 1   region         1823 non-null   category
 2   layer          1823 non-null   int64
 3   x              1823 non-null   float64
 4   y              1823 non-null   float64
 5   z              1823 non-null   float64
 6   synapse_class  1823 non-null   category
 7   mtype          1823 non-null   category
 8   etype          1823 non-null   category
 9   morphology     1823 non-null   category
 10  depth          1823 non-null   float64
dtypes: category(5), float64(4), int64(2)
memory usage: 2.9 MB
{'connectivity': {'simplex-counts': <connsense.develop.topotap.TapDataset object at 0x7ffe18be9a60>}}
#+end_example
: dim
: 0      1823
: 1     88675
: 2    276930
: 3     85837
: 4      3495
: 5        21
: Name: simplex_count, dtype: int64
:END:

* Controls

** Test develop

We have setup a computation of controls for ~simplex-counts~ for the original adjacencies.

#+name: notebook-connsense-controls
#+begin_src jupyter-python
CTRLSPACE = CONNSPACE / "test"

SIMPSPACE = CTRLSPACE / "run" / "analyze-connectivity" / "simplex-counts"

setup_items = list(f.name for f in SIMPSPACE.glob('*'))
setup = {"compute_nodes": [c for c in setup_items if c.startswith("compute-node-")],
         "configs": [x for x in setup_items if not x.startswith("compute-node-")]}

print("Use number of compute nodes: ", len(setup["compute_nodes"]))
pprint(setup["configs"])
#+end_src

#+RESULTS: notebook-connsense-controls
: Use number of compute nodes:  100
: ['subtargets.h5',
:  'pipeline.yaml',
:  'runtime.yaml',
:  'setup.json',
:  'launchscript-0.sh',
:  'description.json']

Let us take a peak at the datasets at one of the compute nodes,

#+name: notebook-connsense-controls-cn0
#+begin_src jupyter-python
CN0 = SIMPSPACE / "compute-node-0"

pprint(list(f.name for f in CN0.glob('*')))
#+end_src

#+RESULTS: notebook-connsense-controls-cn0
#+begin_example
['connsense-1.h5',
 'analyze__connectivity.err',
 'analyze__connectivity.out',
 'inputs.h5',
 'connsense-0.h5',
 'connsense-3.h5',
 'pipeline.yaml',
 'INPROGRESS',
 'runtime.yaml',
 'topology_analysis.err',
 'setup.json',
 'analyze-connectivity.sbatch']
#+end_example

* Examples

Let us develop some examples to show how to work with ~topotap~.

** Subset subtargets

For development purposes, 239 subtargets in the SSCx flatmap are too many. Let us use ~topotap~ to create a subset of these subtargets and save it to a workspace where we can test develop...

#+begin_src jupyter-python :tangle develop_topotap.py
topotap.subtargets
#+end_src

#+RESULTS:
#+begin_example
             subtarget  flat_i  flat_j        flat_x  flat_y
subtarget_id
1               R18;C0     -27      27  3.802528e-13  6210.0
2               R19;C0     -28      29  1.991858e+02  6555.0
3               R18;C1     -26      28  3.983717e+02  6210.0
4               R19;C1     -27      30  5.975575e+02  6555.0
5               R16;C0     -24      24  3.380025e-13  5520.0
...                ...     ...     ...           ...     ...
236             R4;C12       6      18  4.780460e+03  1380.0
237             R9;C15       2      29  6.174761e+03  3105.0
238            R15;C13      -9      36  5.378018e+03  5175.0
239             R3;C11       7      16  4.581274e+03  1035.0
240            R15;C15      -7      38  6.174761e+03  5175.0

[240 rows x 5 columns]
#+end_example
