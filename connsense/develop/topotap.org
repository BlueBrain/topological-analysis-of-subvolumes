#+PROPERTY: header-args:jupyter-python :session ~/Library/Jupyter/runtime/active-py38-ssh.json
#+PROPERTY: header-args:jupyter :session ~/Library/Jupyter/runtime/active-py38-ssh.json

#+STARTUP: overview
#+title: Topotap

* Setup :noexport:

In our discussion we will develop scientific concepts to measure the circuit, and implement Python functions to compute them. Here we setup a notebook template to test and explore, and the structure of a ~Python~ package for our methods.

** A notebook template to explore and develop

Let us setup an interactive ~Python~ session where we can run the code developed here.

#+begin_src jupyter
print("Welcome to EMACS Jupyter")
#+end_src

#+RESULTS:
: Welcome to EMACS Jupyter

** Introduction

#+name: notebook-init
#+begin_src jupyter-python
from importlib import reload
from collections.abc import Mapping
from collections import OrderedDict
from pprint import pprint, pformat
from pathlib import Path

import numpy as np
import pandas as pd

import matplotlib

reload(matplotlib)
from matplotlib import pylab as plt
import seaborn as sbn
GOLDEN = (1. + np.sqrt(5.))/2.

from IPython.display import display

from bluepy import Synapse, Cell, Circuit

print("We will plot golden aspect ratios: ", GOLDEN)
#+end_src

#+RESULTS: notebook-init
: We will plot golden aspect ratios:  1.618033988749895

We have run ~connsense-CRAP~ for the SSCx dissemination variant /Bio-M/, extracting data that we will use to compute the factology. Here is a list of workspaces we will need to generate factsheets.

** Workspaces

#+name: notebook-workspaces
#+begin_src jupyter-python
from connsense.pipeline import pipeline
from connsense.develop import parallelization as devprl

from connsense.pipeline.store import store as tap_store
from connsense.develop import topotap as devtap

ROOTSPACE = Path("/")
PROJSPACE = ROOTSPACE / "gpfs/bbp.cscs.ch/project/proj83"
CONNSPACE = PROJSPACE / "home/sood" / "topological-analysis-subvolumes/test/v2"
#+end_src

#+RESULTS: notebook-workspaces

While test-developing it will be good to have direct access to the ~connsense-TAP-store~ we will use,

We can collect the code above in a ~Pyhton~ template file that can be used to generate notebooks,

** ~connsense~ Modules

#+name: notebook-connsense-tap
#+begin_src jupyter-python
topaz = pipeline.TopologicalAnalysis(CONNSPACE/"pipeline.yaml", CONNSPACE/"runtime.yaml")
tap = tap_store.HDFStore(topaz._config)
circuit = tap.get_circuit("Bio_M")

topotap = devtap.HDFStore(CONNSPACE/"pipeline.yaml")
print("Available analyses: ")
pprint(topotap.analyses)
#+end_src

#+RESULTS: notebook-connsense-tap
:  2022-10-25 14:37:15,281: Configure slurm for create-index
:  2022-10-25 14:37:15,283: No runtime configured for computation type create-index
:  2022-10-25 14:37:15,284: Configure slurm for define-subtargets
:  2022-10-25 14:37:15,285: Configure slurm for extract-node-populations
:  2022-10-25 14:37:15,285: Configure slurm for extract-edge-populations
:  2022-10-25 14:37:15,286: Configure slurm for analyze-connectivity
:  2022-10-25 14:37:15,288: Load circuit Bio_M
: Available analyses:
: {'connectivity': {'model-params-dd2': <connsense.develop.topotap.TapDataset object at 0x7ffec6bc2970>,
:                   'simplex-counts': <connsense.develop.topotap.TapDataset object at 0x7ffed04aea90>}}


Finally, here is a template that we can use to start test-developing. We will deposit the code in a sub-directory, of the directory holding this file.

#+begin_src jupyter-python :tangle develop_topotap.py :comments no :noweb yes :padline yes
# %% [markdown]
"""# Test Develop a Circuit Factology
"""

# %% [code]
<<notebook-init>>

<<notebook-workspaces>>

<<notebook-connsense-tap>>

<<notebook-reloads>>


#+end_src

#+RESULTS:
#+begin_example
 2022-10-25 14:37:45,622: Configure slurm for create-index
 2022-10-25 14:37:45,623: No runtime configured for computation type create-index
 2022-10-25 14:37:45,624: Configure slurm for define-subtargets
 2022-10-25 14:37:45,625: Configure slurm for extract-node-populations
 2022-10-25 14:37:45,626: Configure slurm for extract-edge-populations
 2022-10-25 14:37:45,627: Configure slurm for analyze-connectivity
 2022-10-25 14:37:45,628: Load circuit Bio_M
We will plot golden aspect ratios:  1.618033988749895
Available analyses:
{'connectivity': {'model-params-dd2': <connsense.develop.topotap.TapDataset object at 0x7ffec68e1250>,
                  'simplex-counts': <connsense.develop.topotap.TapDataset object at 0x7ffec6ba27c0>}}
#+end_example


* Introduction

 While the ~Python-environment~ setup above loads the other packages that we will need, we will be most interested in discussing a layer for topological-analyses.

** DevNotes
We will test-develop our implementations, using a notebook,

#+name: notebook-connsense-topotap
#+begin_src jupyter-python :tangle develop_topotap.py
# %% [markdown]
"""Load a connsense-TAP to analyze topology of a circuit
"""
# %% [code]

from connsense.develop import topotap as topotap_store
reload(topotap_store)
topotap = topotap_store.HDFStore(CONNSPACE/"pipeline.yaml")
print("Available analyses: ")
pprint(topotap.analyses)
#+end_src

#+RESULTS: notebook-connsense-topotap
: Available analyses:
: {'connectivity': {'model-params-dd2': <connsense.develop.topotap.TapDataset object at 0x7ffe2e3bf250>,
:                   'simplex-counts': <connsense.develop.topotap.TapDataset object at 0x7ffe2e3bfa30>}}

Let us analyze degrees of the subtargets. To get to the data about circuit edges / adjacency, we start by looking at the ~circuit-subtargets~ that we are analyzed,

#+name: notebook-connsense-subtargets
#+begin_src jupyter-python :tangle develop_topotap.py
# %% [markdown]
"""## Subtargets in connsense-TAP
"""
# %% [code]

topotap.subtargets
#+end_src

#+RESULTS: notebook-connsense-subtargets
:RESULTS:
:  2022-10-31 15:23:12,272: Load dataset ('define-subtargets', 'flatmap-columns'):
: ('Hexaongal prism like columns oriented along cortical layers, from '
:  'white-matter to pia. The data is loaded from an NRRD file that maps each '
:  'circuit voxel to a subtarget ids corresponding to a flatmap column.The '
:  'subtarget ids should be mapped to the subtargets they refer to in a '
:  'dataframe provided as the input `info`.')
#+begin_example
             subtarget  flat_i  flat_j        flat_x  flat_y
subtarget_id
1               R18;C0     -27      27  3.802528e-13  6210.0
2               R19;C0     -28      29  1.991858e+02  6555.0
3               R18;C1     -26      28  3.983717e+02  6210.0
4               R19;C1     -27      30  5.975575e+02  6555.0
5               R16;C0     -24      24  3.380025e-13  5520.0
...                ...     ...     ...           ...     ...
236             R4;C12       6      18  4.780460e+03  1380.0
237             R9;C15       2      29  6.174761e+03  3105.0
238            R15;C13      -9      36  5.378018e+03  5175.0
239             R3;C11       7      16  4.581274e+03  1035.0
240            R15;C15      -7      38  6.174761e+03  5175.0

[240 rows x 5 columns]
#+end_example
:END:

This is the information that was provided in the configured definition of subtargets. In the earlier version of ~connsense-TAP~ this information was part of each ~connsense-dataset~'s index. However it does not need to. We can get the data from ~connsense-TAP~ store for analyses that need it.

For each subtarget, ~connsense-TAP~ saves the ~circuit-cell-gids~ as a list. We do not expect each user to need this list --- ~connsense-TAP~ hides that information. However the ~gids~ are avaialble among the node properties.

#+name: notebook-connsense-nodes
#+begin_src jupyter-python :tangle develop_topotap.py
# %% [markdown]
"""## Nodes in connsense-TAP
"""
# %% [code]

topotap.nodes.dataset
#+end_src

#+RESULTS: notebook-connsense-nodes
:RESULTS:
:  2022-10-31 15:23:27,560: Load dataset ('extract-node-populations', 'default'):
: ('The default population will be that of neurons in the SSCx. To extract the '
:  'neurons we will use a `connsense` method that uses ~bluepy~.')
:  2022-10-31 15:23:27,562: Initialize a DataFrameStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/test/connsense.h5 / nodes/populations/default
#+begin_example
subtarget_id  circuit_id
1             0             <connsense.analyze_connectivity.matrices.BeLaz...
2             0             <connsense.analyze_connectivity.matrices.BeLaz...
3             0             <connsense.analyze_connectivity.matrices.BeLaz...
4             0             <connsense.analyze_connectivity.matrices.BeLaz...
5             0             <connsense.analyze_connectivity.matrices.BeLaz...
                                                  ...
235           0             <connsense.analyze_connectivity.matrices.BeLaz...
236           0             <connsense.analyze_connectivity.matrices.BeLaz...
237           0             <connsense.analyze_connectivity.matrices.BeLaz...
238           0             <connsense.analyze_connectivity.matrices.BeLaz...
239           0             <connsense.analyze_connectivity.matrices.BeLaz...
Length: 239, dtype: object
#+end_example
:END:

The contents for each ~circuit-subtarget~ entry are a mysterious object ~BeLazy~ which is nothing more than an instruction to load the data,

#+name: notebook-connsense-nodes-load-lazy
#+begin_src jupyter-python :tangle develop_topotap.py
# %% [markdown]
"""Contents of nodes
"""
# %% [code]

topotap.nodes.dataset.iloc[0].get_value().info()
#+end_src

#+RESULTS: notebook-connsense-nodes-load-lazy
#+begin_example
 2022-10-31 15:23:35,938: Initialize a DataFrameStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/test/connsense.h5 / nodes/populations/default
<class 'pandas.core.frame.DataFrame'>
Int64Index: 4570 entries, 0 to 4569
Data columns (total 11 columns):
 #   Column         Non-Null Count  Dtype
---  ------         --------------  -----
 0   gid            4570 non-null   int64
 1   region         4570 non-null   category
 2   layer          4570 non-null   int64
 3   x              4570 non-null   float64
 4   y              4570 non-null   float64
 5   z              4570 non-null   float64
 6   synapse_class  4570 non-null   category
 7   mtype          4570 non-null   category
 8   etype          4570 non-null   category
 9   morphology     4570 non-null   category
 10  depth          4570 non-null   float64
dtypes: category(5), float64(4), int64(2)
memory usage: 3.0 MB
#+end_example

Lazy-data is necessary to track all the subtargets in a dataset as each can be big. However we can interact with ~connsense-TAP~ without having to know about laziness of the data,

#+name: notebook-connsense-nodes-subtarget-circuit
#+begin_src jupyter-python :tangle develop_topotap.py
# %% [markdown]
"""Contents of nodes
"""
# %% [code]

topotap.nodes(subtarget="R19;C0", circuit="Bio_M").info()
#+end_src

#+RESULTS: notebook-connsense-nodes-subtarget-circuit
#+begin_example
 2022-10-31 15:23:48,974: Initialize a DataFrameStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/test/connsense.h5 / nodes/populations/default
<class 'pandas.core.frame.DataFrame'>
Int64Index: 1823 entries, 0 to 1822
Data columns (total 11 columns):
 #   Column         Non-Null Count  Dtype
---  ------         --------------  -----
 0   gid            1823 non-null   int64
 1   region         1823 non-null   category
 2   layer          1823 non-null   int64
 3   x              1823 non-null   float64
 4   y              1823 non-null   float64
 5   z              1823 non-null   float64
 6   synapse_class  1823 non-null   category
 7   mtype          1823 non-null   category
 8   etype          1823 non-null   category
 9   morphology     1823 non-null   category
 10  depth          1823 non-null   float64
dtypes: category(5), float64(4), int64(2)
memory usage: 2.9 MB
#+end_example


We don't have to provide the circuit,

#+name: notebook-connsense-nodes-subtarget
#+begin_src jupyter-python :tangle develop_topotap.py
# %% [markdown]
"""Nodes of a subtarget
"""
# %% [code]

topotap.nodes(subtarget="R19;C0").info()
#+end_src

#+RESULTS: notebook-connsense-nodes-subtarget
#+begin_example
 2022-10-31 15:24:03,195: Initialize a DataFrameStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/test/connsense.h5 / nodes/populations/default
<class 'pandas.core.frame.DataFrame'>
Int64Index: 1823 entries, 0 to 1822
Data columns (total 11 columns):
 #   Column         Non-Null Count  Dtype
---  ------         --------------  -----
 0   gid            1823 non-null   int64
 1   region         1823 non-null   category
 2   layer          1823 non-null   int64
 3   x              1823 non-null   float64
 4   y              1823 non-null   float64
 5   z              1823 non-null   float64
 6   synapse_class  1823 non-null   category
 7   mtype          1823 non-null   category
 8   etype          1823 non-null   category
 9   morphology     1823 non-null   category
 10  depth          1823 non-null   float64
dtypes: category(5), float64(4), int64(2)
memory usage: 2.9 MB
#+end_example

We can access the adjacencies,

#+name: notebook-connsense-adjacency
#+begin_src jupyter-python :tangle develop_topotap.py
# %% [markdown]
"""## Adjacency datasets
"""
# %% [code]
topotap.adjacency["local"].dataset
#+end_src

#+RESULTS: notebook-connsense-adjacency
:RESULTS:
:  2022-10-31 15:24:30,624: Load dataset ('extract-edge-populations', 'local'):
: None
:  2022-10-31 15:24:30,626: Load dataset ('extract-edge-populations', 'long-range'):
: 'Add connections from two connectomes in section input/connectome'
#+begin_example
subtarget_id  circuit_id  connectome_id
1             0           0                <connsense.io.write_results.LazyMatrix object ...
2             0           0                <connsense.io.write_results.LazyMatrix object ...
3             0           0                <connsense.io.write_results.LazyMatrix object ...
4             0           0                <connsense.io.write_results.LazyMatrix object ...
5             0           0                <connsense.io.write_results.LazyMatrix object ...
                                                                 ...
235           0           0                <connsense.io.write_results.LazyMatrix object ...
236           0           0                <connsense.io.write_results.LazyMatrix object ...
237           0           0                <connsense.io.write_results.LazyMatrix object ...
238           0           0                <connsense.io.write_results.LazyMatrix object ...
239           0           0                <connsense.io.write_results.LazyMatrix object ...
Length: 239, dtype: object
#+end_example
:END:

That behaves similarly to ~topotap.nodes~ with an additional level for connectome. We have only one connectome that allows us to get adjacencies,

#+name: notebook-connsense-adjacency-load
#+begin_src jupyter-python :tangle develop_topotap.py
# %% [markdown]
""" Adjacency of a subtarget
"""
# %% [code]
topotap.adjacency["local"].dataset.iloc[0].get_value()
#+end_src

#+RESULTS: notebook-connsense-adjacency-load
: <4570x4570 sparse matrix of type '<class 'numpy.int64'>'
: 	with 431358 stored elements in Compressed Sparse Row format>


#+begin_src jupyter-python :tangle develop_topotap.py
topotap.adjacency["local"](subtarget="R19;C0")
#+end_src

#+RESULTS:
: <1823x1823 sparse matrix of type '<class 'numpy.int64'>'
: 	with 88675 stored elements in Compressed Sparse Row format>

And we have simplex-counts
#+name: notebook-connsense-analyses
#+begin_src jupyter-python :tangle develop_topotap.py
# %% [markdown]
"""## Analyses
"""
# %% [code]
pprint(topotap.analyses)
#+end_src

#+RESULTS: notebook-connsense-analyses
: {'connectivity': {'model-params-dd2': <connsense.develop.topotap.TapDataset object at 0x7ffe2e3bf250>,
:                   'simplex-counts': <connsense.develop.topotap.TapDataset object at 0x7ffe2e3bfa30>}}

that we can access using the same indexing scheme,

#+name: notebook-connsense-analyses-load
#+begin_src jupyter-python :tangle develop_topotap.py
# %% [markdown]
"""Simplex counts
"""
# %% [code]
simplex_counts = topotap.analyses["connectivity"]["simplex-counts"]
simplex_counts.dataset
#+end_src

#+RESULTS: notebook-connsense-analyses-load
:RESULTS:
:  2022-10-31 15:25:06,047: Pour analyses for analyze-connectivity
:  2022-10-31 15:25:06,048: Initialize a SeriesStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/test/connsense.h5 / analyses/connectivity/simplex-counts
#+begin_example
subtarget_id  circuit_id  connectome_id
1             0           0                <connsense.analyze_connectivity.matrices.BeLaz...
2             0           0                <connsense.analyze_connectivity.matrices.BeLaz...
3             0           0                <connsense.analyze_connectivity.matrices.BeLaz...
4             0           0                <connsense.analyze_connectivity.matrices.BeLaz...
5             0           0                <connsense.analyze_connectivity.matrices.BeLaz...
                                                                 ...
235           0           0                <connsense.analyze_connectivity.matrices.BeLaz...
236           0           0                <connsense.analyze_connectivity.matrices.BeLaz...
237           0           0                <connsense.analyze_connectivity.matrices.BeLaz...
238           0           0                <connsense.analyze_connectivity.matrices.BeLaz...
239           0           0                <connsense.analyze_connectivity.matrices.BeLaz...
Length: 239, dtype: object
#+end_example
:END:

That also responds to calls,

#+name: notebook-connsense-simplex-counts-load
#+begin_src jupyter-python :tangle develop_topotap.py
# %% [markdown]
"""Simplex counts
"""
# %% [code]
simplex_counts = topotap.analyses["connectivity"]["simplex-counts"]
simplex_counts("R19;C0")
#+end_src

#+RESULTS: notebook-connsense-simplex-counts-load
:RESULTS:
:  2022-10-31 15:25:10,376: Pour analyses for analyze-connectivity
:  2022-10-31 15:25:10,377: Initialize a SeriesStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/test/connsense.h5 / analyses/connectivity/simplex-counts
: dim
: 0      1823
: 1     88675
: 2    276930
: 3     85837
: 4      3495
: 5        21
: Name: simplex_count, dtype: int64
:END:

#+RESULTS:
:RESULTS:
:  2022-10-11 14:26:40,429: Pour analyses for analyze-connectivity
:  2022-10-11 14:26:40,431: Initialize a SeriesStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/connsense.h5 / analyses/connectivity/simplex-counts
: dim
: 0      1823
: 1     88675
: 2    276930
: 3     85837
: 4      3495
: 5        21
: Name: simplex_count, dtype: int64
:END:


* HDFStore

The long-range connectivity in the SSCx circuit is based on a topographical mapping connections between subregions.
The mapping projects each voxel in the circuit atlas to a /pixel/ in the circuit's /flatmap/. This ~voxel-->pixel~ map, from the circuit's physical space to it's ~flatmap~ space, is used to compute neighborhoods of /intra-SSCx/ white-matter (WM) projections. WM projections are expected to enter the cortex from under layer 6 and proceed upwards along cortical layers. Thalamo-cortical (TC) projections follow similar trajectories. We want to analyze local connectivity in such cortical columns.

We want an interface to a ~connsense-TAP~ instance developed for topological network analyses of a brain circuit. Here we implement o replacement of ~connsense.pipeline.store.store.HDFStore~ adding methods for simpler interaction with the pipeline's data.

** Imports
#+name: tap-imports
#+begin_src python
"""Interface to the HD5-store where the pipeline stores it's data.
"""
from collections.abc import Iterable, Mapping
from collections import OrderedDict, defaultdict
from copy import deepcopy
from pprint import pformat
from lazy import lazy
from pathlib import Path
import h5py

import pandas as pd

from connsense import plugins
from connsense.define_subtargets.config import SubtargetsConfig
from connsense import analyze_connectivity as anzconn
from connsense.analyze_connectivity import matrices
from connsense.io import read_config
from connsense.io.write_results import (read as read_dataset,
                                        read_subtargets,
                                        read_node_properties,
                                        read_toc_plus_payload)
from connsense.io import logging
from connsense.pipeline import ConfigurationError, NotConfiguredError, COMPKEYS
from connsense.pipeline.parallelization import parallelization as prl

LOG = logging.get_logger(__name__)
#+end_src

Paths are specified in ~connsense-TAP~ condiguration, using which we can locate the H5 file with the data that results from running ~connsense-TAP~. The configuration provides paths to the H5 file, and the keys in the data-store for each of the computations / steps in the configuration. An HDFStore interface will need these paths,

** Loaders

#+name: tap-locate
#+begin_src python
def locate_store(config, in_connsense_h5=None):
    """..."""
    if not in_connsense_h5:
        return Path(config["paths"]["input"]["store"])
    return Path(in_connsense_h5)


def group_steps(config):
    """..."""
    inputs = config["paths"]["input"]["steps"]
    return {step: group for step, (_, group) in inputs.items()}

#+end_src

~connsense-TAP~ store data with integer IDs in the index, while saving the names for the entries in H5. The names for IDs used are,

#+name: tap-connsense-index
#+begin_src python
SUBTARGET_ID = "subtarget_id"
CIRCUIT_ID = "circuit_id"
CONNECTOME_ID = "connectome_id"
MTYPE_ID = "mtype_id"
MORPHOLOGY_ID = "morphology_id"

from connsense.pipeline import COMPKEYS, PARAMKEY, ConfigurationError, NotConfiguredError
#+end_src

Each individual configured of computation is entered in a list under a key that depends on it's computation type. Here is a list of these parameter keys for each computation type that ~connsense-TAP~ knows about,

#+begin_src python
PARAMKEY = {"define-subtargets": "definitions",
            "extract-voxels": "annotations",
            "extract-node-types": "modeltypes",
            "extract-edge-types": "models",
            "create-index": "variables",
            "extract-node-populations": "populations",
            "extract-edge-populations": "populations",
            "sample-edge-populations": "analyses",
            "randomize-connectivity": "algorithms",
            "configure-inputs": "analyses",
            "analyze-geometry": "analyses",
            "analyze-node-types": "analyses",
            "analyze-composition": "analyses",
            "analyze-connectivity": "analyses",
            "analyze-physiology": "analyses"}
#+end_src

#+RESULTS:
: None

We can instantiate an HDFStore interface instance with a path to the ~pipeline~ config, or the ~config~ itself. The ~config~ should contain a path to the H5 file that contains ~connsense-TAP~ data, or we can pass one as a second argument,

** HDFStore

#+name: tap-connsense-hdfstore-init
#+begin_src python
class HDFStore:
    """An interface to the H5 data extracted by connsense-TAP.
    """
    def __init__(self, config, in_connsense_h5=None):
        """Initialize an instance of connsense-TAP HDFStore.

        config: Path to a YAML / JSON file that configures the pipeline, or a Mapping resulting from reading
        ~       such a config file.
        in_consense_h5: Path to the connsense-TAP H5 store if different from the one configured
        ~               This can be used for testing the data produced in individual compute-nodes during
        ~               a pipeline run.
        """
        self._config = read_config.read(config) if not isinstance(config, Mapping) else config
        self._root = locate_store(self._config, in_connsense_h5)
        self._groups = group_steps(self._config)

#+end_src

*** Parameters

Once we have an object to interface with a ~connsense-TAP~, we will want to load datasets to further analyze them. Information about the configured computations are in the section ~parameters~,

#+name: tap-parameters
#+begin_src python
@lazy
def parameters(tap):
    """Section `parameters` of the config, loaded without `create-index`.
    """
    return {param: config for param, config in tap._config["parameters"].items() if param != "create-index"}

#+end_src

Each parameters entry is for a ~computation-type~ that may have multiple quantities under it. Each ~(computation-type, of_quantity)~ is a dataset that ~connsense-TAP~ can provide usWe can ask ~connsense-TAP~ to describe these computations. The quantities for a ~parameters~ entry are provided under a key,

#+name: tap-paramkey
#+begin_src python
def get_paramkey(tap, computation_type):
    """..."""
    return PARAMKEY[computation_type]

#+end_src

Here we have assumed that the computations are valid, /i.e/ they have a ~paramkey~ entry known to ~connsense-TAP~. We should check the configured ~computation-types~ against ~connsense-TAP~ when ~HDFStore~ is initialized (TODO).

*** Descriptions

We want *rich* descriptions from ~connsense-TAP~ about the ~config~ as well as the extracted data.

#+name: tap-describe
#+begin_src python
def describe(tap, computation_type=None, of_quantity=None):
    """...Describe the dataset associated with a `(computation_type, of_quantity)`.

    computation_type: should be an entry in the configuration section parameters,
    ~                 if not provided, all computation-types
    of_quantity: should be an entry under argued `computation_type`
    ~            if not provided, all quantities under `computation_type`
    """
    if not computation_type:
        assert not of_quantity, "because a quantity without a computation-type does not make sense."
        return {c: tap.describe(computation_type=c) for c in tap.parameters}

    try:
        config = tap.parameters[computation_type]
    except KeyError as kerr:
        LOG.error("computation-type %s not configured! Update the config, or choose from \n%s",
                  computation_type, pformat(tap.parameters.keys()))
        raise NotConfiguredError(computation_type) from kerr

    paramkey = tap.get_paramkey(computation_type)
    try:
        config = config[paramkey]
    except KeyError as kerr:
        LOG.error("Missing %s entries in %s config.", paramkey, computation_type)
        raise ConfigurationError(f"{paramkey} entries for {computation_type}")

    def describe_quantity(q):
        return {"description": config[q].get("description", None), "dataset": (computation_type, q)}

    if not of_quantity:
        return [describe_quantity(q) for q in config]

    return describe_quantity(q=of_quantity)


#+end_src

*** TAP datasets

Data formats used by ~connsense-TAP~ may different between ~computation-types~.

#+name: tap-pour-dataset
#+begin_src python

def get_path(tap, computation_type):
    """..."""
    return (tap._root, tap._groups[computation_type])

def pour_dataset(tap, computation_type, of_quantity):
    """..."""
    connsense_h5, hdf_group = tap.get_path(computation_type)
    dataset = '/'.join([hdf_group, of_quantity])

    with h5py.File(tap._root, 'r') as hdf:
        if "data" in hdf[dataset]:
            dataset = '/'.join([dataset, "data"])

    if computation_type == "extract-node-populations":
        return matrices.get_store(connsense_h5, dataset, pd.DataFrame).toc

    if computation_type == "extract-edge-populations":
        return read_toc_plus_payload((connsense_h5, dataset), "extract-edge-populations")

    if computation_type.startswith("analyze-"):
        return tap.pour_analyses(computation_type, of_quantity)

    return read_dataset((connsense_h5, dataset), computation_type)

def pour(tap, dataset):
    """For convenience, allow queries with tuples (computation_type, of_quantity).
    """
    return tap.pour_dataset(*dataset)

#+end_src

*** TAP analyses

Analyses ~computation-type~ should be of the form ~analyze-phenomenon~. This allows us to have a method to ~pour-analyses~,
#+name: tap-pour-analyses
#+begin_src python

def decompose(self, computation_type, of_quantity):
    """Some computations may have components.
    We need to strip computation keys from the config, and return the resulting dict.
    """
    parameters = prl.parameterize(computation_type, of_quantity, self._config)
    return {var: val for var, val in parameters.items() if var not in COMPKEYS}


def pour_analyses(tap, computation_type, quantity):
    """Pour the results of running an analysis computation.
    """
    LOG.info("Pour analyses for %s", computation_type)
    connsense_h5, hdf_group = tap.get_path(computation_type)
    dataset = '/'.join([hdf_group, quantity])
    paramkey = tap.get_paramkey(computation_type)

    def pour_component(c, parameters):
        """..."""
        LOG.info("Pour %s %s component %s: \n%s\n from store %s", computation_type, quantity, c, pformat(parameters),
                 (connsense_h5, '/'.join([dataset, c])))
        store = matrices.get_store(connsense_h5, '/'.join([dataset, c]), parameters["output"], in_mode='r')
        return store.toc if store else None

    components = tap.decompose(computation_type, quantity)
    if not components:
        parameters = tap.parameters[computation_type][paramkey][quantity]
        store = matrices.get_store(connsense_h5, dataset, parameters["output"], in_mode='r')
        return store.toc if store else None

    return {'/'.join([quantity, c]): pour_component(c, parameters) for c, parameters in components.items()}


#+end_src

*** TAP Indices

With methods to pour datasets from a ~connsense-TAP~, we can provide some convenient interfaces to get subtargets, nodes, adjacencies, analyses. In its H5 data, ~connsense-TAP~ will index the computations using the configuration entry for ~parameters/create-index~,

#+name: tap-create-index
#+begin_src python
def create_index(tap, variable):
    """..."""
    described = tap._config["parameters"]["create-index"]["variables"][variable]

    if isinstance(described, pd.Series):
        values = descibed.values
    elif isinstance(described, Mapping):
        try:
            dataset = described["dataset"]
        except KeyError as kerr:
            LOG.error("Cannot create an index for %s of no dataset in config.", variable)
            raise ConfigurationError("No create-index %s dataset", variable)
        return tap.pour(dataset)
    elif isinstance(described, Iterable):
        values = list(described)
    else:
        raise ConfigurationError(f"create-index %s using config \n%s", pformat(described))

    return pd.Series(values, name=variable, index=pd.RangeIndex(0, len(values), 1, name=f"{variable}_id"))


#+end_src

*** TAP Subtargets

#+name: tap-subtargets
#+begin_src python
@lazy
def subtargets(tap):
    """Subtargets in connsense-TAP
    """
    definitions = tap.describe("define-subtargets")
    pour_subtargets = lambda dataset: tap.pour(("define-subtargets", dataset))

    if len(definitions) == 0:
        LOG.warning("No subtargets configured!")
        return None

    def of_(definition):
        """..."""
        LOG.info("Load dataset %s: \n%s", definition["dataset"], pformat(definition["description"]))
        _, group = definition["dataset"]
        subtargets = pour_subtargets(f"{group}/name")
        info = pour_subtargets(f"{group}/info")
        return pd.concat([subtargets, info], axis=1)

    if len(definitions) == 1:
        return of_(definitions[0])
    return {definition["dataset"][1]: of_(definition) for definition in definitions}


#+end_src

*** TAP Nodes
#+name: tap-nodes
#+begin_src python
@lazy
def nodes(tap):
    """Nodes in connsense-TAP
    """
    populations = tap.describe("extract-node-populations")

    if len(populations) == 0:
        LOG.warning("No populations configured!")
        return None

    def of_(population):
        """..."""
        LOG.info("Load dataset %s: \n%s", population["dataset"], pformat(population["description"]))
        return TapDataset(tap, population["dataset"])

    if len(populations) == 1:
        return of_(populations[0])
    return {population["dataset"][1]: of_(population) for population in populations}


#+end_src

*** TAP Dataset

#+name: tap-dataset
#+begin_src python


class TapDataset:
    """A dataset computed by connsense-TAP.
    """
    def __init__(self, tap, dataset):
        """..."""
        self._tap = tap
        self._dataset = dataset

    def index_ids(self, variable):
        """..."""
        try:
            series = self._tap.create_index(variable)
        except KeyError:
            LOG.warn("No values for %s in TAP at %s", variable, tap._root)
            return None

        return pd.Series(series.index.values, name=f"{series.name}_id",
                         index=pd.Index(series.values, name=series.name))

    @lazy
    def id_subtargets(self):
        """..."""
        return self.index_ids("subtarget")
    @lazy
    def id_circuits(self):
        """..."""
        return self.index_ids("circuit")
    @lazy
    def id_connectomes(self):
        """..."""
        return self.index_ids("connectome")

    @property
    def dataset(self):
        """..."""
        return self._tap.pour(self._dataset).sort_index()

    def index(self, subtarget, circuit=None, connectome=None):
        """Get `connsense-TAP`index for the arguments.
        """
        subtarget_id = self.id_subtargets.loc[subtarget]

        if not circuit:
            assert not connectome, f"connectome must be of a circuit"
            return (subtarget_id,)

        circuit_id = self.id_circuits.loc[circuit]

        if not connectome:
            return (subtarget_id, circuit_id)

        connectome_id = self.id_connectomes.loc[connectome]
        return (subtarget_id, circuit_id, connectome_id)


    def __call__(self, subtarget, circuit=None, connectome=None):
        """Call to get data using the names for (subtarget, circuit, connectome).
        """
        result = self.dataset.loc[self.index(subtarget, circuit, connectome)]

        try:
            evaluate = result.get_value
        except AttributeError:
            pass
        else:
            return evaluate()

        if len(result) == 1:
            return result.iloc[0].get_value()
        return result


#+end_src

#+RESULTS: eap-dataset
: None

*** TAP Adjacency

#+name: tap-adjacency
#+begin_src python
@lazy
def adjacency(tap):
    """Adjacency matrices of subtargets in connsense-TAP
    """
    populations = tap.describe("extract-edge-populations")

    if len(populations) == 0:
        LOG.warning("No populations configured!")
        return None

    def of_(population):
        """..."""
        LOG.info("Load dataset %s: \n%s", population["dataset"], pformat(population["description"]))
        return TapDataset(tap, population["dataset"])

    if len(populations) == 1:
        return of_(populations[0])
    return {population["dataset"][1]: of_(population) for population in populations}

#+end_src

*** TAP Analyses

For analyses we have an additional level, of phenomenon.

#+name: tap-analyses
#+begin_src python
def get_phenomenon(tap, computation_type):
    """..."""
    analysis = computation_type.split('-')
    if analysis[0] != "analyze":
        LOG.warn("%s is not an analysis", computaiton_tyoe)
        return None

    return '-'.join(analysis[1:])

def find_analyses(tap, phenomenon=None):
    """Find all analyses of phenomenon in the config.
    """

    if phenomenon:
        analyzed = tap.parameters[f"analyze-{phenomenon}"]
        return analyzed["analyses"]

    return {p: tap.find_analyses(phenomenon=p) for p in tap.phenomena}

@property
def phenomena(tap):
    """The analyze phenomena.
    """
    return [tap.get_phenomenon(computation_type=c) for c in tap.parameters if c.startswith("analyze-")]

def describe_analyses(tap, phenomenon=None):
    """..."""
    analyze = "analyze-{}".format
    if phenomenon:
        return tap.describe(analyze(phenomenon))
    return {p: tap.describe(analyze(p)) for p in tap.phenomena}

@lazy
def analyses(tap):
    """..."""
    analyses = tap.describe_analyses()
    return {phenomenon: {q["dataset"][1]: TapDataset(tap, q["dataset"]) for q in quantities}
            for phenomenon, quantities in analyses.items()}

#+end_src

#+RESULTS: tap-analyses

#+begin_src python


    def describe_analyses(self, phenomenon):
        """Describe analyses..."""
        computation_type = f"analyze-{phenomenon}"
        raise NotImplementedError("INPROGRESS")

    def find_datasets(self, computation_type=None, of_quantity=None, available=False):
        """Show datasets, either the configured ones, or those that have been computed

        computation_type: name of the computation to show datasets for,
        ~                 or all of the datasets
        of_quantity: name of the quantity in the entries of computation_type to show datasets,
        ~            or all the datasets of `computation_type`
        available: show only the datasets that have been computed.
        """
        if available:
            raise NotImplementedError("INPROGRESS")

        if computation_type:
            if of_quantity:
                description = self.describe(computation_type, of_quantity)
                return description.get("description", None)

            description = self.describe(computation_type)
            return [((computation_type, q), self.find_datasets(computation_type, q))
                    for q in description[PARAMKEY[computation_type]]]

        assert not of_quantity, f"Missing computation-type {of_quantity}"

        description = self.describe()
        return {c: self.find_datasets(computation_type=c) for c in self.describe()}

    def find_analyses(self, phenomenon, quantity):
        """Show datasets for anayses of a phenomenon, quantity.
        connsense-TAP will look for analyses configured for dataset reference [analyze-phenomenon, quantity]
        """
        return find_datasets(f"analyze-{phenomenon}", quantity)

    def get_path(self, computation_type):
        """..."""
        return (self._root, self._groups[computation_type])

    @lazy
    def analysis_phenomena(self):
        """..."""
        return ['-'.join(key.split('-')[1:]) for key in self.parameters if key.startswith("analyze-")]

    def pour_analyses(self, phenomenon, quantity=None):
        """..."""
        if phenomenon not in self.analysis_phenomena:
            LOG.error(f"Unknown analyze-{phenomenon}. Update connsense-TAP, or choose from \n%s",
                      pformat(self.analysis_phenomena))
            raise NotConfiguredError(f"analyze-{phenomenon}")

        computation_type = f"analyze-{phenomenon}"
        dataset = lambda q: [computation_type, q]


        raise NotImplementedError

    @lazy
    def analyses(self):
        """Datasets for configured analyses."""
        return {p: self.pour_analyses(phenomenon=p) for p in self.analysis_phenomena}

    def pour_dataset(self, computation, of_quantity):
        """..."""
        h5, group = self.get_path(computation)

        if computation.startswith("analyze-"):
            dataset = self.analyses['-'.join(computation.split('-')[1:])].get(of_quantity, None)
        elif computation == "extract-node-populations":
            dataset = matrices.get_store(h5, group+'/'+of_quantity, pd.DataFrame).toc
        elif computation == "extract-edge-populations":
            dataset = read_toc_plus_payload(h5, group+'/'+of_quantity, computation).sort_index()
        else:
            raise KeyError(f"Unknown {computation}")
        return dataset

    def pour(self, dataset):
        """Pour a dataset loaded from the H5 store.

        dataset: (computation_type, of_quantity)
        """
        from connsense.pipeline.parallelization import parallelization as prl
        computation_type, of_quantity = prl.describe(dataset)

        with h5py.File(self._root, 'r') as hdf_store:
            _, group = self.get_path(computation_type)
            key = '/'.join([group, of_quantity])
            datakey = of_quantity + "/data" if "data" in hdf_store[key] else of_quantity

        return self.pour_dataset(computation_type, datakey)

#+end_src

We want to get the datasets without a knowledge of what is in the config. We can etpose the common computation types as ~tap-attributes~, with helpful logging and error-messages. All the configured computations follow a convention that allows us to define a ~TapDataset~,

#+name: tap-nodes
#+begin_src python
@lazy
def nodes(tap):
    """Nodes that were extracted
    """
    return TapDataset(self, "extract-node-populations")
#+end_src

** Results

Finally, let us collect the code in a module,

#+begin_src python :tangle topotap.py :comments org :padline yes :noweb yes
<<tap-imports>>

<<tap-locate>>

<<tap-connsense-index>>

<<tap-dataset>>

<<tap-connsense-hdfstore-init>>

    <<tap-parameters>>

    <<tap-paramkey>>

    <<tap-describe>>

    <<tap-pour-dataset>>

    <<tap-pour-analyses>>

    <<tap-create-index>>

    <<tap-subtargets>>

    <<tap-nodes>>

    <<tap-adjacency>>

    <<tap-analyses>>
#+end_src

and also the notebook,

#+begin_src jupyter-python :tangle develop_topotap.py :comments no :noweb yes :padline yes
<<notebook-connsense-topotap>>

<<notebook-connsense-subtargets>>

<<notebook-connsense-nodes>>

<<notebook-connsense-nodes-load-lazy>>

<<notebook-connsense-nodes-subtarget-circuit>>

<<notebook-connsense-nodes-subtarget>>

<<notebook-connsense-adjacency>>

<<notebook-connsense-adjacency-load>>

<<notebook-connsense-analyses>>

<<notebook-connsense-analyses-load>>

<<notebook-connsense-simplex-counts-load>>

#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
 2022-10-11 14:30:33,388: Load dataset ('define-subtargets', 'flatmap-columns'):
('Hexaongal prism like columns oriented along cortical layers, from '
 'white-matter to pia. The data is loaded from an NRRD file that maps each '
 'circuit voxel to a subtarget ids corresponding to a flatmap column.The '
 'subtarget ids should be mapped to the subtargets they refer to in a '
 'dataframe provided as the input `info`.')
 2022-10-11 14:30:33,403: Load dataset ('extract-node-populations', 'default'):
('The default population will be that of neurons in the SSCx. To extract the '
 'neurons we will use a `connsense` method that uses ~bluepy~.')
 2022-10-11 14:30:33,404: Initialize a DataFrameStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/connsense.h5 / nodes/populations/default
 2022-10-11 14:30:33,413: Initialize a DataFrameStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/connsense.h5 / nodes/populations/default
Available analyses:
{'connectivity': {'simplex-counts': <connsense.develop.topotap.TapDataset object at 0x7ffe18be9a60>}}
 2022-10-11 14:30:33,637: Initialize a DataFrameStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/connsense.h5 / nodes/populations/default
<class 'pandas.core.frame.DataFrame'>
Int64Index: 4570 entries, 0 to 4569
Data columns (total 11 columns):
 #   Column         Non-Null Count  Dtype
---  ------         --------------  -----
 0   gid            4570 non-null   int64
 1   region         4570 non-null   category
 2   layer          4570 non-null   int64
 3   x              4570 non-null   float64
 4   y              4570 non-null   float64
 5   z              4570 non-null   float64
 6   synapse_class  4570 non-null   category
 7   mtype          4570 non-null   category
 8   etype          4570 non-null   category
 9   morphology     4570 non-null   category
 10  depth          4570 non-null   float64
dtypes: category(5), float64(4), int64(2)
memory usage: 3.0 MB
 2022-10-11 14:30:33,904: Initialize a DataFrameStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/connsense.h5 / nodes/populations/default
<class 'pandas.core.frame.DataFrame'>
Int64Index: 1823 entries, 0 to 1822
Data columns (total 11 columns):
 #   Column         Non-Null Count  Dtype
---  ------         --------------  -----
 0   gid            1823 non-null   int64
 1   region         1823 non-null   category
 2   layer          1823 non-null   int64
 3   x              1823 non-null   float64
 4   y              1823 non-null   float64
 5   z              1823 non-null   float64
 6   synapse_class  1823 non-null   category
 7   mtype          1823 non-null   category
 8   etype          1823 non-null   category
 9   morphology     1823 non-null   category
 10  depth          1823 non-null   float64
dtypes: category(5), float64(4), int64(2)
memory usage: 2.9 MB
 2022-10-11 14:30:34,133: Load dataset ('extract-edge-populations', 'local'):
None
 2022-10-11 14:30:34,200: Pour analyses for analyze-connectivity
 2022-10-11 14:30:34,200: Initialize a SeriesStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/connsense.h5 / analyses/connectivity/simplex-counts
 2022-10-11 14:30:34,211: Pour analyses for analyze-connectivity
 2022-10-11 14:30:34,212: Initialize a SeriesStore matrix store loading / writing data at /gpfs/bbp.cscs.ch/project/proj83/home/sood/topological-analysis-subvolumes/test/v2/connsense.h5 / analyses/connectivity/simplex-counts
<class 'pandas.core.frame.DataFrame'>
Int64Index: 1823 entries, 0 to 1822
Data columns (total 11 columns):
 #   Column         Non-Null Count  Dtype
---  ------         --------------  -----
 0   gid            1823 non-null   int64
 1   region         1823 non-null   category
 2   layer          1823 non-null   int64
 3   x              1823 non-null   float64
 4   y              1823 non-null   float64
 5   z              1823 non-null   float64
 6   synapse_class  1823 non-null   category
 7   mtype          1823 non-null   category
 8   etype          1823 non-null   category
 9   morphology     1823 non-null   category
 10  depth          1823 non-null   float64
dtypes: category(5), float64(4), int64(2)
memory usage: 2.9 MB
{'connectivity': {'simplex-counts': <connsense.develop.topotap.TapDataset object at 0x7ffe18be9a60>}}
#+end_example
: dim
: 0      1823
: 1     88675
: 2    276930
: 3     85837
: 4      3495
: 5        21
: Name: simplex_count, dtype: int64
:END:


* Controls

** Test develop

We have setup a computation of controls for ~simplex-counts~ for the original adjacencies.

#+name: notebook-connsense-controls
#+begin_src jupyter-python
CTRLSPACE = CONNSPACE / "test"

SIMPSPACE = CTRLSPACE / "run" / "analyze-connectivity" / "simplex-counts"

setup_items = list(f.name for f in SIMPSPACE.glob('*'))
setup = {"compute_nodes": [c for c in setup_items if c.startswith("compute-node-")],
         "configs": [x for x in setup_items if not x.startswith("compute-node-")]}

print("Use number of compute nodes: ", len(setup["compute_nodes"]))
pprint(setup["configs"])
#+end_src

#+RESULTS: notebook-connsense-controls
: Use number of compute nodes:  100
: ['subtargets.h5',
:  'pipeline.yaml',
:  'runtime.yaml',
:  'setup.json',
:  'launchscript-0.sh',
:  'description.json']

Let us take a peak at the datasets at one of the compute nodes,

#+name: notebook-connsense-controls-cn0
#+begin_src jupyter-python
CN0 = SIMPSPACE / "compute-node-0"

pprint(list(f.name for f in CN0.glob('*')))
#+end_src

#+RESULTS: notebook-connsense-controls-cn0
#+begin_example
['connsense-1.h5',
 'analyze__connectivity.err',
 'analyze__connectivity.out',
 'inputs.h5',
 'connsense-0.h5',
 'connsense-3.h5',
 'pipeline.yaml',
 'INPROGRESS',
 'runtime.yaml',
 'topology_analysis.err',
 'setup.json',
 'analyze-connectivity.sbatch']
#+end_example


* Examples

Let us develop some examples to show how to work with ~topotap~.

** Subset subtargets

For development purposes, 239 subtargets in the SSCx flatmap are too many. Let us use ~topotap~ to create a subset of these subtargets and save it to a workspace where we can test develop...

#+begin_src jupyter-python :tangle develop_topotap.py
topotap.subtargets
#+end_src

#+RESULTS:
#+begin_example
             subtarget  flat_i  flat_j        flat_x  flat_y
subtarget_id
1               R18;C0     -27      27  3.802528e-13  6210.0
2               R19;C0     -28      29  1.991858e+02  6555.0
3               R18;C1     -26      28  3.983717e+02  6210.0
4               R19;C1     -27      30  5.975575e+02  6555.0
5               R16;C0     -24      24  3.380025e-13  5520.0
...                ...     ...     ...           ...     ...
236             R4;C12       6      18  4.780460e+03  1380.0
237             R9;C15       2      29  6.174761e+03  3105.0
238            R15;C13      -9      36  5.378018e+03  5175.0
239             R3;C11       7      16  4.581274e+03  1035.0
240            R15;C15      -7      38  6.174761e+03  5175.0

[240 rows x 5 columns]
#+end_example
